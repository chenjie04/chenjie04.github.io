{"posts":[{"title":"双线性插值法的直观理解","content":"在学习可变形卷积的过程中，涉及到给采样点加上偏移量进行卷积感受野变形的操作，但是偏移量是浮点数，偏移后一般不能保证刚好落到某个像素点上，多是落到类似（10.3，25.7）这样的虚拟像素点上，这样我们就需要用到双线性插值法来计算出虚拟像素点的值。双线性插值法就是找出虚拟像素点周围最近的4个真实像素点，然后对这4个像素点的值进行加权求和，作为虚拟像素的值。如上图所示（图片来自:https://zhuanlan.zhihu.com/p/110754637），假设我们的虚拟像素点为P(x,y)P(x,y)P(x,y)。首先，我们需要找出周围最近的4个真实像素点:Q11(x1,y1)Q11(x1,y1)Q11(x1,y1)、Q12(x1,y2)Q12(x1,y2)Q12(x1,y2)、Q21(x2,y1)Q21(x2,y1)Q21(x2,y1)、Q22(x2,y2)Q22(x2,y2)Q22(x2,y2)。怎么找？简单！xxx向下取整得到x1x1x1，x1+1x1+1x1+1得到x2x2x2；yyy向下取整得到y1y1y1，y1+1y1+1y1+1得到y2y2y2；支持4个点全部找到。然后，Q11(x1,y1)Q11(x1,y1)Q11(x1,y1)和Q21(x2,y1)Q21(x2,y1)Q21(x2,y1)进行一个x方向的插值，求出点R1R1R1的值（记为f(R1)f(R1)f(R1)，其他点同理）。具体怎么做？我们说R1R1R1越靠近Q11Q11Q11，那么f(Q11)f(Q11)f(Q11)的权重就应该越大，反之越小。很直观，很合理吧？从图中，我们可以看到R1R1R1刚好把(x2−x1)(x2-x1)(x2−x1)分成了x2−xx2-xx2−x和x−x1x-x1x−x1两段。R1R1R1越靠近Q11Q11Q11，段x2−xx2-xx2−x越长，x−x1x-x1x−x1越短，那么我们把x2−xx2-xx2−x/x2−x1x2-x1x2−x1作为f(Q11)f(Q11)f(Q11)的权重不就好了吗？同样x−x1x-x1x−x1/x2−x1x2-x1x2−x1作为f(Q21)f(Q21)f(Q21)的权重，则有：f(R1)=x2−xx2−x1f(Q11)+x−x1x2−x1f(Q21)f(R1)=\\frac{x2-x}{x2-x1}f(Q11)+\\frac{x-x1}{x2-x1}f(Q21)f(R1)=x2−x1x2−x​f(Q11)+x2−x1x−x1​f(Q21)同样，Q12(x1,y2)Q12(x1,y2)Q12(x1,y2)和Q22(x2,y2)Q22(x2,y2)Q22(x2,y2)进行一个x方向的插值，求出点R2R2R2的值:f(R2)=x2−xx2−x1f(Q12)+x−x1x2−x1f(Q21)f(R2)=\\frac{x2-x}{x2-x1}f(Q12)+\\frac{x-x1}{x2-x1}f(Q21)f(R2)=x2−x1x2−x​f(Q12)+x2−x1x−x1​f(Q21)最后，R1(x,y1)R1(x,y1)R1(x,y1)和R2(x,y2)R2(x,y2)R2(x,y2)进行一个y方向的插值，就求出点P(x,y)P(x,y)P(x,y)的值:f(P)=y2−yy2−y1f(R1)+y−y1y2−y1f(R2)f(P)=\\frac{y2-y}{y2-y1}f(R1)+\\frac{y-y1}{y2-y1}f(R2)f(P)=y2−y1y2−y​f(R1)+y2−y1y−y1​f(R2)把前面求得的f(R1)f(R1)f(R1)，f(R2)f(R2)f(R2)代进来得：因为分母都为1，所以得","link":"https://chenjie04.github.io/post/shuang-xian-xing-cha-zhi-fa-de-zhi-guan-li-jie/"},{"title":"卷积输出大小的计算详解","content":"一直以来都没有详细计算过卷积输出大小的过程，都是需要的时候上网查，网上的教程都比较乱或者直接给出公式，不够形象，因此自己记录一下。考虑上图的通用卷积场景：输入大小为9×99\\times99×9，卷积核大小为3×33\\times33×3，空洞因子（dilationrate）为3，卷积滑动步长为1，padding也为1。首先，计算实际感受野的大小。考虑空洞卷积的情况下，实际上我们得在卷积核内隔一个像素插入一个空洞，即插入kernel_size−1kernel\\_size-1kernel_size−1个空洞，每个空洞大小为dilation−1dilation-1dilation−1，因此实际感受野为，Filed=kernel_size+(kernel_size−1)(dilation−1)Filed=kernel\\_size+(kernel\\_size-1)(dilation-1)Filed=kernel_size+(kernel_size−1)(dilation−1)有人会进一步化简为，Filed=dilation×(kernel_size−1)+1Filed=dilation\\times(kernel\\_size-1)+1Filed=dilation×(kernel_size−1)+1我还是习惯用最原始、最直观的计算公式来理解，然后，我们回到上图，卷积是滑窗操作，不考虑第一个卷积计算，实际可滑动的距离d为多少？显然，就是图像大小加上padding后减去感受野的大小，即d=width+2×pad−(kernel_size+(kernel_size−1)(dilation−1))d=width+2\\timespad-(kernel\\_size+(kernel\\_size-1)(dilation-1))d=width+2×pad−(kernel_size+(kernel_size−1)(dilation−1))那么，输出的特征图大小不就是滑动距离除以步长再加上第一个卷积计算嘛，那么输出大小为，output_w=(width+2×pad−(kernel_size+(kernel_size−1)(dilation−1)))/stride_w+1output\\_w=(width+2\\timespad-(kernel\\_size+(kernel\\_size-1)(dilation-1)))/stride\\_w+1output_w=(width+2×pad−(kernel_size+(kernel_size−1)(dilation−1)))/stride_w+1一般来说，在卷积中，输入和卷积核都是正方形的，所以输出的高度计算公式也一样。另外，在代码实现的时候最好采用简化都是感受野计算公式，帮助减少点计算量，训练也会更快点嘛！","link":"https://chenjie04.github.io/post/juan-ji-shu-chu-da-xiao-de-ji-suan-xiang-jie/"},{"title":"ROS2开发笔记——Launch系统（六）","content":"在前面启动某个节点时，往往都是使用ros2run命令，这样美启动一个节点都需要打开一个新的终端，显然不科学。那么，我们是如何管理我们的节点呢？答案就是用Launch系统。使用Launch系统可以同时启动多个节点，还可以配置节点的参数（parameter），注册事件处理函数等。一、Launch系统的使用Launch系统使用launch文件描述如何启动系统。在包目录下新建一个launch目录并新建turtlesim_mimic_launch.py文件，fromlaunchimportLaunchDescriptionfromlaunch.actionsimportDeclareLaunchArgumentfromlaunch.substitutionsimportLaunchConfiguration,TextSubstitutionfromlaunch_ros.actionsimportNodedefgenerate_launch_description():background_r_launch_arg=DeclareLaunchArgument('background_r',default_value=TextSubstitution(text='0'))background_g_launch_arg=DeclareLaunchArgument('background_g',default_value=TextSubstitution(text='84'))background_b_launch_arg=DeclareLaunchArgument('background_b',default_value=TextSubstitution(text='122'))returnLaunchDescription([background_r_launch_arg,background_g_launch_arg,background_b_launch_arg,Node(package='turtlesim',namespace='turtlesim1',executable='turtlesim_node',name='sim'),Node(package='turtlesim',namespace='turtlesim2',executable='turtlesim_node',name='sim',parameters=[{'background_r':LaunchConfiguration('background_r'),'background_g':LaunchConfiguration('background_g'),'background_b':LaunchConfiguration('background_b'),}]),Node(package='turtlesim',executable='mimic',name='mimic',remappings=[('/input/pose','/turtlesim1/turtle1/pose'),('/output/cmd_vel','/turtlesim2/turtle1/cmd_vel'),])])首先导入相应的python模块，fromlaunchimportLaunchDescriptionfromlaunch.actionsimportDeclareLaunchArgumentfromlaunch.substitutionsimportLaunchConfiguration,TextSubstitutionfromlaunch_ros.actionsimportNode然后，开始描述加载，defgenerate_launch_description():returnLaunchDescription([])加载第一个节点，从turtlesim包执行turtlesim_node程序，并指定命名空间和节点名称。命名空间的存在，使得我们可以在不同命名空间下运行同名的节点。Node(package='turtlesim',namespace='turtlesim1',executable='turtlesim_node',name='sim'),加载第二个节点，不过我们重新指定了窗口的背景颜色，并作为参数传递给节点，background_r_launch_arg=DeclareLaunchArgument('background_r',default_value=TextSubstitution(text='0'))background_g_launch_arg=DeclareLaunchArgument('background_g',default_value=TextSubstitution(text='84'))background_b_launch_arg=DeclareLaunchArgument('background_b',default_value=TextSubstitution(text='122'))...Node(package='turtlesim',namespace='turtlesim2',executable='turtlesim_node',name='sim',parameters=[{'background_r':LaunchConfiguration('background_r'),'background_g':LaunchConfiguration('background_g'),'background_b':LaunchConfiguration('background_b'),}]),第三个节点是想让第二个节点启动的小海龟模仿第一个节点启动的小海龟的行为，Node(package='turtlesim',executable='mimic',name='mimic',remappings=[('/input/pose','/turtlesim1/turtle1/pose'),('/output/cmd_vel','/turtlesim2/turtle1/cmd_vel'),])通过launch文件启动系统，$ros2launch&lt;package_name&gt;&lt;launch_file_name&gt;编译之前需要在package.xml中加入以下依赖&lt;exec_depend&gt;ros2launch&lt;/exec_depend&gt;执行之后，应该就会出现两个窗口，通过以下命令操纵第一个小海龟，第二个也跟着动，ros2topicpub-r1/turtlesim1/turtle1/cmd_velgeometry_msgs/msg/Twist&quot;{linear:{x:2.0,y:0.0,z:0.0},angular:{x:0.0,y:0.0,z:-1.8}}&quot;查看节点，$rqt_graphLaunch系统还有很多高阶用法，请参考官方教程！","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-launch-xi-tong-liu/"},{"title":"ROS2开发笔记——动作（action）样例（五）","content":"Action是基于topic和service之上的通信方式，使用于那些需要较长时间执行的任务。Action通信示例如下，下面，我们将通过一个计算斐波那契(Fibonacci)数列的例子演示action通信。在斐波那契数列中第一和第二个元素分别为0和1，然后其余每位元素是其前两位元素之和，因此需要一步步递归计算，属于需要较长时间执行的任务。在数学上，斐波那契数定义如下，∗F0=0∗F1=1∗Fn=Fn−1+Fn−2(n≤2)*F_{0}=0\\\\*F_{1}=1\\\\*F_{n}=F_{n-1}+F_{n-2}(n\\leq2)\\\\∗F0​=0∗F1​=1∗Fn​=Fn−1​+Fn−2​(n≤2)一、Action通信数据结构新建包$mkdir-pros2_ws/src#youcanreuseexistingworkspacewiththisnamingconvention$cdros2_ws/src$ros2pkgcreateaction_tutorials_interfaces$cdaction_tutorials_interfaces$mkdiraction在action目录下新建接口文件Fibonacci.action。Action通信构建于topic和service之上，客户端发起请求，服务端响应并返回结果，期间需要不断反馈任务进行进度，因此action通信的数据结构应当由以下三部分构成，#Request---#Result---#Feedback首先，客户端发起的请求是求几阶的斐波那契数列，因此请求应该是整型数；服务端应该将对应的数列返回，因此结果应该是整型数组；而反馈是计算的中间结果，因此同样是整型数组。根据上述分析，定义数据结构如下，int32order---int32[]sequence---int32[]partial_sequence更新CMakeList.txtfind_package(rosidl_default_generatorsREQUIRED)rosidl_generate_interfaces(${PROJECT_NAME}&quot;action/Fibonacci.action&quot;)更新package.xml&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;&lt;depend&gt;action_msgs&lt;/depend&gt;&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;编译#Changetotherootoftheworkspace$cd~/ros2_ws#Build$colconbuild测试#Sourceourworkspace#OnWindows:callinstall/setup.bat$.install/setup.bash#Checkthatouractiondefinitionexists$ros2interfaceshowaction_tutorials_interfaces/action/Fibonacci二、Action通信样例1、环境准备$ros2pkgcreate--build-typeament_pythonaction_tutorials_py2、编写ActionServer在src/action_tutorials_py/action_tutorials_py下新建fibonacci_action_server.py文件，importtimeimportrclpyfromrclpy.actionimportActionServer#导入ActionServerfromrclpy.nodeimportNodefromaction_tutorials_interfaces.actionimportFibonacciclassFibonacciActionServer(Node):def__init__(self):super().__init__('fibonacci_action_server')#初始化ActionServeself._action_server=ActionServer(self,Fibonacci,'fibonacci',self.execute_callback)defexecute_callback(self,goal_handle):self.get_logger().info('Executinggoal...')#日志feedback_msg=Fibonacci.Feedback()#实例化反馈信息，注意调用方式feedback_msg.partial_sequence=[0,1]#初始化反馈信息foriinrange(1,goal_handle.request.order):#从goal_handle获取请求#迭代计算斐波那契数列，并将计算结果作为反馈信息发布feedback_msg.partial_sequence.append(feedback_msg.partial_sequence[i]+feedback_msg.partial_sequence[i-1])self.get_logger().info('Feedback:{0}'.format(feedback_msg.partial_sequence))goal_handle.publish_feedback(feedback_msg)time.sleep(1)goal_handle.succeed()#任务执行完成后，将标志置为SUCCEEDresult=Fibonacci.Result()#实例化结果信息result.sequence=feedback_msg.partial_sequence#将最后一个反馈信息作为结果returnresultdefmain(args=None):rclpy.init(args=args)fibonacci_action_server=FibonacciActionServer()rclpy.spin(fibonacci_action_server)if__name__=='__main__':main()3、编写ActionClient在src/action_tutorials_py/action_tutorials_py下新建fibonacci_action_client.py文件，importrclpyfromrclpy.actionimportActionClientfromrclpy.nodeimportNodefromaction_tutorials_interfaces.actionimportFibonacciclassFibonacciActionClient(Node):def__init__(self):super().__init__('fibonacci_action_client')self._action_client=ActionClient(self,Fibonacci,'fibonacci')#新建ActionClient，用过action的名字'fibonacci'与ActionServer建立通信defsend_goal(self,order):#发起请求的函数goal_msg=Fibonacci.Goal()goal_msg.order=orderself._action_client.wait_for_server()#等待服务self._send_goal_future=self._action_client.send_goal_async(goal_msg,feedback_callback=self.feedback_callback)#异步执行，将请求发给server，返回一个futureself._send_goal_future.add_done_callback(self.goal_response_callback)#注册响应回调函数defgoal_response_callback(self,future):goal_handle=future.result()ifnotgoal_handle.accepted:self.get_logger().info('Goalrejected:(')returnself.get_logger().info('Goalaccepted:)')self._get_result_future=goal_handle.get_result_async()self._get_result_future.add_done_callback(self.get_result_callback)defget_result_callback(self,future):result=future.result().resultself.get_logger().info('Result:{0}'.format(result.sequence))rclpy.shutdown()deffeedback_callback(self,feedback_msg):feedback=feedback_msg.feedbackself.get_logger().info('Receivedfeedback:{0}'.format(feedback.partial_sequence))defmain(args=None):rclpy.init(args=args)action_client=FibonacciActionClient()action_client.send_goal(10)rclpy.spin(action_client)if__name__=='__main__':main()4、更新依赖与程序执行入口package.xml&lt;depend&gt;action_tutorials_interfaces&lt;/depend&gt;&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;setup.pyentry_points={'console_scripts':['fibonacci_action_server=action_tutorials_py.fibonacci_action_server:main','fibonacci_action_client=action_tutorials_py.fibonacci_action_client:main',],},5、编译与运行$colconbuild--packages-selectaction_tutorials_py$sourceinstall/setup.bash$ros2runaction_tutorials_pyfibonacci_action_server$sourceinstall/setup.bash$ros2runaction_tutorials_pyfibonacci_action_client","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-dong-zuo-actionyang-li/"},{"title":"ROS2开发笔记——自定义数据结构（msg、srv）（四）","content":"目前，我们学习的topic和service两种通信方式用的都是ROS预定义的数据结构，但是有时候为了满足特殊需求就得自定义数据结构。一般而言，好的开发习惯是将数据结构放到独立的包中方便管理，也有数据结构的功能代码放到同一个包中的，例如官方教程中的Implementingcustominterfaces，这里，我们遵循官方建议的习惯，将数据结构放到独立的包中。一、新建包好像现在只支持用C++定义数据结构，在ros2_ws/src下新建数据结构包，$ros2pkgcreate--build-typeament_cmaketutorial_interfaces采用C++开发时，新建的包稍微有些不一样，tutorial_interfaces/CMakeLists.txtinclude/tutorial_interfaces/package.xmlsrc/package.xml还是指定了包的依赖（依赖包的名字会不一样），src/存放源文件，include/tutorial_interfaces/存放头文件，CMakeLists.txt描述了怎么编译这个包。在这个样例中我们学习怎么自定义信息和服务两种数据结构，在ros2_ws/src/tutorial_interfaces目录下再新建两个对应的目录，$mkdirmsgsrv二、定义数据结构如果我们要定义的数据结构是信息，那么就再tutorial_interfaces/msg目录下新建对应的文件，信息数据结构文件以.msg结尾，例如我们定义一个Num.msg，如下所示，它只有一个整型变量int64num在我们定义的数据结构中也可以引用ROS中预定义的数据结构，假如我们想定义一个球的数据结构，球需要定义球心和半径，我们就可以如下定义Sphere.msg，geometry_msgs/Point是ROS预定义的数据结构geometry_msgs/Pointcenterfloat64radius如果我们要定义的数据结构是服务，那么就再tutorial_interfaces/srv目录下新建对应的文件，服务的数据结构以.srv结尾，仿照前面服务通信用到加法服务数据结构定义3个整数的加法服务数据结构AddThreeInts.srv，int64aint64bint64c---int64sum同样，上半部分是请求，下半部分是结果。三、更新配置文件3.1、CMakeLists.txt新建包时生成的CMakeLists.txt如下所示，cmake_minimum_required(VERSION3.8)project(tutorial_interfaces)if(CMAKE_COMPILER_IS_GNUCXXORCMAKE_CXX_COMPILER_IDMATCHES&quot;Clang&quot;)add_compile_options(-Wall-Wextra-Wpedantic)endif()#finddependenciesfind_package(ament_cmakeREQUIRED)#uncommentthefollowingsectioninordertofillin#furtherdependenciesmanually.#find_package(&lt;dependency&gt;REQUIRED)if(BUILD_TESTING)find_package(ament_lint_autoREQUIRED)#thefollowinglineskipsthelinterwhichchecksforcopyrights#commentthelinewhenacopyrightandlicenseisaddedtoallsourcefilesset(ament_cmake_copyright_FOUNDTRUE)#thefollowinglineskipscpplint(onlyworksinagitrepo)#commentthelinewhenthispackageisinagitrepoandwhen#acopyrightandlicenseisaddedtoallsourcefilesset(ament_cmake_cpplint_FOUNDTRUE)ament_lint_auto_find_test_dependencies()endif()ament_package()我们的球数据结构Sphere.msg依赖geometry_msgs包，同时我们还需要调用rosidl_default_generators来把我们自定义是数据结构转换成语言相关的代码，默认的生成器应该包含了C++和Python两种语言。最后，调用rosidl_generate_interfaces()生成对应的接口，find_package(geometry_msgsREQUIRED)find_package(rosidl_default_generatorsREQUIRED)rosidl_generate_interfaces(${PROJECT_NAME}&quot;msg/Num.msg&quot;&quot;msg/Sphere.msg&quot;&quot;srv/AddThreeInts.srv&quot;DEPENDENCIESgeometry_msgs#Addpackagesthatabovemessagesdependon,inthiscasegeometry_msgsforSphere.msg)3.2、package.xmlpackage.xml增加如下代码，&lt;depend&gt;geometry_msgs&lt;/depend&gt;#Sphere.msg依赖该包&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;#用于生成语言相关的代码&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;#这是使用自定义接口需要的运行时&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;#我们的接口是关联到这个包的四、编译与验证编译$colconbuild--packages-selecttutorial_interfaces验证$sourceinstall/setup.bash$ros2interfaceshowtutorial_interfaces/msg/Numint64num$ros2interfaceshowtutorial_interfaces/msg/Spheregeometry_msgs/Pointcenterfloat64xfloat64yfloat64zfloat64radius$ros2interfaceshowtutorial_interfaces/srv/AddThreeIntsint64aint64bint64c---int64sum4.1、在发布/订阅者通信中验证回到我们的发布/订阅者通信开发样例中，按如下修改代码，我们发布的主题不再使用预定义的String类型，而是简单发布一个数字，用我们自定义的Num.msg类型，发布者代码修改如下，importrclpyfromrclpy.nodeimportNodefromtutorial_interfaces.msgimportNum#CHANGEclassMinimalPublisher(Node):def__init__(self):super().__init__('minimal_publisher')self.publisher_=self.create_publisher(Num,'topic',10)#CHANGEtimer_period=0.5self.timer=self.create_timer(timer_period,self.timer_callback)self.i=0deftimer_callback(self):msg=Num()#CHANGEmsg.num=self.i#CHANGEself.publisher_.publish(msg)self.get_logger().info('Publishing:&quot;%d&quot;'%msg.num)#CHANGEself.i+=1defmain(args=None):rclpy.init(args=args)minimal_publisher=MinimalPublisher()rclpy.spin(minimal_publisher)minimal_publisher.destroy_node()rclpy.shutdown()if__name__=='__main__':main()订阅者代码修改如下，importrclpyfromrclpy.nodeimportNodefromtutorial_interfaces.msgimportNum#CHANGEclassMinimalSubscriber(Node):def__init__(self):super().__init__('minimal_subscriber')self.subscription=self.create_subscription(Num,#CHANGE'topic',self.listener_callback,10)self.subscriptiondeflistener_callback(self,msg):self.get_logger().info('Iheard:&quot;%d&quot;'%msg.num)#CHANGEdefmain(args=None):rclpy.init(args=args)minimal_subscriber=MinimalSubscriber()rclpy.spin(minimal_subscriber)minimal_subscriber.destroy_node()rclpy.shutdown()if__name__=='__main__':main()我们还需要在package.xml添加依赖&lt;exec_depend&gt;tutorial_interfaces&lt;/exec_depend&gt;最后，重新编译运行$colconbuild--packages-selectpy_pubsub$ros2runpy_pubsubtalker[INFO][minimal_publisher]:Publishing:'0'[INFO][minimal_publisher]:Publishing:'1'[INFO][minimal_publisher]:Publishing:'2'$ros2runpy_pubsublistener...4.2、在服务通信中验证代码修改如下服务端fromtutorial_interfaces.srvimportAddThreeInts#CHANGEimportrclpyfromrclpy.nodeimportNodeclassMinimalService(Node):def__init__(self):super().__init__('minimal_service')self.srv=self.create_service(AddThreeInts,'add_three_ints',self.add_three_ints_callback)#CHANGEdefadd_three_ints_callback(self,request,response):response.sum=request.a+request.b+request.c#CHANGEself.get_logger().info('Incomingrequest\\na:%db:%dc:%d'%(request.a,request.b,request.c))#CHANGEreturnresponsedefmain(args=None):rclpy.init(args=args)minimal_service=MinimalService()rclpy.spin(minimal_service)rclpy.shutdown()if__name__=='__main__':main()客户端fromtutorial_interfaces.srvimportAddThreeInts#CHANGEimportsysimportrclpyfromrclpy.nodeimportNodeclassMinimalClientAsync(Node):def__init__(self):super().__init__('minimal_client_async')self.cli=self.create_client(AddThreeInts,'add_three_ints')#CHANGEwhilenotself.cli.wait_for_service(timeout_sec=1.0):self.get_logger().info('servicenotavailable,waitingagain...')self.req=AddThreeInts.Request()#CHANGEdefsend_request(self):self.req.a=int(sys.argv[1])self.req.b=int(sys.argv[2])self.req.c=int(sys.argv[3])#CHANGEself.future=self.cli.call_async(self.req)defmain(args=None):rclpy.init(args=args)minimal_client=MinimalClientAsync()minimal_client.send_request()whilerclpy.ok():rclpy.spin_once(minimal_client)ifminimal_client.future.done():try:response=minimal_client.future.result()exceptExceptionase:minimal_client.get_logger().info('Servicecallfailed%r'%(e,))else:minimal_client.get_logger().info('Resultofadd_three_ints:for%d+%d+%d=%d'%#CHANGE(minimal_client.req.a,minimal_client.req.b,minimal_client.req.c,response.sum))#CHANGEbreakminimal_client.destroy_node()rclpy.shutdown()if__name__=='__main__':main()添加依赖&lt;exec_depend&gt;tutorial_interfaces&lt;/exec_depend&gt;编译运行$colconbuild--packages-selectpy_srvcli$ros2runpy_srvcliservice$ros2runpy_srvcliclient231","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-zi-ding-yi-shu-ju-jie-gou-msgsrv/"},{"title":"ROS2开发笔记——service样例（三）","content":"一、创建包服务的通信方式就是客户端发起其服务请求，服务端收到请求后响应。在这里，我们通过一个加法求解的样例演示怎么进行服务开发，客户端发起请求——“我想知道a+b=？”，服务端收到a和b这两个数后把它们相加并返回给客户端。根据问题的定义，那么这个服务的数据结构就应该如下所示，int64aint64b---int64sum上半部定义了请求的两个整型变量，下半部定义响应结果。该数据结构定义在了example_interfaces包里面，就是为了方便我们学习，后面我们也会学习怎么自定义需要的各类数据结构。除了上述的数据结构包，我们还需要依赖rclpy这个包，知道了这些信息，我们就可以在新建包的时候指定这些依赖包，$ros2pkgcreate--build-typeament_pythonpy_srvcli--dependenciesrclpyexample_interfaces更新package.xml和setup.py中的信息，&lt;description&gt;Pythonclientservertutorial&lt;/description&gt;&lt;maintaineremail=&quot;you@email.com&quot;&gt;YourName&lt;/maintainer&gt;&lt;license&gt;ApacheLicense2.0&lt;/license&gt;maintainer='YourName',maintainer_email='you@email.com',description='Pythonclientservertutorial',license='ApacheLicense2.0',二、编写服务端节点在ros2_ws/src/py_srvcli/py_srvcli目录下创建一个service_member_function.py文件，fromexample_interfaces.srvimportAddTwoInts#数据结构类型importrclpy#ROS2python客户端fromrclpy.nodeimportNode#节点父类classMinimalService(Node):def__init__(self):super().__init__('minimal_service')self.srv=self.create_service(AddTwoInts,'add_two_ints',self.add_two_ints_callback)#创建服务defadd_two_ints_callback(self,request,response):response.sum=request.a+request.bself.get_logger().info('Incomingrequest\\na:%db:%d'%(request.a,request.b))returnresponsedefmain():rclpy.init()minimal_service=MinimalService()rclpy.spin(minimal_service)rclpy.shutdown()if__name__=='__main__':main()我们在创建包的时候已经指定了依赖，所以现在我们只需要给程序添加执行入口，entry_points={'console_scripts':['service=py_srvcli.service_member_function:main',],},三、编写客户端节点在ros2_ws/src/py_srvcli/py_srvcli目录下创建一个client_member_function.py文件，importsys#导入sys，以便能够通过sys.argv在命令行读取输入fromexample_interfaces.srvimportAddTwoIntsimportrclpyfromrclpy.nodeimportNodeclassMinimalClientAsync(Node):def__init__(self):super().__init__('minimal_client_async')self.cli=self.create_client(AddTwoInts,'add_two_ints')#创建客户端，注意服务的名字和类型要和上面服务端的一致whilenotself.cli.wait_for_service(timeout_sec=1.0):#检查客户端指定的服务是否可用，如果先启动客户端节点，再启动服务端节点，就会出现服务不可用的情况，命令行提示等待self.get_logger().info('servicenotavailable,waitingagain...')self.req=AddTwoInts.Request()defsend_request(self,a,b):#定义怎么发送请求self.req.a=aself.req.b=bself.future=self.cli.call_async(self.req)#异步请求(非阻塞)rclpy.spin_until_future_complete(self,self.future)#等待服务端应答returnself.future.result()defmain():rclpy.init()minimal_client=MinimalClientAsync()response=minimal_client.send_request(int(sys.argv[1]),int(sys.argv[2]))#从命令行读取输入，并发送服务请求minimal_client.get_logger().info('Resultofadd_two_ints:for%d+%d=%d'%(int(sys.argv[1]),int(sys.argv[2]),response.sum))minimal_client.destroy_node()rclpy.shutdown()if__name__=='__main__':main()添加程序执行入口，entry_points={'console_scripts':['service=py_srvcli.service_member_function:main','client=py_srvcli.client_member_function:main',],},四、编译与运行检查依赖，$rosdepinstall-i--from-pathsrc--rosdistrohumble-y编译$colconbuild--packages-selectpy_srvcli启动服务端$sourceinstall/setup.bash$ros2runpy_srvcliservice在新的终端启动客户端$sourceinstall/setup.bash$ros2runpy_srvcliclient23[INFO][minimal_client_async]:Resultofadd_two_ints:for2+3=5回到服务端窗口，我们可以看到刚刚输出了以下提示信息，[INFO][minimal_service]:Incomingrequesta:2b:3","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-service-yang-li/"},{"title":"ROS2开发笔记——Topic样例（二）","content":"一、新建工作空间ROS采用工作空间的概念，在工作空间目录下一般设src目录存放源代码。ROS采用包的形式管理项目，所有工作空间src目录下一般有多个包。ROS可以采用python或C++进行开发，也可以同时采用这两种语言混合开发，因此一个典型ROS工作空间如下所示，workspace_folder/src/cpp_package_1/CMakeLists.txtinclude/cpp_package_1/package.xmlsrc/py_package_1/package.xmlresource/py_package_1setup.cfgsetup.pypy_package_1/...cpp_package_n/CMakeLists.txtinclude/cpp_package_n/package.xmlsrc/下面，我们新建自己的工作空间，$mkdir-p~/ros2_ws/src$cd~/ros2_ws/src二、新建包ROS提供了工具帮助我们按照模板新建包，&lt;build−type&gt;&lt;build-type&gt;&lt;build−type&gt;指定了编译类型，也就是指定了我们使用什么语言进行开发，使用C++则&lt;build−type&gt;&lt;build-type&gt;&lt;build−type&gt;为ament_cmake，而使用python则是ament_python。&lt;packagename&gt;&lt;package_name&gt;&lt;packagen​ame&gt;是我们的包名字。ros2pkgcreate--build-type&lt;build-type&gt;&lt;package_name&gt;下面，我们新建一个py_pubsub包，学习使用python进行ROS主题通信的开发，$ros2pkgcreate--build-typeament_pythonpy_pubsub我们看一下新建的py_pubsub的目录结构，py_pubsub/py_pubsub/resource/py_pubsubtest/package.xmlsetup.cfgsetup.py2.1、package.xml首先，查看第一个package.xml文件，首先它包含了一些包的名字、版本号、描述信息、许可以及维护者的信息。更重要的是package.xml指定了这个包的依赖，方便编译器在编译的时候能够找到对应的依赖。依赖以_depend结尾的标签指定，&lt;testdepend&gt;&lt;test_depend&gt;&lt;testd​epend&gt;顾名思义是测试时用到的依赖，还有&lt;builddepend&gt;&lt;build_depend&gt;&lt;buildd​epend&gt;编译依赖，&lt;execdepend&gt;&lt;exec_depend&gt;&lt;execd​epend&gt;执行依赖，以及&lt;buildexportdepend&gt;&lt;build_export_depend&gt;&lt;builde​xportd​epend&gt;导出依赖等。&lt;?xmlversion=&quot;1.0&quot;?&gt;&lt;?xml-modelhref=&quot;http://download.ros.org/schema/package_format3.xsd&quot;schematypens=&quot;http://www.w3.org/2001/XMLSchema&quot;?&gt;&lt;packageformat=&quot;3&quot;&gt;&lt;name&gt;py_pubsub&lt;/name&gt;&lt;version&gt;0.0.0&lt;/version&gt;&lt;description&gt;TODO:Packagedescription&lt;/description&gt;&lt;maintaineremail=&quot;user@todo.todo&quot;&gt;user&lt;/maintainer&gt;&lt;license&gt;TODO:Licensedeclaration&lt;/license&gt;&lt;test_depend&gt;ament_copyright&lt;/test_depend&gt;&lt;test_depend&gt;ament_flake8&lt;/test_depend&gt;&lt;test_depend&gt;ament_pep257&lt;/test_depend&gt;&lt;test_depend&gt;python3-pytest&lt;/test_depend&gt;&lt;export&gt;&lt;build_type&gt;ament_python&lt;/build_type&gt;&lt;/export&gt;&lt;/package&gt;加入我们项目中需要用到一个包，但是我们系统上没有，那么编译的时候必然会报错，那么我们怎么安装这些外部依赖包呢？这需要用到rosdep这个工具。使用rosdep之前，需要先对它进行初始化并更新，$sudorosdepinit$rosdepupdate但是sudorosdepinit需要从GitHub下载数据，由于国内的原因，运行这个命令必然会失败。这成为了每个学习ros的人必然要面对的一个问题。国情原因我理解，只是不知道为什么国内ros社区没有去维护一个国内版本，可能是许可的原因吧，以致大家需要使用一些邪魔外道的手段才可以解决这个问题。不过，现在博主鱼香ROS，自己维护了国内版rosdepc，算是解决了这个问题，rosdepc的安装与使用$pipinstallrosdepc$rosdepcinit$rosdepcupdate检查并安装依赖，rosdepcinstall--from-pathssrc-y--ignore-src--from-pathssrc表示检查src目录下的package.xml中的依赖，-y就是默认yes，--ignore-src表示加入package.xml中指定的某个依赖是当前工作空间中的就忽略，因为编译之后必然会安装。2.2、setup.pysetup.py是python包的安装指令文件，指定了怎么安装这个包，同样也包含该包的一些相关信息，我们需要注意的是执行程序入口entry_points，示例中定义了一个执行程序入口为my_node，该入口指向my_py_pkg.my_node:main函数，后面写代码的时候会有具体示例。fromsetuptoolsimportsetuppackage_name='py_pubsub'setup(name=package_name,version='0.0.0',packages=[package_name],data_files=[('share/ament_index/resource_index/packages',['resource/'+package_name]),('share/'+package_name,['package.xml']),],install_requires=['setuptools'],zip_safe=True,maintainer='TODO',maintainer_email='TODO',description='TODO:Packagedescription',license='TODO:Licensedeclaration',tests_require=['pytest'],entry_points={'console_scripts':[#'my_node=my_py_pkg.my_node:main'],},)2.3、setup.cfgsetup.cfg是为了执行ros2run命令时能够找到对应的可执行程序，[develop]script_dir=$base/lib/py_pubsub[install]install_scripts=$base/lib/py_pubsub2.4、其他剩下的是三个目录，一个是跟包同名的目录，用于存放我们的源代码文件，该目录下还有__init__.py文件。另外一个是resource目录，底下也是跟包同名的目录，用于存放相关资源文件，比如图片啥的。最后就是自动的测试脚本了，暂时不用管。三、编写publisher节点在源代码的存放目录新建一个名为publisher_member_function.py的python文件，publisher节点示例如下，importrclpy#ROS提供的python接口fromrclpy.nodeimportNode#应该是ROS节点的父类了fromstd_msgs.msgimportString#从标准信息类型库中导入字符串类型，其实就是数据类型classMinimalPublisher(Node):#定义类并继承Nodedef__init__(self):super().__init__('minimal_publisher')self.publisher_=self.create_publisher(String,'topic',10)#创建一个发布者，该发布者在一个名为'topic'（可以定义为其他名字）的主题上发布一个String类型的信息，10表示缓冲队列的大小，队列大小是一项必需的QoS（服务质量）设置，如果订阅者接收消息的速度不够快，它会限制排队消息的数量。timer_period=0.5#secondsself.timer=self.create_timer(timer_period,self.timer_callback)#创建一个计时器，该计时器按一定时间间隔（0.5秒）调用回调函数self.timer_callbackself.i=0deftimer_callback(self):#回调函数，构造并发布信息msg=String()msg.data='HelloWorld:%d'%self.iself.publisher_.publish(msg)#发布信息（这是给订阅者发布的）self.get_logger().info('Publishing:&quot;%s&quot;'%msg.data)#同时还发布到终端consoleself.i+=1defmain(args=None):rclpy.init(args=args)#初始化节点minimal_publisher=MinimalPublisher()#实例化发布者节点rclpy.spin(minimal_publisher)#启动发布者节点#Destroythenodeexplicitly#(optional-otherwiseitwillbedoneautomatically#whenthegarbagecollectordestroysthenodeobject)minimal_publisher.destroy_node()#销毁发布者节点rclpy.shutdown()#关闭节点if__name__=='__main__':main()因为我们在发布者节点中调用了两个库：rclpy和std_msgs，这两个库是运行时需要的，数据执行依赖，所以我们打开这个包下的package.xml文件，添加以下两行，&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;&lt;exec_depend&gt;std_msgs&lt;/exec_depend&gt;然后，我们需要为程序调用定义入口，打开setup.py文件，添加如下代码，entry_points={'console_scripts':['talker=py_pubsub.publisher_member_function:main',],},其中，talker是自定义的名字，调用时如下所示，$ros2runpy_pubsubtalker四、编写subscriber节点该节点简单接受publisher发布的信息并打印到终端，importrclpyfromrclpy.nodeimportNodefromstd_msgs.msgimportStringclassMinimalSubscriber(Node):def__init__(self):super().__init__('minimal_subscriber')self.subscription=self.create_subscription(String,'topic',self.listener_callback,10)#新建订阅者，接收'topic'的数据并调用self.listener_callback函数self.subscription#preventunusedvariablewarningdeflistener_callback(self,msg):self.get_logger().info('Iheard:&quot;%s&quot;'%msg.data)#打印到终端defmain(args=None):rclpy.init(args=args)minimal_subscriber=MinimalSubscriber()rclpy.spin(minimal_subscriber)#Destroythenodeexplicitly#(optional-otherwiseitwillbedoneautomatically#whenthegarbagecollectordestroysthenodeobject)minimal_subscriber.destroy_node()rclpy.shutdown()if__name__=='__main__':main()由于它和发布者程序调用的依赖是一样的，所以我们不需要再一次添加依赖，但是需要给它添加执行入口，打开setup.py，按如下所示添加，entry_points={'console_scripts':['talker=py_pubsub.publisher_member_function:main','listener=py_pubsub.subscriber_member_function:main',],},五、编译与运行按照惯例，编译前检查一下依赖，$rosdepcinstall-i--from-pathsrc--rosdistrohumble-yrosdepc可以自定义很多参数，我们看到多少学多少，这里--rosdistrohumble是指定ROS版本，不指定也行，这是多版本混合开发才需要的。ROS2采用的编译器是colcon，回到工作空间的根目录ros2_ws，执行编译，$colconbuild--packages-selectpy_pubsub--packages-selectpy_pubsub是指定只编译py_pubsub，如果不指定，直接执行colconbuild会编译整个工作空间中所有的包。编译之后，我们会发现工作空间中多了三个目录，build/用于存放编译过程的中间文件，install/就是安装目录，就是我们把刚才的py_pubsub包安装到这里了，log/顾名思义日志文件。ros2_ws/build/install/log/src/我们要使用刚编译的py_pubsub，还需要执行以下以下命令来配置环境变量，$sourceinstall/setup.bash运行发布者节点，$ros2runpy_pubsubtalker[INFO][minimal_publisher]:Publishing:&quot;HelloWorld:0&quot;[INFO][minimal_publisher]:Publishing:&quot;HelloWorld:1&quot;[INFO][minimal_publisher]:Publishing:&quot;HelloWorld:2&quot;[INFO][minimal_publisher]:Publishing:&quot;HelloWorld:3&quot;[INFO][minimal_publisher]:Publishing:&quot;HelloWorld:4&quot;...运行订阅者节点，$ros2runpy_pubsublistener[INFO][minimal_subscriber]:Iheard:&quot;HelloWorld:10&quot;[INFO][minimal_subscriber]:Iheard:&quot;HelloWorld:11&quot;[INFO][minimal_subscriber]:Iheard:&quot;HelloWorld:12&quot;[INFO][minimal_subscriber]:Iheard:&quot;HelloWorld:13&quot;[INFO][minimal_subscriber]:Iheard:&quot;HelloWorld:14&quot;好了！至此入门ros开发第一课结束了，从开发、编译、运行整个流程都走了一遍了，下一步学习服务通信的开发！","link":"https://chenjie04.github.io/post/ros-kai-fa-bi-ji/"},{"title":"ROS2概念学习笔记（一）","content":"一、节点（Node）节点是ROS的最小执行单元，每个节点应该只负责单一一个目的。节点之间通过主题（topic）、服务（services）、动作（actions）或参数（parameters）等方式进行通信。ROS系统就是由许许多多的节点组成。一个或多个节点组成一个可执行程序（C++程序，python程序等）。一个或多个可行执程序组成一个包（package）。例如，我们通过以下命令安装我们的第一个ROS包，也就是小海龟仿真，$sudoaptupdate$sudoaptinstallros-humble-turtlesim通过以下命令查看turtlesim包有哪些可执行程序，$ros2pkgexecutablesturtlesimturtlesimdraw_squareturtlesimmimicturtlesimturtle_teleop_keyturtlesimturtlesim_node回顾我们熟悉的ROS命令：#ros2run&lt;package_name&gt;&lt;executable_name&gt;$ros2runturtlesimturtlesim_node其中，turtlesim是一个包，而turtlesim_node是一个可执行程序，也就是通过执行turtlesim包的turtlesim_node程序创建了一个/turtlesim。（怎么有点类和实例的味道？）假如，我们想查看当前系统中运行这哪些节点，$ros2nodelist$/turtlesim当前只运行了/turtlesim这一个节点。在启动另外一个控制节点，$ros2runturtlesimturtle_teleop_key#查看节点$ros2nodelist$/turtlesim/teleop_turtle我们可以通过Remapping修改节点的属性，比如我们想启动一个新的/turtlesim并将名字改为/my_turtle，$ros2runturtlesimturtlesim_node--ros-args--remap__node:=my_turtle$ros2nodelist$/my_turtle/turtlesim/teleop_turtle知道节点的名字，我们就可以通过以下命令查看节点的更多信息$ros2nodeinfo&lt;node_name&gt;$ros2nodeinfo/my_turtle$/my_turtleSubscribers:/parameter_events:rcl_interfaces/msg/ParameterEvent/turtle1/cmd_vel:geometry_msgs/msg/TwistPublishers:/parameter_events:rcl_interfaces/msg/ParameterEvent/rosout:rcl_interfaces/msg/Log/turtle1/color_sensor:turtlesim/msg/Color/turtle1/pose:turtlesim/msg/PoseServiceServers:/clear:std_srvs/srv/Empty/kill:turtlesim/srv/Kill/my_turtle/describe_parameters:rcl_interfaces/srv/DescribeParameters/my_turtle/get_parameter_types:rcl_interfaces/srv/GetParameterTypes/my_turtle/get_parameters:rcl_interfaces/srv/GetParameters/my_turtle/list_parameters:rcl_interfaces/srv/ListParameters/my_turtle/set_parameters:rcl_interfaces/srv/SetParameters/my_turtle/set_parameters_atomically:rcl_interfaces/srv/SetParametersAtomically/reset:std_srvs/srv/Empty/spawn:turtlesim/srv/Spawn/turtle1/set_pen:turtlesim/srv/SetPen/turtle1/teleport_absolute:turtlesim/srv/TeleportAbsolute/turtle1/teleport_relative:turtlesim/srv/TeleportRelativeServiceClients:ActionServers:/turtle1/rotate_absolute:turtlesim/action/RotateAbsoluteActionClients:二、主题（topic）话题是单向消息发送/接收的一种通信方式，一个节点发送一个接受就完事。不过主题这种通信方式是一个一对一、一对多、多对一、多对多的。需要引入另外一个信息（Message）的概念，信息定义了主题的数据格式。吐槽一下：主题和信息这两个数据可真是够怪异的，居然信息是数据格式，主题才是数据载体，按照我们的生活习惯应该反过来才对。Nodessenddataovertopicsusingmessages.例如，下面定义了小海龟姿态的信息数据格式，#turtlesim/msg/Pose.msgfloat32xfloat32yfloat32thetafloat32linear_velocityfloat32angular_velocity我们再次启动两个节点观察它们通信的主题，$ros2runturtlesimturtlesim_node$ros2runturtlesimturtle_teleop_key然后，我们就可以通过以下命令查看$ros2topiclist/parameter_events/rosout/turtle1/cmd_vel/turtle1/color_sensor/turtle1/poseros2topiclist-t会额外输出每个主题的信息类型，$ros2topiclist-t/parameter_events[rcl_interfaces/msg/ParameterEvent]/rosout[rcl_interfaces/msg/Log]/turtle1/cmd_vel[geometry_msgs/msg/Twist]/turtle1/color_sensor[turtlesim/msg/Color]/turtle1/pose[turtlesim/msg/Pose]打开rqt_graph程序查看节点和主题（取消隐藏复选框，另外不同版本的输出略有不同），为了方便debug，我们可以从终端打印主题内部的数据，#ros2topicecho&lt;topic_name&gt;$ros2topicecho/turtle1/cmd_vellinear:x:2.0y:0.0z:0.0angular:x:0.0y:0.0z:0.0---回到rqt_graph并去掉debug的复选框，如果没有去掉的话，这时候我们发现多了一个节点/_ros2cli_26646订阅/turtle1/cmd_vel这个主题。这个节点就是上面的echo命令创建的。同样，我们可以通过以下命令查看主题的相关信息，$ros2topicinfo/turtle1/cmd_velType:geometry_msgs/msg/TwistPublishercount:1Subscriptioncount:2我们知道主题对应的信息类型之后，可以查看具体数据结构，例如上面主题的信息类型为geometry_msgs/msg/Twist，表示geometry_msgs包有一个msg名为Twist，那么查看的命令为$ros2interfaceshowgeometry_msgs/msg/Twist#Thisexpressesvelocityinfreespacebrokenintoitslinearandangularparts.Vector3linearfloat64xfloat64yfloat64zVector3angularfloat64xfloat64yfloat64z知道了主题，也知道了信息类型，那么我们在debug的时候就可以通过终端直接发布主题，如，#ros2topicpub&lt;topic_name&gt;&lt;msg_type&gt;'&lt;args&gt;'$ros2topicpub--once/turtle1/cmd_velgeometry_msgs/msg/Twist&quot;{linear:{x:2.0,y:0.0,z:0.0},angular:{x:0.0,y:0.0,z:1.8}}&quot;publisher:beginninglooppublishing#1:geometry_msgs.msg.Twist(linear=geometry_msgs.msg.Vector3(x=2.0,y=0.0,z=0.0),angular=geometry_msgs.msg.Vector3(x=0.0,y=0.0,z=1.8))--once表示仅发布一条主题然后程序退出。我们也可以看到小海龟转了一个小弯，我们也可以以一定的频率持续发布主题，比如1Hz，$ros2topicpub--rate1/turtle1/cmd_velgeometry_msgs/msg/Twist&quot;{linear:{x:2.0,y:0.0,z:0.0},angular:{x:0.0,y:0.0,z:1.8}}&quot;那么小海龟就会在转圈圈，我们可以查看某个主题的发布频率，如$ros2topichz/turtle1/poseaveragerate:59.354min:0.005smax:0.027sstddev:0.00284swindow:58总结一下：主题这种通信方式就是一个节点作为发布者以一定的频率发布主题，另外一个节点作为订阅者订阅了该主题之后就可以接受这条主题，主题的内容由信息类型定义。一个节点可以作为发布者发布不同的主题，同时也可以作为订阅者订阅不同的主题。例如小海龟节点作为订阅者订阅了两条主题，同时作为发布者发布了4条主题，如下所示。$/my_turtleSubscribers:/parameter_events:rcl_interfaces/msg/ParameterEvent/turtle1/cmd_vel:geometry_msgs/msg/TwistPublishers:/parameter_events:rcl_interfaces/msg/ParameterEvent/rosout:rcl_interfaces/msg/Log/turtle1/color_sensor:turtlesim/msg/Color/turtle1/pose:turtlesim/msg/Pose三、服务（services）上面那种主题的通信方式是发布者以固定的频率持续输出数据，而订阅者只能被动接受的单向通信。显然有些功能并不需要怎么高频率的通信。服务是以请求/响应双向通信的方式，客服端有通信需求了就想服务器发起通信请求，服务器才响应并把数据发给客户端。如下图所示，服务这种通信方式，需要有一个节点作为服务端，另外一个作为客户端，服务定义了请求和响应的数据格式。服务可以是一对一也可以是一对多。同样，启动两个节点看看它们启用了哪些服务通信，$ros2runturtlesimturtlesim_node$ros2runturtlesimturtle_teleop_key查看激活的服务，$ros2servicelist-t/clear[std_srvs/srv/Empty]/kill[turtlesim/srv/Kill]/reset[std_srvs/srv/Empty]/spawn[turtlesim/srv/Spawn].../turtle1/set_pen[turtlesim/srv/SetPen]/turtle1/teleport_absolute[turtlesim/srv/TeleportAbsolute]/turtle1/teleport_relative[turtlesim/srv/TeleportRelative]...不同的服务属于不同的类型，类型正是定义了请求和响应的数据格式。单独查看某个服务的类型，$ros2servicetype/clearstd_srvs/srv/Empty查看某个类型下有哪些服务，$ros2servicefindstd_srvs/srv/Empty/clear/reset查看某个类型定义的数据格式，$ros2interfaceshowstd_srvs/srv/Empty---返回---说明该服务的请求和响应不需要发送数据。/Spawn是生成小海龟的服务，发起请求时需要给定小海龟的坐标和朝向，也可以指定小海龟的名字，服务响应之后返回小海龟的名字。$ros2interfaceshowturtlesim/srv/Spawnfloat32xfloat32yfloat32thetastringname#Optional.Auniquenamewillbecreatedandreturnedifthisisempty---stringname从终端直接发起服务请求，例如请求清空小海龟的运动的轨迹，#ros2servicecall&lt;service_name&gt;&lt;service_type&gt;&lt;arguments&gt;$ros2servicecall/clearstd_srvs/srv/Empty请求再生成一个小海龟，$ros2servicecall/spawnturtlesim/srv/Spawn&quot;{x:2,y:2,theta:0.2,name:''}&quot;requester:makingrequest:turtlesim.srv.Spawn_Request(x=2.0,y=2.0,theta=0.2,name='')response:turtlesim.srv.Spawn_Response(name='turtle2')四、动作（action）动作是构建在主题和服务之上的一种通信方式，适用于那些需要长时间的任务。想象一个例子，你的老板让你给他写一个本子（动作客户端发起目的服务请求），你回好的！（动作服务端响应目的服务），接着你的老板说本子给我吧（动作客户端发起结果服务请求），但是写本子是需要时间的啊，没办法马上给他，只能给他持续的反馈（反馈主题，动作服务端是发布者，动作客户端是订阅者）：撰写进度10%……20%……80%……100%%，然后把本子甩给老板（动作服务端响应结果服务）。动作通信大概就是这么个工作方式，结合下图再理解一下，启动节点，$ros2runturtlesimturtlesim_node$ros2runturtlesimturtle_teleop_key在启动控制节点turtle_teleop_key之后，终端出现以下提示信息，其中第二行“用G|B|V|C|D|E|R|T键旋转到绝对方向，按F键取消”这一功能就是通过动作来通信的。Usearrowkeystomovetheturtle.UseG|B|V|C|D|E|R|Tkeystorotatetoabsoluteorientations.'F'tocancelarotation.查看动作$ros2actionlist-t/turtle1/rotate_absolute[turtlesim/action/RotateAbsolute]动作名称为/turtle1/rotate_absolute，类型turtlesim/action/RotateAbsolute。查看动作信息$ros2actioninfo/turtle1/rotate_absoluteAction:/turtle1/rotate_absoluteActionclients:1/teleop_turtleActionservers:1/turtlesim查看动作通信的数据结构$ros2interfaceshowturtlesim/action/RotateAbsolute#Thedesiredheadinginradiansfloat32theta---#Theangulardisplacementinradianstothestartingpositionfloat32delta---#Theremainingrotationinradiansfloat32remaining第一部分是目的服务的数据结构，第二部分是结果服务的数据结构，第三部分是反馈主题的数据结构。从终端发送目的命令#ros2actionsend_goal&lt;action_name&gt;&lt;action_type&gt;&lt;values&gt;$ros2actionsend_goal/turtle1/rotate_absoluteturtlesim/action/RotateAbsolute&quot;{theta:-1.57}&quot;--feedbackSendinggoal:theta:-1.57GoalacceptedwithID:e6092c831f994afda92f0086f220da27Feedback:remaining:-3.1268222332000732Feedback:remaining:-3.1108222007751465…Result:delta:3.1200008392333984Goalfinishedwithstatus:SUCCEEDED五、参数（parameter）参数就是节点的配置信息，在生成节点的时候需要进行读取。启动节点，$ros2runturtlesimturtlesim_node$ros2runturtlesimturtle_teleop_key查看各个节点的参数，$ros2paramlist/teleop_turtle:qos_overrides./parameter_events.publisher.depthqos_overrides./parameter_events.publisher.durabilityqos_overrides./parameter_events.publisher.historyqos_overrides./parameter_events.publisher.reliabilityscale_angularscale_linearuse_sim_time/turtlesim:background_bbackground_gbackground_rqos_overrides./parameter_events.publisher.depthqos_overrides./parameter_events.publisher.durabilityqos_overrides./parameter_events.publisher.historyqos_overrides./parameter_events.publisher.reliabilityuse_sim_time获取特定节点的特定参数，#ros2paramget&lt;node_name&gt;&lt;parameter_name&gt;$ros2paramget/turtlesimbackground_gIntegervalueis:86设置定节点的特定参数，#ros2paramset&lt;node_name&gt;&lt;parameter_name&gt;&lt;value&gt;$ros2paramset/turtlesimbackground_r150Setparametersuccessful保存某个节点的参数到文件#ros2paramdump&lt;node_name&gt;$ros2paramdump/turtlesim&gt;turtlesim.yaml/turtlesim:ros__parameters:background_b:255background_g:86background_r:150qos_overrides:/parameter_events:publisher:depth:1000durability:volatilehistory:keep_lastreliability:reliableuse_sim_time:false从配置文件中加载参数到节点，#ros2paramload&lt;node_name&gt;&lt;parameter_file&gt;$ros2paramload/turtlesimturtlesim.yamlSetparameterbackground_bsuccessfulSetparameterbackground_gsuccessfulSetparameterbackground_rsuccessfulSetparameterqos_overrides./parameter_events.publisher.depthfailed:parameter'qos_overrides./parameter_events.publisher.depth'cannotbesetbecauseitisread-onlySetparameterqos_overrides./parameter_events.publisher.durabilityfailed:parameter'qos_overrides./parameter_events.publisher.durability'cannotbesetbecauseitisread-onlySetparameterqos_overrides./parameter_events.publisher.historyfailed:parameter'qos_overrides./parameter_events.publisher.history'cannotbesetbecauseitisread-onlySetparameterqos_overrides./parameter_events.publisher.reliabilityfailed:parameter'qos_overrides./parameter_events.publisher.reliability'cannotbesetbecauseitisread-onlySetparameteruse_sim_timesuccessful在节点启动时加载配置文件中的参数，#ros2run&lt;package_name&gt;&lt;executable_name&gt;--ros-args--params-file&lt;file_name&gt;$ros2runturtlesimturtlesim_node--ros-args--params-fileturtlesim.yaml六、查阅日志rqt_console是一个日志查看工具,$ros2runrqt_consolerqt_console七、记录并回放实验数据ROS是通过bag这个包记录并回放实验数据的，不过好像只能记录主题数据。安装bag包$sudoapt-getinstallros-humble-ros2bag\\ros-humble-rosbag2-storage-default-plugins启动节点$ros2runturtlesimturtlesim_node$ros2runturtlesimturtle_teleop_key控制节点中使用方向键移动小海龟（Usearrowkeystomovetheturtle）就是通过主题/turtle1/cmd_vel实现的，/turtle1/cmd_vel的信息类型为geometry_msgs/msg/Twist，数据格式如下，linear:x:2.0y:0.0z:0.0angular:x:0.0y:0.0z:0.0---解释：Considerthatyouareinsomespace,thenthereare3axes-x,yandzwhicharemutuallyperpendiculartoeachotherandtheirpointofintersectioniscalledtheorigin(x=0,y=0,z=0).Thiscanbeaframeofreferencei.e.youcandefinevariouspointsanddirectionsw.r.t.them.Thex,y,andzinTwist.lineararethelinearvelocitiesinx,yandzdirectionsw.r.t.thatframeofreference.Similarly,thex,y,andzinTwist.angulararetheangularvelocitiesaboutthex,yandzdirectionsrespectivelyw.r.t.thesameframeofreference.Sinceyouhaveagroundrobot,mostprobablyyourangularvelocitywillbeinzi.e.robot'sturningspeed.Andyourlinearvelocitywillbemostlyinxi.e.robot'smovingstraightspeed.ThisisthecasefortheTurtlebot2atleast./turtle1/cmd_vel主题可以控制小海龟的速度和方向。同时，我们一般还会记录小海龟是姿态/turtle1/pose，记录示例，$mkdirbag_files$cdbag_files$ros2bagrecord-osubset/turtle1/cmd_vel/turtle1/pose-o指定记录名字（bag_file_name），Ctrl+C停止记录查看记录信息$ros2baginfosubsetFiles:subset.db3Bagsize:228.5KiBStorageid:sqlite3Duration:48.47sStart:Oct11201906:09:09.12(1570799349.12)EndOct11201906:09:57.60(1570799397.60)Messages:3013Topicinformation:Topic:/turtle1/cmd_vel|Type:geometry_msgs/msg/Twist|Count:9|SerializationFormat:cdrTopic:/turtle1/pose|Type:turtlesim/msg/Pose|Count:3004|SerializationFormat:cdr记录回放$ros2bagplaysubset[INFO][rosbag2_storage]:Openeddatabase'subset'.","link":"https://chenjie04.github.io/post/ros2-gai-nian-xue-xi-bi-ji/"},{"title":"Simple Baseline for Image Restoration","content":"配置文件default_scope='mmedit'save_dir='./work_dirs/'default_hooks=dict(timer=dict(type='IterTimerHook'),logger=dict(type='LoggerHook',interval=100),param_scheduler=dict(type='ParamSchedulerHook'),checkpoint=dict(type='CheckpointHook',interval=5000,out_dir='./work_dirs/',by_epoch=False,max_keep_ckpts=10,save_best='PSNR',rule='greater',save_optimizer=True),sampler_seed=dict(type='DistSamplerSeedHook'))env_cfg=dict(cudnn_benchmark=False,mp_cfg=dict(mp_start_method='fork',opencv_num_threads=4),dist_cfg=dict(backend='nccl'))log_level='INFO'log_processor=dict(type='EditLogProcessor',window_size=100,by_epoch=False)load_from=Noneresume=Falsevis_backends=[dict(type='LocalVisBackend')]visualizer=dict(type='ConcatImageVisualizer',vis_backends=[dict(type='LocalVisBackend')],fn_key='gt_path',img_keys=['gt_img','input','pred_img'],bgr2rgb=False)custom_hooks=[dict(type='BasicVisualizationHook',interval=1)]experiment_name='nafnet_c64eb11128mb1db1111_lr1e-3_400k_gopro'work_dir='./work_dirs/nafnet_c64eb11128mb1db1111_lr1e-3_400k_gopro'model=dict(type='BaseEditModel',generator=dict(type='NAFNetLocal',img_channels=3,mid_channels=64,enc_blk_nums=[1,1,1,28],middle_blk_num=1,dec_blk_nums=[1,1,1,1]),pixel_loss=dict(type='PSNRLoss'),train_cfg=dict(),test_cfg=dict(),data_preprocessor=dict(type='EditDataPreprocessor',mean=[0.0,0.0,0.0],std=[255.0,255.0,255.0]))train_pipeline=[dict(type='LoadImageFromFile',key='img'),dict(type='LoadImageFromFile',key='gt'),dict(type='SetValues',dictionary=dict(scale=1)),dict(type='Flip',keys=['img','gt'],flip_ratio=0.5,direction='horizontal'),dict(type='Flip',keys=['img','gt'],flip_ratio=0.5,direction='vertical'),dict(type='RandomTransposeHW',keys=['img','gt'],transpose_ratio=0.5),dict(type='PairedRandomCrop',gt_patch_size=256),dict(type='PackEditInputs')]val_pipeline=[dict(type='LoadImageFromFile',key='img',channel_order='rgb'),dict(type='LoadImageFromFile',key='gt',channel_order='rgb'),dict(type='PackEditInputs')]dataset_type='BasicImageDataset'train_dataloader=dict(num_workers=8,batch_size=8,persistent_workers=False,sampler=dict(type='InfiniteSampler',shuffle=True),dataset=dict(type='BasicImageDataset',metainfo=dict(dataset_type='gopro',task_name='deblur'),data_root='./data/gopro/train',data_prefix=dict(gt='sharp',img='blur'),ann_file='meta_info_gopro_train.txt',pipeline=[dict(type='LoadImageFromFile',key='img'),dict(type='LoadImageFromFile',key='gt'),dict(type='SetValues',dictionary=dict(scale=1)),dict(type='Flip',keys=['img','gt'],flip_ratio=0.5,direction='horizontal'),dict(type='Flip',keys=['img','gt'],flip_ratio=0.5,direction='vertical'),dict(type='RandomTransposeHW',keys=['img','gt'],transpose_ratio=0.5),dict(type='PairedRandomCrop',gt_patch_size=256),dict(type='PackEditInputs')]))val_dataloader=dict(num_workers=4,persistent_workers=False,drop_last=False,sampler=dict(type='DefaultSampler',shuffle=False),dataset=dict(type='BasicImageDataset',metainfo=dict(dataset_type='gopro',task_name='deblur'),data_root='./data/gopro/test',ann_file='meta_info_gopro_test.txt',data_prefix=dict(gt='sharp',img='blur'),pipeline=[dict(type='LoadImageFromFile',key='img',channel_order='rgb'),dict(type='LoadImageFromFile',key='gt',channel_order='rgb'),dict(type='PackEditInputs')]))test_dataloader=dict(num_workers=4,persistent_workers=False,drop_last=False,sampler=dict(type='DefaultSampler',shuffle=False),dataset=dict(type='BasicImageDataset',metainfo=dict(dataset_type='gopro',task_name='deblur'),data_root='./data/gopro/test',ann_file='meta_info_gopro_test.txt',data_prefix=dict(gt='sharp',img='blur'),pipeline=[dict(type='LoadImageFromFile',key='img',channel_order='rgb'),dict(type='LoadImageFromFile',key='gt',channel_order='rgb'),dict(type='PackEditInputs')]))val_evaluator=[dict(type='MAE'),dict(type='PSNR'),dict(type='SSIM')]test_evaluator=[dict(type='MAE'),dict(type='PSNR'),dict(type='SSIM')]train_cfg=dict(type='IterBasedTrainLoop',max_iters=400000,val_interval=20000)val_cfg=dict(type='EditValLoop')test_cfg=dict(type='EditTestLoop')optim_wrapper=dict(constructor='DefaultOptimWrapperConstructor',type='OptimWrapper',optimizer=dict(type='AdamW',lr=0.001,weight_decay=0.001,betas=(0.9,0.9)))param_scheduler=dict(type='CosineAnnealingLR',by_epoch=False,T_max=400000,eta_min=1e-07)randomness=dict(seed=10,diff_rank_seed=True)","link":"https://chenjie04.github.io/post/simple-baseline-for-image-restoration/"},{"title":"mmdetection-3.x学习笔记——RTMDet模型检测头源码阅读","content":"RTMDet模型的检测头同样基于YOLOXHead，采用分类分支和回归分支解耦的形式，目的就是缓解分类和回归两个任务之间的冲突。不同的是YOLOX在3个尺度特征图上都共享同一个检测头以降低参数量，但是这也会削弱模型的能力。大多数模型为了提升模型能力都不共享检测头。RTMDet选择了中间策略，不同尺度特征图只共享卷积层，配备独立的BN层、激活层以及最后特征图到预测结果的预测层。当然RTMDet也实现了全共享检测头的RTMDetHead，不过默认采用独立BN层的RTMDetSepBNHead，配置文件如下:bbox_head=dict(type=&quot;RTMDetSepBNHead&quot;,num_classes=80,in_channels=256,stacked_convs=2,feat_channels=256,anchor_generator=dict(type=&quot;MlvlPointGenerator&quot;,offset=0,strides=[8,16,32]),bbox_coder=dict(type=&quot;DistancePointBBoxCoder&quot;),loss_cls=dict(type=&quot;QualityFocalLoss&quot;,use_sigmoid=True,beta=2.0,loss_weight=1.0),loss_bbox=dict(type=&quot;GIoULoss&quot;,loss_weight=2.0),with_objectness=False,exp_on_reg=True,share_conv=True,pred_kernel_size=1,norm_cfg=dict(type=&quot;SyncBN&quot;),act_cfg=dict(type=&quot;SiLU&quot;,inplace=True),)train_cfg=(dict(assigner=dict(type=&quot;DynamicSoftLabelAssigner&quot;,topk=13),allowed_border=-1,pos_weight=-1,debug=False,),)RTMDetSepBNHead的网络结构除却参数共享设置不一致之外，RTMDetSepBNHead和YOLOXHead的结构基本一样，另外，YOLOXHead的Objectness分支在RTMDetSepBNHead被设为了可选选项，而且在目标检测的模型配置文件中都没有启用（with_objectness=False），也就是去掉Objectness分支，进一步将Head轻量化。。由上图可知，检测头分分类和回归两个分支，每个分支由2个卷积层加一个预测层，可选的Objectness分支附在回归分支上，检测头网络由def_init_layers(self)-&gt;None构建，具体代码如下：def_init_layers(self)-&gt;None:&quot;&quot;&quot;Initializelayersofthehead.&quot;&quot;&quot;conv=DepthwiseSeparableConvModuleifself.use_depthwiseelseConvModuleself.cls_convs=nn.ModuleList()#分类分支的卷积层self.reg_convs=nn.ModuleList()#回归分支的卷积层self.rtm_cls=nn.ModuleList()#分类分支的预测层self.rtm_reg=nn.ModuleList()#回归分支的预测层ifself.with_objectness:#如果启用Objectness分支self.rtm_obj=nn.ModuleList()#Objectness分支的预测层#遍历strides，也就是遍历不同尺度的特征图，RTMDetSepBNHead选择先为每个尺度的#特征图构建独立的检测头forninrange(len(self.prior_generator.strides)):cls_convs=nn.ModuleList()reg_convs=nn.ModuleList()foriinrange(self.stacked_convs):#遍历卷积层层数并构建chn=self.in_channelsifi==0elseself.feat_channelscls_convs.append(conv(chn,self.feat_channels,3,stride=1,padding=1,conv_cfg=self.conv_cfg,norm_cfg=self.norm_cfg,act_cfg=self.act_cfg,))reg_convs.append(conv(chn,self.feat_channels,3,stride=1,padding=1,conv_cfg=self.conv_cfg,norm_cfg=self.norm_cfg,act_cfg=self.act_cfg,))self.cls_convs.append(cls_convs)self.reg_convs.append(reg_convs)self.rtm_cls.append(nn.Conv2d(self.feat_channels,self.num_base_priors*self.cls_out_channels,self.pred_kernel_size,#一般设为1padding=self.pred_kernel_size//2,))self.rtm_reg.append(nn.Conv2d(self.feat_channels,self.num_base_priors*4,self.pred_kernel_size,padding=self.pred_kernel_size//2,))ifself.with_objectness:self.rtm_obj.append(nn.Conv2d(self.feat_channels,1,self.pred_kernel_size,padding=self.pred_kernel_size//2,))为每个尺度特征图构建检测头之后，采用self.share_conv控制是否共享卷积层，具体代码如下：ifself.share_conv:forninrange(len(self.prior_generator.strides)):#遍历特征图的尺度foriinrange(self.stacked_convs):#遍历卷积层#将第n个尺度对应的检测头中第i卷积模块的卷积层都指向第0个检测头中第i卷积模块的#卷积层，从而实现卷积层共享，可以通过id(self.cls_convs[n][i].conv)验证self.cls_convs[n][i].conv=self.cls_convs[0][i].convself.reg_convs[n][i].conv=self.reg_convs[0][i].convRTMDetSepBNHead采用MMCV自定义的ConvModule来构建卷积模块，其为每个网络层定义了名字，卷积层为conv，BN层为bn，激活层为activate，如下所示，因此可以用self.cls_convs[n][i].conv的方式获取该卷积模块的卷积层，(0):ConvModule(#params:0.15M,#flops:1.24G,#acts:1.08M(conv):Conv2d(128,128,kernel_size=(3,3),stride=(1,1),padding=(1,1),bias=False#params:0.15M,#flops:1.24G,#acts:1.08M)(bn):_BatchNormXd(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True#params:0.26K,#flops:1.64M,#acts:0)(activate):SiLU(inplace=True))1、RTMDetSepBNHead的输出由上面YOLOXHead的图，我们可以知道，YOLOX为每个尺度的特征图生成一个H×W×CH\\timesW\\timesCH×W×C的分类预测图，其中HHH和WWW为特征图的高和宽，CCC为目标类别总数，当然这里忽略了batchsizes，而且在采用卷积层为预测层的实现中实际输出维度为B×C×H×WB\\timesC\\timesH\\timesWB×C×H×W。另外，生成的回归分支预测图为H×W×4H\\timesW\\times4H×W×4，objectness分支的预测图为H×W×1H\\timesW\\times1H×W×1。回到RTMDetSepBNHead中，其与YOLOXHead的区别在于：（1）引入self.num_base_priors，可以控制模型为特征图的每个网格点预测多个边界框（2）在启用objectness分支时，将objectness预测和分类预测合并所有RTMDetSepBNHead每个尺度的特征图只有两个输出，所有的尺度的输出再组成元组，具体参考下面的注释，需要注意一点就是回归的边界框为[tl_x,tl_y,br_x,br_y][tl\\_x,tl\\_y,br\\_x,br\\_y][tl_x,tl_y,br_x,br_y]格式，结合采用的编解码过程（mmdet.DistancePointBBoxCoder）来看，这并不是左上角和右下角坐标，而是中心点到4边的距离(top,bottom,left,right)，需要经过解码过程才得到左上角和右下角坐标。defforward(self,feats:Tuple[Tensor,...])-&gt;tuple:&quot;&quot;&quot;Forwardfeaturesfromtheupstreamnetwork.Args:feats(tuple[Tensor]):Featuresfromtheupstreamnetwork,eachisa4D-tensor.Returns:tuple:Usuallyatupleofclassificationscoresandbboxprediction-cls_scores(tuple[Tensor]):Classificationscoresforallscalelevels,eachisa4D-tensor,thechannelsnumberisnum_anchors*num_classes.BoxscoresforeachscalelevelHasshape(N,num_anchors*num_classes,H,W)-bbox_preds(tuple[Tensor]):Boxenergies/deltasforallscalelevels,eachisa4D-tensor,thechannelsnumberisnum_anchors*4.Decodedboxforeachscalelevelwithshape(N,num_anchors*4,H,W)in[tl_x,tl_y,br_x,br_y]format.&quot;&quot;&quot;cls_scores=[]bbox_preds=[]#遍历所有尺度特征图foridx,(x,stride)inenumerate(zip(feats,self.prior_generator.strides)):cls_feat=xreg_feat=x#分类分支forcls_layerinself.cls_convs[idx]:cls_feat=cls_layer(cls_feat)cls_score=self.rtm_cls[idx](cls_feat)#分类预测输出#回归分支forreg_layerinself.reg_convs[idx]:reg_feat=reg_layer(reg_feat)#objectness分支ifself.with_objectness:objectness=self.rtm_obj[idx](reg_feat)#将objectness预测和分类预测合并#这应该是源于TOOD（https://arxiv.org/abs/2108.07755v3）#将分类分数以及IOU相乘计算Cost矩阵进行标签匹配#sigmoid_geometric_mean(cls_score,objectness)实现两个sigmoid连续相乘#即z=(x_sigmoid*y_sigmoid).sqrt()#inverse_sigmoid()即sigmoid的反函数cls_score=inverse_sigmoid(sigmoid_geometric_mean(cls_score,objectness))ifself.exp_on_reg:#bbox的两种解码方式reg_dist=self.rtm_reg[idx](reg_feat).exp()*stride[0]else:reg_dist=self.rtm_reg[idx](reg_feat)*stride[0]cls_scores.append(cls_score)bbox_preds.append(reg_dist)returntuple(cls_scores),tuple(bbox_preds)2、获得points先验获得points先验,其实就是获取特征图每个点在原图的坐标，RTMDet采用的是MlvlPointGenerator，anchor_generator=dict(type=&quot;MlvlPointGenerator&quot;,offset=0,strides=[8,16,32]),通过MlvlPointGenerator.grid_priors()获取先验，计算过程很简单，公式为：(xorigin,yorigin)=(xfeat×stride,yfeat×stride)(x_{origin},y_{origin})=(x_{feat}\\timesstride,y_{feat}\\timesstride)(xorigin​,yorigin​)=(xfeat​×stride,yfeat​×stride)调用代码：multi_level_anchors=self.prior_generator.grid_priors(featmap_sizes,device=device,with_stride=True)RTMDet在3个尺度的特征图上进行检测，那么multi_level_anchors的维度应该类似于[(bs,h1∗w1,4),(bs,h2∗w2,4),(bs,h3∗w3,4)][(bs,h1*w1,4),(bs,h2*w2,4),(bs,h3*w3,4)][(bs,h1∗w1,4),(bs,h2∗w2,4),(bs,h3∗w3,4)]defgrid_priors()函数代码defgrid_priors(self,featmap_sizes:List[Tuple],dtype:torch.dtype=torch.float32,device:DeviceType='cuda',with_stride:bool=False)-&gt;List[Tensor]:&quot;&quot;&quot;Generategridpointsofmultiplefeaturelevels.Args:featmap_sizes(list[tuple]):Listoffeaturemapsizesinmultiplefeaturelevels,eachsizearrangeasas(h,w).例：[(8,8),(16,16),(32,32)]dtype(:obj:`dtype`):Dtypeofpriors.Defaultstotorch.float32.device(str|torch.device):Thedevicewheretheanchorswillbeputon.with_stride(bool):Whethertoconcatenatethestridetothelastdimensionofpoints.Return:list[torch.Tensor]:Pointsofmultiplefeaturelevels.Thesizesofeachtensorshouldbe(N,2)whenwithstrideis``False``,whereN=width*height,widthandheightarethesizesofthecorrespondingfeaturelevel,andthelastdimension2represent(coord_x,coord_y),otherwisetheshapeshouldbe(N,4),andthelastdimension4represent(coord_x,coord_y,stride_w,stride_h).&quot;&quot;&quot;assertself.num_levels==len(featmap_sizes)multi_level_priors=[]foriinrange(self.num_levels):priors=self.single_level_grid_priors(featmap_sizes[i],level_idx=i,dtype=dtype,device=device,with_stride=with_stride)multi_level_priors.append(priors)returnmulti_level_priorsdefsingle_level_grid_priors(self,featmap_size:Tuple[int],level_idx:int,dtype:torch.dtype=torch.float32,device:DeviceType='cuda',with_stride:bool=False)-&gt;Tensor:&quot;&quot;&quot;GenerategridPointsofasinglelevel.Note:Thisfunctionisusuallycalledbymethod``self.grid_priors``.Args:featmap_size(tuple[int]):Sizeofthefeaturemaps,arrangeas(h,w).level_idx(int):Theindexofcorrespondingfeaturemaplevel.dtype(:obj:`dtype`):Dtypeofpriors.Defaultstotorch.float32.device(str|torch.device):Thedevicethetensorwillbeputon.Defaultsto'cuda'.with_stride(bool):Concatenatethestridetothelastdimensionofpoints.Return:Tensor:Pointsofsinglefeaturelevels.Theshapeoftensorshouldbe(N,2)whenwithstrideis``False``,whereN=width*height,widthandheightarethesizesofthecorrespondingfeaturelevel,andthelastdimension2represent(coord_x,coord_y),otherwisetheshapeshouldbe(N,4),andthelastdimension4represent(coord_x,coord_y,stride_w,stride_h).&quot;&quot;&quot;feat_h,feat_w=featmap_sizestride_w,stride_h=self.strides[level_idx]#生成一个行状为(0,feat_w)的列表，乘以stride，映射回原图shift_x=(torch.arange(0,feat_w,device=device)+self.offset)*stride_w#keepfeatmap_sizeasTensorinsteadofint,sothatwe#canconverttoONNXcorrectlyshift_x=shift_x.to(dtype)shift_y=(torch.arange(0,feat_h,device=device)+self.offset)*stride_h#keepfeatmap_sizeasTensorinsteadofint,sothatwe#canconverttoONNXcorrectlyshift_y=shift_y.to(dtype)#获得网格点，就是每个anchor的中心点在原图的坐标shift_xx,shift_yy=self._meshgrid(shift_x,shift_y)ifnotwith_stride:shifts=torch.stack([shift_xx,shift_yy],dim=-1)else:#把strides带上#use`shape[0]`insteadof`len(shift_xx)`forONNXexportstride_w=shift_xx.new_full((shift_xx.shape[0],),stride_w).to(dtype)stride_h=shift_xx.new_full((shift_yy.shape[0],),stride_h).to(dtype)shifts=torch.stack([shift_xx,shift_yy,stride_w,stride_h],dim=-1)all_points=shifts.to(device)returnall_pointsdef_meshgrid(self,x:Tensor,y:Tensor,row_major:bool=True)-&gt;Tuple[Tensor,Tensor]:yy,xx=torch.meshgrid(y,x)ifrow_major:#warning.flatten()wouldcauseerrorinONNXexporting#havetousereshapeherereturnxx.reshape(-1),yy.reshape(-1)else:returnyy.reshape(-1),xx.reshape(-1)valid_flags（个人理解，还不确定是否准确）由于在pytorch的dataloader组装batch时，也就是调用collate_fn函数时，填充了大量黑边，所以在黑边上的point不用计算loss，可以忽略，节省算力。因此valid_flags返回有效的point索引。我认为这一步只能忽略组batch时添加的padding，而在数据增广流程中调用的Pad类添加的padding是无法去掉的。所以在RTMDet中由于在数据增广流程中调用Pad类将每张图片padding成(640,640)大小，那么组batch时就不用padding了，那么这里的每个points先验都是有效的。collate_fn(Callable,optional)–mergesalistofsamplestoformamini-batchofTensor(s).Usedwhenusingbatchedloadingfromamap-styledataset.defvalid_flags(self,featmap_sizes:List[Tuple[int,int]],pad_shape:Tuple[int],device:DeviceType='cuda')-&gt;List[Tensor]:#pad_shape是有效的特征图大小，是指Pad后的size，collate_fn之前&quot;&quot;&quot;Generatevalidflagsofpointsofmultiplefeaturelevels.Args:featmap_sizes(list(tuple)):Listoffeaturemapsizesinmultiplefeaturelevels,eachsizearrangeasas(h,w).pad_shape(tuple(int)):Thepaddedshapeoftheimage,arrangeas(h,w).device(str|torch.device):Thedevicewheretheanchorswillbeputon.Return:list(torch.Tensor):Validflagsofpointsofmultiplelevels.&quot;&quot;&quot;assertself.num_levels==len(featmap_sizes)multi_level_flags=[]foriinrange(self.num_levels):point_stride=self.strides[i]feat_h,feat_w=featmap_sizes[i]h,w=pad_shape[:2]#获得有效的特征图的高和宽valid_feat_h=min(int(np.ceil(h/point_stride[1])),feat_h)valid_feat_w=min(int(np.ceil(w/point_stride[0])),feat_w)flags=self.single_level_valid_flags((feat_h,feat_w),(valid_feat_h,valid_feat_w),device=device)multi_level_flags.append(flags)returnmulti_level_flagsdefsingle_level_valid_flags(self,featmap_size:Tuple[int,int],valid_size:Tuple[int,int],device:DeviceType='cuda')-&gt;Tensor:&quot;&quot;&quot;Generatethevalidflagsofpointsofasinglefeaturemap.Args:featmap_size(tuple[int]):Thesizeoffeaturemaps,arrangeasas(h,w).valid_size(tuple[int]):Thevalidsizeofthefeaturemaps.Thesizearrangeasas(h,w).device(str|torch.device):Thedevicewheretheflagswillbeputon.Defaultsto'cuda'.Returns:torch.Tensor:Thevalidflagsofeachpointsinasinglelevel\\featuremap.&quot;&quot;&quot;feat_h,feat_w=featmap_sizevalid_h,valid_w=valid_sizeassertvalid_h&lt;=feat_handvalid_w&lt;=feat_w#有效位置设置为1，否则为0valid_x=torch.zeros(feat_w,dtype=torch.bool,device=device)valid_y=torch.zeros(feat_h,dtype=torch.bool,device=device)valid_x[:valid_w]=1valid_y[:valid_h]=1valid_xx,valid_yy=self._meshgrid(valid_x,valid_y)valid=valid_xx&amp;valid_yyreturnvalid3、将分类分支的预测结果展成以为并将同一张图片的预测结果串联起来flatten_cls_scores=torch.cat([cls_score.permute(0,2,3,1).reshape(num_imgs,-1,self.cls_out_channels)forcls_scoreincls_scores],1)此时flatten_cls_scores的维度应该是[numimgs,numpriors,numclasses][num_imgs,num_priors,num_classes][numi​mgs,nump​riors,numc​lasses]，num_imgs就是batch_size，num_priors应该等于3张特征图维度相加，即(h1∗w1+h2∗w2+h3∗w3)∗self.numbasepriors(h1*w1+h2*w2+h3*w3)*self.num_base_priors(h1∗w1+h2∗w2+h3∗w3)∗self.numb​asep​riors4、将回归分支预测的中心点到四边的距离解码成bbox并将同一张图片的预测结果串联起来decoded_bboxes=[]foranchor,bbox_predinzip(anchor_list[0],bbox_preds):anchor=anchor.reshape(-1,4)bbox_pred=bbox_pred.permute(0,2,3,1).reshape(num_imgs,-1,4)bbox_pred=distance2bbox(anchor,bbox_pred)decoded_bboxes.append(bbox_pred)flatten_bboxes=torch.cat(decoded_bboxes,1)RTNDet采用的编解码器为DistancePointBBoxCoder，bbox_coder=dict(type=&quot;DistancePointBBoxCoder&quot;)首先来看一下从预测距离转化成真实的边界框的左上坐标和右下坐标的解码过程。defdistance2bbox(points:Tensor,distance:Tensor,....)-&gt;Tensor:&quot;&quot;&quot;Decodedistancepredictiontoboundingbox.Args:points(Tensor):Shape(B,N,2)or(N,2).网格中心点坐标distance(Tensor):Distancefromthegivenpointto4boundaries(left,top,right,bottom).Shape(B,N,4)or(N,4)预测的中心点到边界框四边的距离Returns:Tensor:Boxeswithshape(N,4)or(B,N,4)&quot;&quot;&quot;x1=points[...,0]-distance[...,0]#中心点x坐标-左边距离=边界框左上角x坐标y1=points[...,1]-distance[...,1]#中心点y坐标-上边距离=边界框左上角y坐标x2=points[...,0]+distance[...,2]#中心点x坐标-右边距离=边界框右下角x坐标y2=points[...,1]+distance[...,3]#中心点y坐标-下边距离=边界框右下角y坐标bboxes=torch.stack([x1,y1,x2,y2],-1)......returnbboxes将gtbboxes(x1,y1,x2,y2)编码为(top,bottom,left,right)的编码过程，defbbox2distance(points:Tensor,bbox:Tensor,.......)-&gt;Tensor:&quot;&quot;&quot;Decodeboundingboxbasedondistances.Args:points(Tensor):Shape(n,2)or(b,n,2),[x,y].网格中心点坐标bbox(Tensor):Shape(n,4)or(b,n,4),&quot;xyxy&quot;format真实值GroundTruth的左上角和右下角坐标......Returns:Tensor:Decodeddistances.&quot;&quot;&quot;left=points[...,0]-bbox[...,0]#中心点x坐标-边界框左上角x坐标=左边距离top=points[...,1]-bbox[...,1]#中心点y坐标-边界框左上角y坐标=上边距离right=bbox[...,2]-points[...,0]#边界框右下角x坐标-中心点x坐标=右边距离bottom=bbox[...,3]-points[...,1]#边界框右下角y坐标-中心点y坐标=右边距离.....returntorch.stack([left,top,right,bottom],-1)5、将展开后的预测结果、points（anchors）先验、GroundTruth等传入self.get_targets()准备进行正负样本分配cls_reg_targets=self.get_targets(flatten_cls_scores,flatten_bboxes,anchor_list,valid_flag_list,batch_gt_instances,batch_img_metas,batch_gt_instances_ignore=batch_gt_instances_ignore)注意这里是整个batch的数据传入self.get_targets()函数的，而且anchors_list的维度还是[(bs,h1∗w1,4),(bs,h2∗w2,4),(bs,h3∗w3,4)][(bs,h1*w1,4),(bs,h2*w2,4),(bs,h3*w3,4)][(bs,h1∗w1,4),(bs,h2∗w2,4),(bs,h3∗w3,4)]，所以得进行一维展开，defget_targets(self,cls_scores:Tensor,bbox_preds:Tensor,anchor_list:List[List[Tensor]],valid_flag_list:List[List[Tensor]],batch_gt_instances:InstanceList,batch_img_metas:List[dict],batch_gt_instances_ignore:OptInstanceList=None,unmap_outputs=True):num_imgs=len(batch_img_metas)assertlen(anchor_list)==len(valid_flag_list)==num_imgs#anchornumberofmultilevelsnum_level_anchors=[anchors.size(0)foranchorsinanchor_list[0]]#concatalllevelanchorsandflagstoasingletensorforiinrange(num_imgs):assertlen(anchor_list[i])==len(valid_flag_list[i])anchor_list[i]=torch.cat(anchor_list[i])valid_flag_list[i]=torch.cat(valid_flag_list[i])........对gt_ignore处理一下#computetargetsforeachimageifbatch_gt_instances_ignoreisNone:batch_gt_instances_ignore=[None]*num_imgs关于目标检测中gt_ignore的简单理解：目标过小，过于模糊当做ignore属性处理，训练时，给样本-1类别（背景为0类别）。评测时候需要考虑ignore样本，输出不算错。在有的模型的正负样本分配过程中，ignore区域使用是在一张图片挑选完正样本的时候，剩下的负样本按理来说是正常的从中random抽取，但是对于和ignore区域交叉大于0.5的区域是不能作为负样本的，因为这些区域不是没有人的区域，所以说ignore区域是人，但是这个人的遮挡太多了，所以要ignore，所以对于图片上的人啊，led里面的人是不能作为ignore区域的，因为要抑制掉！！！在self.get_targets()函数内部调用self._get_targets_single进行单张图片的正负样本分配(all_anchors,all_labels,all_label_weights,all_bbox_targets,all_assign_metrics,sampling_results_list)=multi_apply(self._get_targets_single,cls_scores.detach(),bbox_preds.detach(),anchor_list,valid_flag_list,batch_gt_instances,batch_img_metas,batch_gt_instances_ignore,unmap_outputs=unmap_outputs)6、在self._get_targets_single()函数内部，去掉超出图片范围的anchors，对模型的预测结果和anchors一起用抽象数据接口InstanceData封装，然后送入样本分配器进行正负样本的分配这里应该是要处理数据增广流程中Pad流程中引入的padding了，这些padding区域的anchors也是无效的。inside_flags=anchor_inside_flags(flat_anchors,valid_flags,img_meta['img_shape'][:2],#这里获取图像的真实范围self.train_cfg['allowed_border'])#是否允许超出一定边界ifnotinside_flags.any():return(None,)*7#assigngtandsampleanchorsanchors=flat_anchors[inside_flags,:]defanchor_inside_flags(flat_anchors:Tensor,valid_flags:Tensor,img_shape:Tuple[int],allowed_border:int=0)-&gt;Tensor:img_h,img_w=img_shape[:2]ifallowed_border&gt;=0:ifisinstance(flat_anchors,BaseBoxes):inside_flags=valid_flags&amp;\\flat_anchors.is_inside([img_h,img_w],all_inside=True,allowed_border=allowed_border)#flat_anchors.is_inside()是BaseBoxes类中找出在图像范围内anchors的方法else:inside_flags=valid_flags&amp;\\(flat_anchors[:,0]&gt;=-allowed_border)&amp;\\(flat_anchors[:,1]&gt;=-allowed_border)&amp;\\(flat_anchors[:,2]&lt;img_w+allowed_border)&amp;\\(flat_anchors[:,3]&lt;img_h+allowed_border)else:inside_flags=valid_flagsreturninside_flags去除无效anchors后，对模型的预测结果和anchors一起用抽象数据接口InstanceData封装，然后送入样本分配器进行正负样本的分配。pred_instances=InstanceData(scores=cls_scores[inside_flags,:],bboxes=bbox_preds[inside_flags,:],priors=anchors)assign_result=self.assigner.assign(pred_instances,gt_instances,gt_instances_ignore)关于抽象数据接口MMEngine定义了数据基类BaseDataElement，中存在两种类型的数据，一种是data类型，如标注框、框的标签、和实例掩码等；另一种是metainfo类型，包含数据的元信息以确保数据的完整性，如img_shape,img_id等数据所在图片的一些基本信息，方便可视化等情况下对数据进行恢复和使用。InstanceData在BaseDataElement的基础上，对data存储的数据做了限制，即要求存储在data中的数据的长度一致。比如在目标检测中,假设一张图像中有N个目标(instance)，可以将图像的所有边界框(bbox)，类别(label)等存储在InstanceData中,InstanceData的bbox和label的长度相同。基于上述假定对InstanceData进行了扩展，包括：对InstanceData中data所存储的数据进行了长度校验data部分支持类字典访问和设置它的属性支持基础索引，切片以及高级索引功能支持具有相同的key但是不同InstanceData的拼接功能。这些扩展功能除了支持基础的数据结构，比如torch.tensor,numpy.dnarray,list,str,tuple,也可以是自定义的数据结构，只要自定义数据结构实现了__len__,__getitem__andcat.详情查阅MMEngine抽象数据接口文档，示例如下：&lt;InstanceData(METAINFORMATIONpad_shape:(800,1216,3)img_shape:(800,1196,3)DATAFIELDScls_scores:tensor([0.8000,0.7000])bbox_preds:tensor([[0.6576,0.5435,0.5253,0.8273],[0.4533,0.6848,0.7230,0.9279]])anchors:tensor([[0.6566,0.1254,0.5253,0.8273],[0.4533,0.6848,0.7230,0.9279]]))at0x7f9f339f8ca0&gt;7、DynamicSoftLabelAssigner正负样本分配RTMDet基于YOLOX的SimOTA进行了改进，提出了DynamicSoftLabelAssigner,正负样本的分配主要由该类完成。assigner=dict(type=&quot;DynamicSoftLabelAssigner&quot;,topk=13)@TASK_UTILS.register_module()classDynamicSoftLabelAssigner(BaseAssigner):&quot;&quot;&quot;Computesmatchingbetweenpredictionsandgroundtruthwithdynamicsoftlabelassignment.Args:soft_center_radius(float):Radiusofthesoftcenterprior.Defaultsto3.0.topk(int):Selecttop-kpredictionstocalculatedynamickbestmatchesforeachgt.Defaultsto13.iou_weight(float):Thescalefactorofioucost.Defaultsto3.0.iou_calculator(ConfigType):ConfigofoverlapsCalculator.Defaultstodict(type='BboxOverlaps2D').&quot;&quot;&quot;def__init__(self,soft_center_radius:float=3.0,topk:int=13,iou_weight:float=3.0,iou_calculator:ConfigType=dict(type='BboxOverlaps2D'))-&gt;None:self.soft_center_radius=soft_center_radiusself.topk=topkself.iou_weight=iou_weightself.iou_calculator=TASK_UTILS.build(iou_calculator)defassign(self,pred_instances:InstanceData,gt_instances:InstanceData,gt_instances_ignore:Optional[InstanceData]=None,**kwargs)-&gt;AssignResult:&quot;&quot;&quot;Assigngttopriors.Args:pred_instances(:obj:`InstanceData`):Instancesofmodelpredictions.Itincludes``priors``,andthepriorscanbeanchorsorpoints,orthebboxespredictedbythepreviousstage,hasshape(n,4).Thebboxespredictedbythecurrentmodelorstagewillbenamed``bboxes``,``labels``,and``scores``,thesameasthe``InstanceData``inotherplaces.gt_instances(:obj:`InstanceData`):Groundtruthofinstanceannotations.Itusuallyincludes``bboxes``,withshape(k,4),and``labels``,withshape(k,).gt_instances_ignore(:obj:`InstanceData`,optional):Instancestobeignoredduringtraining.Itincludes``bboxes``attributedatathatisignoredduringtrainingandtesting.DefaultstoNone.Returns:obj:`AssignResult`:Theassignedresult.&quot;&quot;&quot;gt_bboxes=gt_instances.bboxesgt_labels=gt_instances.labelsnum_gt=gt_bboxes.size(0)decoded_bboxes=pred_instances.bboxespred_scores=pred_instances.scorespriors=pred_instances.priorsnum_bboxes=decoded_bboxes.size(0)#assign0bydefault#每个预测的bbox只允许匹配一个GroundTruthassigned_gt_inds=decoded_bboxes.new_full((num_bboxes,),0,dtype=torch.long)#加入没有GroundTruth或者预测结果为空，则返回空的分配结果ifnum_gt==0ornum_bboxes==0:#Nogroundtruthorboxes,returnemptyassignmentmax_overlaps=decoded_bboxes.new_zeros((num_bboxes,))ifnum_gt==0:#Notruth,assigneverythingtobackgroundassigned_gt_inds[:]=0assigned_labels=decoded_bboxes.new_full((num_bboxes,),-1,dtype=torch.long)returnAssignResult(num_gt,assigned_gt_inds,max_overlaps,labels=assigned_labels)#找出中心点在GroundTruth内的priorsprior_center=priors[:,:2]#(n,2)ifisinstance(gt_bboxes,BaseBoxes):is_in_gts=gt_bboxes.find_inside_points(prior_center)else:#Tensorboxeswillbetreatedashorizontalboxesbydefaults#prior_center[:,None].shape:(num_priors,1,2)#gt_bboxes[:,:2].shape:(num_gt,2)#两者通过广播机制实现相减lt_=prior_center[:,None]-gt_bboxes[:,:2]#中心点到每个真值框左边和上边的距离，维度为(num_priors,num_gt,2)rb_=gt_bboxes[:,2:]-prior_center[:,None]#中心点到每个真值框右边和下边的距离，维度为(num_priors,num_gt,2)deltas=torch.cat([lt_,rb_],dim=-1)is_in_gts=deltas.min(dim=-1).values&gt;0#中心点到每个真值框四边的距离，维度为(num_priors,num_gt,4)，只有四边距离的最小值都大于0，才能说该prior在该真值框内，最后得到的is_in_gts的维度为(num_priors,num_gt)valid_mask=is_in_gts.sum(dim=1)&gt;0#可能存在真值框重叠的情况，即一个prior在多个真值框内，所以对is_in_gts的第一个维度求和大于0的中心点在GroundTruth内的priors#取得中心点在GroundTruth内的有效预测结果valid_decoded_bbox=decoded_bboxes[valid_mask]valid_pred_scores=pred_scores[valid_mask]num_valid=valid_decoded_bbox.size(0)#如果所有预测结果的中心点都不在GroundTruth内（是指没有GroundTruth或预测的bboxes）返回空的分配结果ifnum_valid==0:#Nogroundtruthorboxes,returnemptyassignmentmax_overlaps=decoded_bboxes.new_zeros((num_bboxes,))assigned_labels=decoded_bboxes.new_full((num_bboxes,),-1,dtype=torch.long)returnAssignResult(num_gt,assigned_gt_inds,max_overlaps,labels=assigned_labels)ifhasattr(gt_instances,'masks'):gt_center=center_of_mass(gt_instances.masks,eps=EPS)#实例分割的？elifisinstance(gt_bboxes,BaseBoxes):gt_center=gt_bboxes.centerselse:#Tensorboxeswillbetreatedashorizontalboxesbydefaultsgt_center=(gt_bboxes[:,:2]+gt_bboxes[:,2:])/2.0#（左上角坐标+右下角坐标）/2=中心点坐标valid_prior=priors[valid_mask]#过滤无效的先验，假设有效先验的数量为num_val，则valid_prior维度为(num_val,4)strides=valid_prior[:,2]#进而获取有效先验对应的strides,维度为(num_val,1)distance=(valid_prior[:,None,:2]-gt_center[None,:,:]).pow(2).sum(-1).sqrt()/strides[:,None]#有效先验中心点到GroundTruth中心点的距离，维度应该是(num_val,num_gt)soft_center_prior=torch.pow(10,distance-self.soft_center_radius)pairwise_ious=self.iou_calculator(valid_decoded_bbox,gt_bboxes)#维度应该是(num_val,num_gt)iou_cost=-torch.log(pairwise_ious+EPS)*self.iou_weight#(num_val,num_gt)#pred_scores.shape[-1]是目标类别总数#F.one_hot(gt_labels.to(torch.int64),pred_scores.shape[-1])得到的维度应该是(num_gt,num_class)gt_onehot_label=(F.one_hot(gt_labels.to(torch.int64),pred_scores.shape[-1]).float().unsqueeze(0).repeat(num_valid,1,1))#重复num_val次是因为每个prior需要和每个GroundTruth计算损失，此时维度为(num_val,num_gt,num_class)valid_pred_scores=valid_pred_scores.unsqueeze(1).repeat(1,num_gt,1)#维度为(num_val,num_gt,num_class)soft_label=gt_onehot_label*pairwise_ious[...,None]#预测结果和真实值之间的IoU作为软标签$Y_{soft}$scale_factor=soft_label-valid_pred_scores.sigmoid()#维度为(num_val,num_gt,num_class)对应分类代价函数的$(Y_{soft}-P)$soft_cls_cost=F.binary_cross_entropy_with_logits(valid_pred_scores,soft_label,reduction='none')*scale_factor.abs().pow(2.0)#reduction='none'表示计算交叉熵后不计算平均值，而是保持维度不变soft_cls_cost=soft_cls_cost.sum(dim=-1)#(num_val,num_gt)cost_matrix=soft_cls_cost+iou_cost+soft_center_prior#(num_val,num_gt)代价矩阵，每一行代表该预测结果相对于每个gt的分配代价matched_pred_ious,matched_gt_inds=self.dynamic_k_matching(cost_matrix,pairwise_ious,num_gt,valid_mask)#converttoAssignResultformatassigned_gt_inds[valid_mask]=matched_gt_inds+1assigned_labels=assigned_gt_inds.new_full((num_bboxes,),-1)assigned_labels[valid_mask]=gt_labels[matched_gt_inds].long()max_overlaps=assigned_gt_inds.new_full((num_bboxes,),-INF,dtype=torch.float32)max_overlaps[valid_mask]=matched_pred_iousreturnAssignResult(num_gt,assigned_gt_inds,max_overlaps,labels=assigned_labels)defdynamic_k_matching(self,cost:Tensor,pairwise_ious:Tensor,num_gt:int,valid_mask:Tensor)-&gt;Tuple[Tensor,Tensor]:&quot;&quot;&quot;UseIoUandmatchingcosttocalculatethedynamictop-kpositivetargets.SameasSimOTA.Args:cost(Tensor):Costmatrix.(num_val,num_gt)pairwise_ious(Tensor):Pairwiseioumatrix.(num_val,num_gt)num_gt(int):Numberofgt.valid_mask(Tensor):Maskforvalidbboxes.(num_priors,)其中有限先验为True，无效先验为FalseReturns:tuple:matchediousandgtindexes.&quot;&quot;&quot;#匹配矩阵初始化为0matching_matrix=torch.zeros_like(cost,dtype=torch.uint8)#selectcandidatetopkiousfordynamic-kcalculation#self.topk=13,pairwise_ious.size(0)=num_valcandidate_topk=min(self.topk,pairwise_ious.size(0))topk_ious,_=torch.topk(pairwise_ious,candidate_topk,dim=0)#沿着维度0选取topk，即为每个gt选取前topk个iou最大的bbox,维度为(topk,num_gt)#calculatedynamickforeachgt#计算每一个gt与所有bboxes前13大的iou的和取整后作为这个gt的样本数目,最少为1个,记为dynamic_ks,维度为(num_gt,)dynamic_ks=torch.clamp(topk_ious.sum(0).int(),min=1)#对于每一个gt,将其cost_matrix矩阵前`dynamic_ks小的位置作为该gt的正样本forgt_idxinrange(num_gt):_,pos_idx=torch.topk(cost[:,gt_idx],k=dynamic_ks[gt_idx],largest=False)matching_matrix[:,gt_idx][pos_idx]=1deltopk_ious,dynamic_ks,pos_idx#删除中间变量#对于某一个bbox,如果被匹配到多个gt就将与这些gts的cost_marix中最小的那个作为其labelprior_match_gt_mask=matching_matrix.sum(1)&gt;1#先验匹配gt掩码，判断维度1上的和是否大于1，找出匹配上多个gt的预测点ifprior_match_gt_mask.sum()&gt;0:cost_min,cost_argmin=torch.min(cost[prior_match_gt_mask,:],dim=1)matching_matrix[prior_match_gt_mask,:]*=0matching_matrix[prior_match_gt_mask,cost_argmin]=1#getforegroundmaskinsideboxandcenterpriorfg_mask_inboxes=matching_matrix.sum(1)&gt;0#前景掩码#更新前景掩码，在前面中心先验的前提下进一步筛选正样本#注意这个索引方式,valid_mask是一个bool类型tensor,以自己为索引会返回所有为True的位置#也就是说valid_mask的维度本来是(num_priors,),以自己为索引会返回所有为True的位置后维度自然就变成了(num_val,)valid_mask[valid_mask.clone()]=fg_mask_inboxes#该候选框匹配到哪个gtbboxmatched_gt_inds=matching_matrix[fg_mask_inboxes,:].argmax(1)#提取对应的预测点和gtbbox的ioumatched_pred_ious=(matching_matrix*pairwise_ious).sum(1)[fg_mask_inboxes]returnmatched_pred_ious,matched_gt_inds样本分配结果采用AssignResult类封装classAssignResult(util_mixins.NiceRepr):def__init__(self,num_gts:int,gt_inds:Tensor,max_overlaps:Tensor,labels:Tensor)-&gt;None:self.num_gts=num_gtsself.gt_inds=gt_indsself.max_overlaps=max_overlapsself.labels=labels#Interfaceforpossibleuser-definedpropertiesself._extra_properties={}8、正负样本采样在目标检测流程中，一眼分配完正负样本之后还会进行一个采样的过程，目的是为了让正负样本更加均衡。RTMDet采用的是伪采样器，即实际上不进行采样，直接返回正负样本的索引classPseudoSampler(BaseSampler):&quot;&quot;&quot;Apseudosamplerthatdoesnotdosamplingactually.&quot;&quot;&quot;def__init__(self,**kwargs):passdef_sample_pos(self,**kwargs):&quot;&quot;&quot;Samplepositivesamples.&quot;&quot;&quot;raiseNotImplementedErrordef_sample_neg(self,**kwargs):&quot;&quot;&quot;Samplenegativesamples.&quot;&quot;&quot;raiseNotImplementedErrordefsample(self,assign_result:AssignResult,pred_instances:InstanceData,gt_instances:InstanceData,*args,**kwargs):&quot;&quot;&quot;Directlyreturnsthepositiveandnegativeindicesofsamples.Args:assign_result(:obj:`AssignResult`):Bboxassigningresults.pred_instances(:obj:`InstanceData`):Instancesofmodelpredictions.Itincludes``priors``,andthepriorscanbeanchors,points,orbboxespredictedbythemodel,shape(n,4).gt_instances(:obj:`InstanceData`):Groundtruthofinstanceannotations.Itusuallyincludes``bboxes``and``labels``attributes.Returns::obj:`SamplingResult`:samplerresults&quot;&quot;&quot;gt_bboxes=gt_instances.bboxespriors=pred_instances.priors#torch.nonezero()的作用就是找到tensor中所有不为0的索引。pos_inds=torch.nonzero(#正样本索引assign_result.gt_inds&gt;0,as_tuple=False).squeeze(-1).unique()neg_inds=torch.nonzero(#负样本索引assign_result.gt_inds==0,as_tuple=False).squeeze(-1).unique()gt_flags=priors.new_zeros(priors.shape[0],dtype=torch.uint8)#这是干啥的？sampling_result=SamplingResult(pos_inds=pos_inds,neg_inds=neg_inds,priors=priors,gt_bboxes=gt_bboxes,assign_result=assign_result,gt_flags=gt_flags,avg_factor_with_neg=False)returnsampling_result抽样结果采用SamplingResult封装，比较重要是下面添加注释的那几项，预测结果中哪些是正样本、哪些是负样本，每个正样本负责预测哪个groundtruth，对应的类别是什么classSamplingResult(util_mixins.NiceRepr):def__init__(self,pos_inds:Tensor,neg_inds:Tensor,priors:Tensor,gt_bboxes:Tensor,assign_result:AssignResult,gt_flags:Tensor,avg_factor_with_neg:bool=True)-&gt;None:self.pos_inds=pos_indsself.neg_inds=neg_indsself.num_pos=max(pos_inds.numel(),1)self.num_neg=max(neg_inds.numel(),1)self.avg_factor_with_neg=avg_factor_with_negself.avg_factor=self.num_pos+self.num_neg\\ifavg_factor_with_negelseself.num_posself.pos_priors=priors[pos_inds]#正样本self.neg_priors=priors[neg_inds]#负样本self.pos_is_gt=gt_flags[pos_inds]self.num_gts=gt_bboxes.shape[0]self.pos_assigned_gt_inds=assign_result.gt_inds[pos_inds]-1#正样本负责预测groundtruth的索引self.pos_gt_labels=assign_result.labels[pos_inds]#该正样本应该是哪个类别box_dim=gt_bboxes.box_dimifisinstance(gt_bboxes,BaseBoxes)else4ifgt_bboxes.numel()==0:#hackforindexerrorcaseassertself.pos_assigned_gt_inds.numel()==0self.pos_gt_bboxes=gt_bboxes.view(-1,box_dim)else:iflen(gt_bboxes.shape)&lt;2:gt_bboxes=gt_bboxes.view(-1,box_dim)self.pos_gt_bboxes=gt_bboxes[self.pos_assigned_gt_inds.long()]#正样本负责预测groundtruth的bbox9、根据抽样结果构建用于计算loss的target回顾一下，我们送入正负样本分配器和抽样器的是在真实图片范围内的anchors，anchors=flat_anchors[inside_flags,:]因此需要先构建anchors对应的target，包括bbox_terget(每个anchor对应的目标bbox，正样本对应真实的bbox，负样本对应0)、目标标签labels（同样正样本对应真值的标签，负样本对应0）、标签类别的权重、用于计算QualityFocalLoss的assign_metrics，构建过程如下：num_valid_anchors=anchors.shape[0]#bbox_targets=torch.zeros_like(anchors)#初始化每个有效anchor对应的边界框目标为0labels=anchors.new_full((num_valid_anchors,),#初始化每个有效anchor对应的标签目标为目标类别数self.num_classes,dtype=torch.long)label_weights=anchors.new_zeros(num_valid_anchors,dtype=torch.float)#标签权重初始化为0assign_metrics=anchors.new_zeros(#分配指标初始化为0，指示分配质量好不好，因为RTMDet采用的分类损失函数为'QualityFocalLoss'，这是用来计算分类损失的num_valid_anchors,dtype=torch.float)pos_inds=sampling_result.pos_inds#从抽样结果中获取正样本索引neg_inds=sampling_result.neg_inds#从抽样结果中获取负样本索引iflen(pos_inds)&gt;0:#point-basedpos_bbox_targets=sampling_result.pos_gt_bboxesbbox_targets[pos_inds,:]=pos_bbox_targets#更新负责预测正样本的bboxlabels[pos_inds]=sampling_result.pos_gt_labels#更新负责预测的正样本的类别标签ifself.train_cfg['pos_weight']&lt;=0:label_weights[pos_inds]=1.0else:label_weights[pos_inds]=self.train_cfg['pos_weight']iflen(neg_inds)&gt;0:label_weights[neg_inds]=1.0#采用iou评估分配质量class_assigned_gt_inds=torch.unique(sampling_result.pos_assigned_gt_inds)forgt_indsinclass_assigned_gt_inds:gt_class_inds=pos_inds[sampling_result.pos_assigned_gt_inds==gt_inds]assign_metrics[gt_class_inds]=assign_result.max_overlaps[gt_class_inds]前面把数据增广过程中Pad加上的padding去掉了，只处理在真实图片范围内的anchors，现在把维度还原回去ifunmap_outputs:num_total_anchors=flat_anchors.size(0)anchors=unmap(anchors,num_total_anchors,inside_flags)labels=unmap(labels,num_total_anchors,inside_flags,fill=self.num_classes)label_weights=unmap(label_weights,num_total_anchors,inside_flags)bbox_targets=unmap(bbox_targets,num_total_anchors,inside_flags)assign_metrics=unmap(assign_metrics,num_total_anchors,inside_flags)返回结果return(anchors,labels,label_weights,bbox_targets,assign_metrics,sampling_result)接受返回的结果(all_anchors,all_labels,all_label_weights,all_bbox_targets,all_assign_metrics,sampling_results_list)=multi_apply(self._get_targets_single,cls_scores.detach(),bbox_preds.detach(),anchor_list,valid_flag_list,batch_gt_instances,batch_img_metas,batch_gt_instances_ignore,unmap_outputs=unmap_outputs)#novalidanchorsifany([labelsisNoneforlabelsinall_labels]):returnNone上面把每张图片所有尺度的预测串联合在一起做正负样本的分配、采样等工作，现在再把结果拆分回不同的尺度前面记录每个尺度的特征图有多少anchorsnum_level_anchors=[anchors.size(0)foranchorsinanchor_list[0]]拆分结果并返回anchors_list=images_to_levels(all_anchors,num_level_anchors)labels_list=images_to_levels(all_labels,num_level_anchors)label_weights_list=images_to_levels(all_label_weights,num_level_anchors)bbox_targets_list=images_to_levels(all_bbox_targets,num_level_anchors)assign_metrics_list=images_to_levels(all_assign_metrics,num_level_anchors)return(anchors_list,labels_list,label_weights_list,bbox_targets_list,assign_metrics_list,sampling_results_list)至此，完成了targets的构建，维度与模型的预测输出一致，10、计算loss上一步构建的targets如下：(anchor_list,labels_list,label_weights_list,bbox_targets_list,assign_metrics_list,sampling_results_list)=cls_reg_targets接着按尺度计算losslosses_cls,losses_bbox,\\cls_avg_factors,bbox_avg_factors=multi_apply(self.loss_by_feat_single,#按尺度计算losscls_scores,#分类分支的输出decoded_bboxes,#回归分支的输出#以下是构建的targetslabels_list,label_weights_list,bbox_targets_list,assign_metrics_list,self.prior_generator.strides)defloss_by_feat_single(self,cls_score:Tensor,bbox_pred:Tensor,labels:Tensor,label_weights:Tensor,bbox_targets:Tensor,assign_metrics:Tensor,stride:List[int]):&quot;&quot;&quot;Computelossofasinglescalelevel.Args:cls_score(Tensor):BoxscoresforeachscalelevelHasshape(N,num_anchors*num_classes,H,W).bbox_pred(Tensor):Decodedbboxesforeachscalelevelwithshape(N,num_anchors*4,H,W).labels(Tensor):Labelsofeachanchorswithshape(N,num_total_anchors).label_weights(Tensor):Labelweightsofeachanchorwithshape(N,num_total_anchors).bbox_targets(Tensor):BBoxregressiontargetsofeachanchorwithshape(N,num_total_anchors,4).assign_metrics(Tensor):Assignmetricswithshape(N,num_total_anchors).stride(List[int]):Downsamplestrideofthefeaturemap.Returns:dict[str,Tensor]:Adictionaryoflosscomponents.&quot;&quot;&quot;assertstride[0]==stride[1],'hstrideisnotequaltowstride!'#一维展开cls_score=cls_score.permute(0,2,3,1).reshape(-1,self.cls_out_channels).contiguous()bbox_pred=bbox_pred.reshape(-1,4)bbox_targets=bbox_targets.reshape(-1,4)labels=labels.reshape(-1)assign_metrics=assign_metrics.reshape(-1)label_weights=label_weights.reshape(-1)targets=(labels,assign_metrics)#计算分类损失loss_cls=self.loss_cls(cls_score,targets,label_weights,avg_factor=1.0)#计算回归损失#FGcat_id:[0,num_classes-1],BGcat_id:num_classes这是前景类别id的范围和北京类别的id，比如在coco数据集中共有80个类别，那么前景的类别为[0,79],背景的类别为80bg_class_ind=self.num_classespos_inds=((labels&gt;=0)&amp;(labels&lt;bg_class_ind)).nonzero().squeeze(1)#获取正样本的索引iflen(pos_inds)&gt;0:pos_bbox_targets=bbox_targets[pos_inds]pos_bbox_pred=bbox_pred[pos_inds]pos_decode_bbox_pred=pos_bbox_predpos_decode_bbox_targets=pos_bbox_targets#regressionlosspos_bbox_weight=assign_metrics[pos_inds]loss_bbox=self.loss_bbox(pos_decode_bbox_pred,pos_decode_bbox_targets,weight=pos_bbox_weight,avg_factor=1.0)else:loss_bbox=bbox_pred.sum()*0pos_bbox_weight=bbox_targets.new_tensor(0.)returnloss_cls,loss_bbox,assign_metrics.sum(),pos_bbox_weight.sum()计算分类损失QualityFocalLossloss_cls=dict(type=&quot;QualityFocalLoss&quot;,use_sigmoid=True,beta=2.0,loss_weight=1.0),@MODELS.register_module()classQualityFocalLoss(nn.Module):r&quot;&quot;&quot;QualityFocalLoss(QFL)isavariantof`GeneralizedFocalLoss:LearningQualifiedandDistributedBoundingBoxesforDenseObjectDetection&lt;https://arxiv.org/abs/2006.04388&gt;`_.Args:use_sigmoid(bool):WhethersigmoidoperationisconductedinQFL.DefaultstoTrue.beta(float):Thebetaparameterforcalculatingthemodulatingfactor.Defaultsto2.0.reduction(str):Optionsare&quot;none&quot;,&quot;mean&quot;and&quot;sum&quot;.loss_weight(float):Lossweightofcurrentloss.activated(bool,optional):Whethertheinputisactivated.IfTrue,itmeanstheinputhasbeenactivatedandcanbetreatedasprobabilities.Else,itshouldbetreatedaslogits.DefaultstoFalse.&quot;&quot;&quot;def__init__(self,use_sigmoid=True,beta=2.0,reduction='mean',loss_weight=1.0,activated=False):super(QualityFocalLoss,self).__init__()assertuse_sigmoidisTrue,'OnlysigmoidinQFLsupportednow.'self.use_sigmoid=use_sigmoidself.beta=betaself.reduction=reductionself.loss_weight=loss_weightself.activated=activateddefforward(self,pred,target,weight=None,avg_factor=None,reduction_override=None):&quot;&quot;&quot;Forwardfunction.Args:pred(torch.Tensor):Predictedjointrepresentationofclassificationandquality(IoU)estimationwithshape(N,C),Cisthenumberofclasses.target(Union(tuple([torch.Tensor]),Torch.Tensor)):Thetypeistuple,itshouldbeincludedTargetcategorylabelwithshape(N,)andtargetqualitylabelwithshape(N,).Thetypeistorch.Tensor,thetargetshouldbeone-hotformwithsoftweights.weight(torch.Tensor,optional):Theweightoflossforeachprediction.DefaultstoNone.avg_factor(int,optional):Averagefactorthatisusedtoaveragetheloss.DefaultstoNone.reduction_override(str,optional):Thereductionmethodusedtooverridetheoriginalreductionmethodoftheloss.DefaultstoNone.&quot;&quot;&quot;assertreduction_overridein(None,'none','mean','sum')reduction=(reduction_overrideifreduction_overrideelseself.reduction)ifself.use_sigmoid:ifself.activated:calculate_loss_func=quality_focal_loss_with_probelse:calculate_loss_func=quality_focal_lossifisinstance(target,torch.Tensor):#thetargetshapewith(N,C)or(N,C,...),whichmeans#thetargetisone-hotformwithsoftweights.calculate_loss_func=partial(quality_focal_loss_tensor_target,activated=self.activated)loss_cls=self.loss_weight*calculate_loss_func(pred,target,weight,beta=self.beta,reduction=reduction,avg_factor=avg_factor)else:raiseNotImplementedErrorreturnloss_cls@weighted_lossdefquality_focal_loss(pred,target,beta=2.0):r&quot;&quot;&quot;QualityFocalLoss(QFL)isfrom`GeneralizedFocalLoss:LearningQualifiedandDistributedBoundingBoxesforDenseObjectDetection&lt;https://arxiv.org/abs/2006.04388&gt;`_.Args:pred(torch.Tensor):Predictedjointrepresentationofclassificationandquality(IoU)estimationwithshape(N,C),Cisthenumberofclasses.target(tuple([torch.Tensor])):Targetcategorylabelwithshape(N,)andtargetqualitylabelwithshape(N,).beta(float):Thebetaparameterforcalculatingthemodulatingfactor.Defaultsto2.0.Returns:torch.Tensor:Losstensorwithshape(N,).&quot;&quot;&quot;assertlen(target)==2,&quot;&quot;&quot;targetforQFLmustbeatupleoftwoelements,includingcategorylabelandqualitylabel,respectively&quot;&quot;&quot;#labeldenotesthecategoryid,scoredenotesthequalityscorelabel,score=target#negativesaresupervisedby0qualityscorepred_sigmoid=pred.sigmoid()scale_factor=pred_sigmoidzerolabel=scale_factor.new_zeros(pred.shape)loss=F.binary_cross_entropy_with_logits(pred,zerolabel,reduction='none')*scale_factor.pow(beta)#FGcat_id:[0,num_classes-1],BGcat_id:num_classesbg_class_ind=pred.size(1)pos=((label&gt;=0)&amp;(label&lt;bg_class_ind)).nonzero().squeeze(1)pos_label=label[pos].long()#positivesaresupervisedbybboxquality(IoU)scorescale_factor=score[pos]-pred_sigmoid[pos,pos_label]loss[pos,pos_label]=F.binary_cross_entropy_with_logits(pred[pos,pos_label],score[pos],reduction='none')*scale_factor.abs().pow(beta)loss=loss.sum(dim=1,keepdim=False)returnloss计算回归损失GIoULossloss_bbox=dict(type=&quot;GIoULoss&quot;,loss_weight=2.0),@MODELS.register_module()classGIoULoss(nn.Module):r&quot;&quot;&quot;`GeneralizedIntersectionoverUnion:AMetricandALossforBoundingBoxRegression&lt;https://arxiv.org/abs/1902.09630&gt;`_.Args:eps(float):Epsilontoavoidlog(0).reduction(str):Optionsare&quot;none&quot;,&quot;mean&quot;and&quot;sum&quot;.loss_weight(float):Weightofloss.&quot;&quot;&quot;def__init__(self,eps:float=1e-6,reduction:str='mean',loss_weight:float=1.0)-&gt;None:super().__init__()self.eps=epsself.reduction=reductionself.loss_weight=loss_weightdefforward(self,pred:Tensor,target:Tensor,weight:Optional[Tensor]=None,avg_factor:Optional[int]=None,reduction_override:Optional[str]=None,**kwargs)-&gt;Tensor:&quot;&quot;&quot;Forwardfunction.Args:pred(Tensor):Predictedbboxesofformat(x1,y1,x2,y2),shape(n,4).target(Tensor):Thelearningtargetoftheprediction,shape(n,4).weight(Optional[Tensor],optional):Theweightoflossforeachprediction.DefaultstoNone.avg_factor(Optional[int],optional):Averagefactorthatisusedtoaveragetheloss.DefaultstoNone.reduction_override(Optional[str],optional):Thereductionmethodusedtooverridetheoriginalreductionmethodoftheloss.DefaultstoNone.Optionsare&quot;none&quot;,&quot;mean&quot;and&quot;sum&quot;.Returns:Tensor:Losstensor.&quot;&quot;&quot;ifweightisnotNoneandnottorch.any(weight&gt;0):ifpred.dim()==weight.dim()+1:weight=weight.unsqueeze(1)return(pred*weight).sum()#0assertreduction_overridein(None,'none','mean','sum')reduction=(reduction_overrideifreduction_overrideelseself.reduction)ifweightisnotNoneandweight.dim()&gt;1:#TODO:removethisinthefuture#reducetheweightofshape(n,4)to(n,)tomatchthe#giou_lossofshape(n,)assertweight.shape==pred.shapeweight=weight.mean(-1)loss=self.loss_weight*giou_loss(pred,target,weight,eps=self.eps,reduction=reduction,avg_factor=avg_factor,**kwargs)returnloss@weighted_lossdefgiou_loss(pred:Tensor,target:Tensor,eps:float=1e-7)-&gt;Tensor:r&quot;&quot;&quot;`GeneralizedIntersectionoverUnion:AMetricandALossforBoundingBoxRegression&lt;https://arxiv.org/abs/1902.09630&gt;`_.Args:pred(Tensor):Predictedbboxesofformat(x1,y1,x2,y2),shape(n,4).target(Tensor):Correspondinggtbboxes,shape(n,4).eps(float):Epsilontoavoidlog(0).Return:Tensor:Losstensor.&quot;&quot;&quot;gious=bbox_overlaps(pred,target,mode='giou',is_aligned=True,eps=eps)loss=1-giousreturnlossdefbbox_overlaps(bboxes1,bboxes2,mode='iou',is_aligned=False,eps=1e-6):&quot;&quot;&quot;Calculateoverlapbetweentwosetofbboxes.FP16Contributedbyhttps://github.com/open-mmlab/mmdetection/pull/4889Note:Assumebboxes1isMx4,bboxes2isNx4,whenmodeis'iou',therearesomenewgeneratedvariablewhencalculatingIOUusingbbox_overlapsfunction:1)is_alignedisFalsearea1:Mx1area2:Nx1lt:MxNx2rb:MxNx2wh:MxNx2overlap:MxNx1union:MxNx1ious:MxNx1Totalmemory:S=(9xNxM+N+M)*4Byte,WhenusingFP16,wecanreduce:R=(9xNxM+N+M)*4/2ByteRlargethan(N+M)*4*2isalwaystruewhenNandM&gt;=1.Obviously,N+M&lt;=N*M&lt;3*N*M,whenN&gt;=2andM&gt;=2,N+1&lt;3*N,whenNorMis1.GivenM=40(groundtruth),N=400000(threeanchorboxesinpergrid,FPN,R-CNNs),R=275MB(onetimes)Aspecialcase(densedetection),M=512(groundtruth),R=3516MB=3.43GBWhenthebatchsizeisB,reduce:BxRTherefore,CUDAmemoryrunsoutfrequently.ExperimentsonGeForceRTX2080Ti(11019MiB):|dtype|M|N|Use|Real|Ideal||:----:|:----:|:----:|:----:|:----:|:----:||FP32|512|400000|8020MiB|--|--||FP16|512|400000|4504MiB|3516MiB|3516MiB||FP32|40|400000|1540MiB|--|--||FP16|40|400000|1264MiB|276MiB|275MiB|2)is_alignedisTruearea1:Nx1area2:Nx1lt:Nx2rb:Nx2wh:Nx2overlap:Nx1union:Nx1ious:Nx1Totalmemory:S=11xN*4ByteWhenusingFP16,wecanreduce:R=11xN*4/2ByteSodothe'giou'(largethan'iou').Time-wise,FP16isgenerallyfasterthanFP32.Whengpu_assign_thrisnot-1,ittakesmoretimeoncpubutnotreducememory.There,wecanreducehalfthememoryandkeepthespeed.If``is_aligned``is``False``,thencalculatetheoverlapsbetweeneachbboxofbboxes1andbboxes2,otherwisetheoverlapsbetweeneachalignedpairofbboxes1andbboxes2.Args:bboxes1(Tensor):shape(B,m,4)in&lt;x1,y1,x2,y2&gt;formatorempty.bboxes2(Tensor):shape(B,n,4)in&lt;x1,y1,x2,y2&gt;formatorempty.Bindicatesthebatchdim,inshape(B1,B2,...,Bn).If``is_aligned``is``True``,thenmandnmustbeequal.mode(str):&quot;iou&quot;(intersectionoverunion),&quot;iof&quot;(intersectionoverforeground)or&quot;giou&quot;(generalizedintersectionoverunion).Default&quot;iou&quot;.is_aligned(bool,optional):IfTrue,thenmandnmustbeequal.DefaultFalse.eps(float,optional):Avalueaddedtothedenominatorfornumericalstability.Default1e-6.Returns:Tensor:shape(m,n)if``is_aligned``isFalseelseshape(m,)Example:&gt;&gt;&gt;bboxes1=torch.FloatTensor([&gt;&gt;&gt;[0,0,10,10],&gt;&gt;&gt;[10,10,20,20],&gt;&gt;&gt;[32,32,38,42],&gt;&gt;&gt;])&gt;&gt;&gt;bboxes2=torch.FloatTensor([&gt;&gt;&gt;[0,0,10,20],&gt;&gt;&gt;[0,10,10,19],&gt;&gt;&gt;[10,10,20,20],&gt;&gt;&gt;])&gt;&gt;&gt;overlaps=bbox_overlaps(bboxes1,bboxes2)&gt;&gt;&gt;assertoverlaps.shape==(3,3)&gt;&gt;&gt;overlaps=bbox_overlaps(bboxes1,bboxes2,is_aligned=True)&gt;&gt;&gt;assertoverlaps.shape==(3,)Example:&gt;&gt;&gt;empty=torch.empty(0,4)&gt;&gt;&gt;nonempty=torch.FloatTensor([[0,0,10,9]])&gt;&gt;&gt;asserttuple(bbox_overlaps(empty,nonempty).shape)==(0,1)&gt;&gt;&gt;asserttuple(bbox_overlaps(nonempty,empty).shape)==(1,0)&gt;&gt;&gt;asserttuple(bbox_overlaps(empty,empty).shape)==(0,0)&quot;&quot;&quot;assertmodein['iou','iof','giou'],f'Unsupportedmode{mode}'#Eithertheboxesareemptyorthelengthofboxes'lastdimensionis4assert(bboxes1.size(-1)==4orbboxes1.size(0)==0)assert(bboxes2.size(-1)==4orbboxes2.size(0)==0)#Batchdimmustbethesame#Batchdim:(B1,B2,...Bn)assertbboxes1.shape[:-2]==bboxes2.shape[:-2]batch_shape=bboxes1.shape[:-2]rows=bboxes1.size(-2)cols=bboxes2.size(-2)ifis_aligned:assertrows==colsifrows*cols==0:ifis_aligned:returnbboxes1.new(batch_shape+(rows,))else:returnbboxes1.new(batch_shape+(rows,cols))area1=(bboxes1[...,2]-bboxes1[...,0])*(bboxes1[...,3]-bboxes1[...,1])area2=(bboxes2[...,2]-bboxes2[...,0])*(bboxes2[...,3]-bboxes2[...,1])ifis_aligned:lt=torch.max(bboxes1[...,:2],bboxes2[...,:2])#[B,rows,2]rb=torch.min(bboxes1[...,2:],bboxes2[...,2:])#[B,rows,2]wh=fp16_clamp(rb-lt,min=0)overlap=wh[...,0]*wh[...,1]ifmodein['iou','giou']:union=area1+area2-overlapelse:union=area1ifmode=='giou':enclosed_lt=torch.min(bboxes1[...,:2],bboxes2[...,:2])enclosed_rb=torch.max(bboxes1[...,2:],bboxes2[...,2:])else:lt=torch.max(bboxes1[...,:,None,:2],bboxes2[...,None,:,:2])#[B,rows,cols,2]rb=torch.min(bboxes1[...,:,None,2:],bboxes2[...,None,:,2:])#[B,rows,cols,2]wh=fp16_clamp(rb-lt,min=0)overlap=wh[...,0]*wh[...,1]ifmodein['iou','giou']:union=area1[...,None]+area2[...,None,:]-overlapelse:union=area1[...,None]ifmode=='giou':enclosed_lt=torch.min(bboxes1[...,:,None,:2],bboxes2[...,None,:,:2])enclosed_rb=torch.max(bboxes1[...,:,None,2:],bboxes2[...,None,:,2:])eps=union.new_tensor([eps])union=torch.max(union,eps)ious=overlap/unionifmodein['iou','iof']:returnious#calculategiousenclose_wh=fp16_clamp(enclosed_rb-enclosed_lt,min=0)enclose_area=enclose_wh[...,0]*enclose_wh[...,1]enclose_area=torch.max(enclose_area,eps)gious=ious-(enclose_area-union)/enclose_areareturngious对不同尺度的分类损失和回归损失取均值cls_avg_factor=reduce_mean(sum(cls_avg_factors)).clamp_(min=1).item()#不同尺度的assign_metrics.sum()的均值losses_cls=list(map(lambdax:x/cls_avg_factor,losses_cls))#每个尺度的losses_cls都除以cls_avg_factorbbox_avg_factor=reduce_mean(#同上sum(bbox_avg_factors)).clamp_(min=1).item()losses_bbox=list(map(lambdax:x/bbox_avg_factor,losses_bbox))returndict(loss_cls=losses_cls,loss_bbox=losses_bbox)loss字典最终在BaseModel的deftrain_step()函数中被处理并传给优化器封装器进行模型参数的更新deftrain_step(self,data:Union[dict,tuple,list],optim_wrapper:OptimWrapper)-&gt;Dict[str,torch.Tensor]:#Enableautomaticmixedprecisiontrainingcontext.withoptim_wrapper.optim_context(self):data=self.data_preprocessor(data,True)losses=self._run_forward(data,mode='loss')#type:ignoreparsed_losses,log_vars=self.parse_losses(losses)#type:ignore#大概是对分类损失和回归损失进行加权求和optim_wrapper.update_params(parsed_losses)returnlog_vars","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-jian-ce-tou-yuan-ma-yue-du/"},{"title":"mmdetection-3.x学习笔记——RTMDet模型配置文件（rtmdet_l_8xb32-300e_coco.py）","content":"RTMDet是基于YOLOX再次开发的检测器，检测速度更快，精度更高，还可以做实例分割和旋转目标检测。_base_=[#继承默认配置'../_base_/default_runtime.py','../_base_/schedules/schedule_1x.py','../_base_/datasets/coco_detection.py','./rtmdet_tta.py']model=dict(type='RTMDet',data_preprocessor=dict(#数据预处理器相关配置#数据预处理器的主要功能是把数据和模型从CPU搬运到GPU，然后在GPU中对数据进行归一化操作，也可以实现MixUp、Mosaic一类“将多张图片融合成一张图片”的数据增强操作，不过在这里设置batch_augments=None，即不做批量增强，因为RTMDet实现了新的CachedMosaic和CachedMixUp，采用缓存技术实现Mosaic和MixUp等需要多张图片的数据增广技术，把图片放在缓存中，训练时就不需要加载多张图片就可以实现Mosaic和MixUp，极大地提高数据加载速度。#另外注意，根据mmdetection的设计原则，一般单张图像的数据增广流程会把它们组成train_pipeline传入到CocoDataset数据集类中，而多张图像的增广流程则在数据预处理器中实现，因为这里是DataLoader之后、模型之前，这里有一个batch的数据，可以很容易实现“将多张图片融合成一张图片”的数据增强type='DetDataPreprocessor',mean=[103.53,116.28,123.675],#用于数据归一化的均值std=[57.375,57.12,58.395],#用于数据归一化的标准差bgr_to_rgb=False,#是否交换图片通道，这里为否batch_augments=None),backbone=dict(#基于5*5的深度可分离卷积重新设计了主干网络，采用深度可分离卷积必须额外接一个逐点卷积层（point-wiseconvolution），这样模型的深度增加，因此设计团队适当减少主干网络的每个stage的基础模块并稍微增加模型宽度。type='CSPNeXt',arch='P5',expand_ratio=0.5,deepen_factor=1,widen_factor=1,channel_attention=True,norm_cfg=dict(type='SyncBN'),act_cfg=dict(type='SiLU',inplace=True)),neck=dict(#一个更heavier的颈部网络用于融合多尺度特征对检测不同尺度的目标十分有必要。设计团队提高颈部网络基础模块的膨胀比，以将更多参数和计算放到颈部，提高模型能力。type='CSPNeXtPAFPN',in_channels=[256,512,1024],out_channels=256,num_csp_blocks=3,expand_ratio=0.5,norm_cfg=dict(type='SyncBN'),act_cfg=dict(type='SiLU',inplace=True)),bbox_head=dict(#不同尺度特征图共享检测头，但是得配备不同的BatchNormalization(BN)层。type='RTMDetSepBNHead',num_classes=80,in_channels=256,stacked_convs=2,feat_channels=256,anchor_generator=dict(type='MlvlPointGenerator',offset=0,strides=[8,16,32]),bbox_coder=dict(type='DistancePointBBoxCoder'),loss_cls=dict(type='QualityFocalLoss',use_sigmoid=True,beta=2.0,loss_weight=1.0),loss_bbox=dict(type='GIoULoss',loss_weight=2.0),with_objectness=False,exp_on_reg=True,share_conv=True,pred_kernel_size=1,norm_cfg=dict(type='SyncBN'),act_cfg=dict(type='SiLU',inplace=True)),train_cfg=dict(#重新设计了动态软标签分配策略assigner=dict(type='DynamicSoftLabelAssigner',topk=13),allowed_border=-1,pos_weight=-1,debug=False),test_cfg=dict(#后处理nms_pre=30000,#NMS前的box数min_bbox_size=0,#box允许的最小尺寸score_thr=0.001,#bbox的分数阈值nms=dict(type='nms',iou_threshold=0.65),#NMS类型及阈值max_per_img=300),#NMS后要保留的box数量)#替换默认的train_pipeline，RTMDet采用两段式训练策略，通过PipelineSwitchHook实现#两段式训练是指将训练过程分成两个阶段，第一阶段采用强数据增广Mosaic和MixUp，第二阶段采用弱数据增广，即不用Mosaic和MixUp。YOLOX还在第二阶段引入L1损失函数来微调回归分支。在这里，设计团队不再引入L1损失函数，将第一阶段数据增广时混合图片数量提高到8，且第一阶段设置为280epochs；在最后20个epochs中采用LargeScaleJittering(LSJ)做为数据增广。两段式训练的原因是强数据增广虽然能够提高模型的泛化性，但是会导致训练数据分布与真实分布不一致，所以需要在第二阶段使用与真实分布更接近的数据进行微调。train_pipeline=[dict(type='LoadImageFromFile',file_client_args={{_base_.file_client_args}}),dict(type='LoadAnnotations',with_bbox=True),dict(type='CachedMosaic',img_scale=(640,640),pad_val=114.0),dict(type='RandomResize',scale=(1280,1280),ratio_range=(0.1,2.0),keep_ratio=True),dict(type='RandomCrop',crop_size=(640,640)),dict(type='YOLOXHSVRandomAug'),dict(type='RandomFlip',prob=0.5),dict(type='Pad',size=(640,640),pad_val=dict(img=(114,114,114))),dict(type='CachedMixUp',img_scale=(640,640),ratio_range=(1.0,1.0),max_cached_images=20,pad_val=(114,114,114)),dict(type='PackDetInputs')]train_pipeline_stage2=[dict(type='LoadImageFromFile',file_client_args={{_base_.file_client_args}}),dict(type='LoadAnnotations',with_bbox=True),dict(type='RandomResize',scale=(640,640),ratio_range=(0.1,2.0),keep_ratio=True),dict(type='RandomCrop',crop_size=(640,640)),dict(type='YOLOXHSVRandomAug'),dict(type='RandomFlip',prob=0.5),dict(type='Pad',size=(640,640),pad_val=dict(img=(114,114,114))),dict(type='PackDetInputs')]#替换test_pipelinetest_pipeline=[dict(type='LoadImageFromFile',file_client_args={{_base_.file_client_args}}),dict(type='Resize',scale=(640,640),keep_ratio=True),dict(type='Pad',size=(640,640),pad_val=dict(img=(114,114,114))),dict(type='PackDetInputs',meta_keys=('img_id','img_path','ori_shape','img_shape','scale_factor'))]train_dataloader=dict(batch_size=32,num_workers=10,batch_sampler=None,pin_memory=True,dataset=dict(pipeline=train_pipeline))val_dataloader=dict(batch_size=5,num_workers=10,dataset=dict(pipeline=test_pipeline))test_dataloader=val_dataloadermax_epochs=300stage2_num_epochs=20base_lr=0.004interval=10train_cfg=dict(max_epochs=max_epochs,val_interval=interval,dynamic_intervals=[(max_epochs-stage2_num_epochs,1)])val_evaluator=dict(proposal_nums=(100,1,10))test_evaluator=val_evaluator#optimizeroptim_wrapper=dict(_delete_=True,type='OptimWrapper',optimizer=dict(type='AdamW',lr=base_lr,weight_decay=0.05),paramwise_cfg=dict(norm_decay_mult=0,bias_decay_mult=0,bypass_duplicate=True))#learningrateparam_scheduler=[dict(type='LinearLR',start_factor=1.0e-5,by_epoch=False,begin=0,end=1000),dict(#usecosinelrfrom150to300epochtype='CosineAnnealingLR',eta_min=base_lr*0.05,begin=max_epochs//2,end=max_epochs,T_max=max_epochs//2,by_epoch=True,convert_to_iter_based=True),]#hooksdefault_hooks=dict(checkpoint=dict(interval=interval,max_keep_ckpts=3#onlykeeplatest3checkpoints))custom_hooks=[dict(type='EMAHook',ema_type='ExpMomentumEMA',momentum=0.0002,update_buffers=True,priority=49),dict(type='PipelineSwitchHook',switch_epoch=max_epochs-stage2_num_epochs,switch_pipeline=train_pipeline_stage2)]","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-pei-zhi-wen-jian-rtmdet_l_8xb32-300e_cocopy/"},{"title":"mmdetection-3.x学习笔记——测试时增强相关配置（rtmdet_tta.py）","content":"测试时增强（Testtimeaugmentation，后文简称TTA）是一种测试阶段的数据增强策略，旨在测试过程中，对同一张图片做翻转、缩放等各种数据增强，并在增强后的图像上进行测试，最后将增强后每张图片预测的结果还原到原始尺寸并做融合，以获得更加准确的预测结果。使用测试时增强需要使用TTA对模型进行再次封装，为了让用户更加方便地使用TTA，MMEngine提供了BaseTTAModel类，用户只需按照任务需求，继承BaseTTAModel类，实现不同的TTA策略即可。TTA的核心实现通常分为两个部分：测试时的数据增强：测试时数据增强主要在MMCV中实现，可以参考TestTimeAug的API文档，本文档不再赘述。模型推理以及结果融合：BaseTTAModel的主要功能就是实现这一部分，BaseTTAModel.test_step会解析测试时增强后的数据并进行推理。用户继承BaseTTAModel后只需实现相应的融合策略即可。tta_model=dict(type='DetTTAModel',#TTA模型的类型tta_cfg=dict(#TTA的后处理配置，主要就是非极大值抑制nms=dict(type='nms',#NMS的类型iou_threshold=0.6),#NMS的阈值max_per_img=100))#NMS后要保留的box数量。img_scales=[(640,640),(320,320),(960,960)]#用于定义下面tta_pipeline的图片缩放流程tta_pipeline=[dict(type='LoadImageFromFile',file_client_args=dict(backend='disk')),dict(type='TestTimeAug',transforms=[[#对测试图片进行缩放，由于上面指定了3个图片尺寸，所以缩放后得到3个不同大小的图片dict(type='Resize',scale=s,keep_ratio=True)forsinimg_scales],[#对图片进行随机反转，这里做了两次，一次概率为1，一次概率为0，即每张图片缩放（Resize）后都会进行翻转增强，变成两张图片。结合上面的3次缩放，所以是1张图片变6张图片#``RandomFlip``mustbeplacedbefore``Pad``,otherwise#boundingboxcoordinatesafterflippingcannotbe#recoveredcorrectly.dict(type='RandomFlip',prob=1.),dict(type='RandomFlip',prob=0.)],[#把所有增强后的图片pad成指定大小dict(type='Pad',size=(960,960),pad_val=dict(img=(114,114,114))),],[#最后捡出需要的数据dict(type='PackDetInputs',meta_keys=('img_id','img_path','ori_shape','img_shape','scale_factor','flip','flip_direction'))]])]","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-ce-shi-shi-zeng-qiang-xiang-guan-pei-zhi-rtmdet_ttapy/"},{"title":"mmdetection-3.x学习笔记——数据集相关配置（coco_detection.py）","content":"默认采用coco数据集，结构如下├──data│├──coco││├──annotations││││---instances_train2017.json││││---instances_val2017.json││││---image_info_test-dev2017.json││││---……││├──train2017││││---*.jpg（118287张图片）││├──val2017││││---*.jpg（5000张图片）││├──test2017││││---*.jpg（40670张图片）#datasetsettingsdataset_type='CocoDataset'#数据集类型，这将被用来定义数据集。data_root='data/coco/'#数据的根路径。#file_client_args=dict(#backend='petrel',#path_mapping=dict({#'./data/':'s3://openmmlab/datasets/detection/',#'data/':'s3://openmmlab/datasets/detection/'#}))file_client_args=dict(backend='disk')#文件读取后端的配置，默认从硬盘读取，上面的好像是网络存储设备train_pipeline=[#训练数据处理流程#第1个流程，从文件路径里加载图像。dict(type='LoadImageFromFile',file_client_args=file_client_args),#第2个流程，对于当前图像，加载它的注释信息。#with_bbox=True表示是否使用标注框(boundingbox)，目标检测需要设置为True。#如果同时做语义分割，需要设with_mask=True表示是否使用instancemask。#poly2mask=False表示是否将polygonmask转化为instancemask,设置为False以加速和节省内存。dict(type='LoadAnnotations',with_bbox=True),#变化图像和其标注大小的流程。dict(type='Resize',scale=(1333,800),#图像的最大尺寸keep_ratio=True),#是否保持图像的长宽比。#翻转图像和其标注的数据增广流程。dict(type='RandomFlip',prob=0.5),#翻转图像的概率。#将数据转换为检测器输入格式的流程#数据增广过程中会产生很多中间数据并存放在同一个字典中，该流程主要是捡出目标检测需要的数据dict(type='PackDetInputs')]test_pipeline=[#测试数据处理流程#第1个流程，从文件路径里加载图像。dict(type='LoadImageFromFile',file_client_args=file_client_args),#变化图像大小的流程。dict(type='Resize',scale=(1333,800),keep_ratio=True),#第2个流程，对于当前图像，加载它的注释信息。#Ifyoudon'thaveagtannotation,deletethepipelinedict(type='LoadAnnotations',with_bbox=True),#将数据转换为检测器输入格式的流程dict(type='PackDetInputs',meta_keys=('img_id','img_path','ori_shape','img_shape','scale_factor'))]train_dataloader=dict(#训练dataloader配置batch_size=2,#单个GPU的batchsizenum_workers=2,#单个GPU分配的数据加载线程数persistent_workers=True,#如果设置为True，dataloader在迭代完一轮之后不会关闭数据读取的子进程，可以加速训练sampler=dict(type='DefaultSampler',#默认的训练数据采样器，同时支持分布式和非分布式训练，其实就是打乱图片顺序shuffle=True),#随机打乱每个轮次训练数据的顺序batch_sampler=dict(type='AspectRatioBatchSampler'),#批数据采样器，用于确保每一批次内的数据拥有相似的长宽比，可用于节省显存dataset=dict(#训练数据集的配置type=dataset_type,data_root=data_root,ann_file='annotations/instances_train2017.json',#标注文件路径data_prefix=dict(img='train2017/'),#图片路径前缀filter_cfg=dict(filter_empty_gt=True,min_size=32),#图片和标注的过滤配置pipeline=train_pipeline))#这是由之前创建的train_pipeline定义的数据处理流程。#由上可见，mmdetection采用的是pytorch标准的数据集加载过程，#首先定义数据增广的流程train_pipeline，然后传入CocoDataset类，该类需要实现def__getitem__(self,idx)和def__len__(self)，然后把CocoDataset传入DataLoaders进行封装，DataLoaders可是实现数据批量获取val_dataloader=dict(#验证dataloader配置batch_size=1,#单个GPU的Batchsize。如果batch-szie&gt;1，组成batch时的额外填充会影响模型推理精度num_workers=2,persistent_workers=True,drop_last=False,#是否丢弃最后未能组成一个批次的数据sampler=dict(type='DefaultSampler',shuffle=False),#验证和测试时不打乱数据顺序dataset=dict(type=dataset_type,data_root=data_root,ann_file='annotations/instances_val2017.json',data_prefix=dict(img='val2017/'),test_mode=True,#开启测试模式，避免数据集过滤图片和标注pipeline=test_pipeline))test_dataloader=val_dataloader#coco数据集的测试集不提供标注数据，所以一般就把验证集当测试集用val_evaluator=dict(#验证过程使用的评测器type='CocoMetric',#用于评估检测和实例分割的AR、AP和mAP的coco评价指标ann_file=data_root+'annotations/instances_val2017.json',#标注文件路径metric='bbox',#需要计算的评价指标，`bbox`用于检测，还有`segm`用于实例分割，如果同时做检测和分割，则设为metric=['bbox','segm']format_only=False)#是否只将模型输出转换为coco的JSON格式并保存，这里为否test_evaluator=val_evaluator#由于测试数据集没有标注文件，因此MMDetection中的test_dataloader和test_evaluator配置通常等于val。如果要保存在测试数据集上的检测结果，则可以像这样编写配置：#inferenceontestdatasetand#formattheoutputresultsforsubmission.#注意：coco数据集并没有提供测试集的标注数据，如果想要知道测试集的准确率，#则需要导出预测结果并上传到COCO服务器进行评测，此时需要启用下面的test_dataloader#和test_evaluator，而且需要下载测试图片的信息放到annatations中#（http://images.cocodataset.org/annotations/image_info_test2017.zip）test_dataloader=dict(batch_size=1,num_workers=2,persistent_workers=True,drop_last=False,sampler=dict(type='DefaultSampler',shuffle=False),dataset=dict(type=dataset_type,data_root=data_root,ann_file=data_root+'annotations/image_info_test-dev2017.json',#测试图片信息data_prefix=dict(img='test2017/'),test_mode=True,pipeline=test_pipeline))test_evaluator=dict(type='CocoMetric',metric='bbox',format_only=True,#这里是关键，设置为True之后只将模型输出转换为coco的JSON格式并保存ann_file=data_root+'annotations/image_info_test-dev2017.json',outfile_prefix='./work_dirs/coco_detection/test')#要保存的JSON文件的前缀","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-shu-ju-ji-xiang-guan-pei-zhi-schedule_1xpy/"},{"title":"mmdetection-3.x学习笔记——测试和训练相关配置（schedule_1x.py）","content":"MMEngine的Runner使用Loop来控制训练，验证和测试过程。用户可以使用这些字段设置最大训练轮次和验证间隔。#trainingschedulefor1xtrain_cfg=dict(type='EpochBasedTrainLoop',#训练循环的类型，基于epoch进行训练max_epochs=12,#最大训练轮次val_interval=1)#验证间隔。每个epoch验证一次val_cfg=dict(type='ValLoop')#验证循环的类型test_cfg=dict(type='TestLoop')#测试循环的类型#注意：在模型的配置文件里也存在train_cfg和test_cfg，#那是用于配置模型在训练阶段和测试阶段的相关超参数的，#如训练时的正负样本分配策略（MaxIoUAssigner）、测试阶段的非极大值抑制等#learningrateparam_scheduler=[#超参数调度器dict(type='LinearLR',#使用线性学习率预热start_factor=0.001,#学习率预热的系数，好像是用来计算初始学习率的，#即base_lr*start_factorby_epoch=False,#按iteration更新预热学习率begin=0,#从第一个iteration开始end=500),#到第500个iteration结束dict(type='MultiStepLR',#在训练过程中使用multistep学习率策略begin=0,#从第一个epoch开始end=12,#到第12个epoch结束by_epoch=True,#按epoch更新学习率milestones=[8,11],#在哪几个epoch进行学习率衰减gamma=0.1)#学习率衰减系数,即lr*gamma]#optimizeroptim_wrapper=dict(#优化器封装的配置type='OptimWrapper',#优化器封装的类型。可以切换至AmpOptimWrapper来启用混合精度训练#如果你想要使用BF16，请取消下面一行的代码注释#dtype='float16',#可用值：('float16','bfloat16',None)#警告：截止到PyTorch1.13版本，在Convolution中直接使用torch.bfloat16性能低下，#必须手动设置环境变量TORCH_CUDNN_V8_API_ENABLED=1#以启用CuDNN版本的BF16Convolution。相关讨论见PyTorchIssueoptimizer=dict(#优化器配置。支持PyTorch的各种优化器。#请参考https://pytorch.org/docs/stable/optim.html#algorithmstype='SGD',#随机梯度下降优化器lr=0.02,#基础学习率momentum=0.9,#带动量的随机梯度下降weight_decay=0.0001))#权重衰减#DefaultsettingforscalingLRautomatically#-`enable`meansenablescalingLRautomatically#ornotbydefault.#-`base_batch_size`=(8GPUs)x(2samplesperGPU).auto_scale_lr=dict(enable=False,base_batch_size=16)#学习率自动缩放#在配置文件中的学习率是在8块GPU，每块GPU有2张图像（批大小为8*2=16）的情况下设置的。#如果启用该功能，则会根据机器的GPU数量和训练的批次大小对学习率进行自动缩放，#缩放方式详见线性扩展规则，比如：在8块GPU并且每张GPU上有2张图片的情况下lr=0.01，#那么在16块GPU并且每张GPU上有4张图片的情况下,LR会自动缩放至lr=0.04。","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-ce-shi-he-xun-lian-xiang-guan-pei-zhi-schedule_1xpy/"},{"title":"（未完成）Deformable convolution network v3 cuda算子开发","content":"参考教程：https://zhuanlan.zhihu.com/p/595851188img2col算法：https://inria.hal.science/file/index/docid/112631/filename/p1038112283956.pdf","link":"https://chenjie04.github.io/post/wei-wan-cheng-deformable-convolution-network-v3-cuda-suan-zi-kai-fa/"},{"title":"（未完成）【论文阅读笔记】Deformable ConvNets v2: More Deformable, Better Results","content":"论文地址：https://arxiv.org/abs/1811.11168v2代码地址：python代码https://github.com/4uiiurz1/pytorch-deform-conv-v2/blob/master/deform_conv_v2.py","link":"https://chenjie04.github.io/post/bi-ji-deformable-convnets-v2-more-deformable-better-results/"},{"title":"mmdetection-3.x学习笔记——配置程序运行环境","content":"mmdetection会在Runner类初始化时调用setup_env(env_cfg)函数来配置程序的运行环境，包括多线程和分布式等环境信息，defsetup_env(self,env_cfg:Dict)-&gt;None:&quot;&quot;&quot;Setupenvironment.Anexampleof``env_cfg``::env_cfg=dict(cudnn_benchmark=True,mp_cfg=dict(mp_start_method='fork',opencv_num_threads=0),dist_cfg=dict(backend='nccl'),resource_limit=4096)Args:env_cfg(dict):Configforsettingenvironment.&quot;&quot;&quot;#如果env_cfg中存在'cudnn_benchmark'且值为True，则开启cudnnbenchmarkifenv_cfg.get('cudnn_benchmark'):#cudnnbenchmark对针对卷积、池化等等常见操作进行了底层优化，#开启可以提升速度，但是如果卷积层的设置一直变化，将会导致程序不停地做优化，#反而会耗费更多的时间，所以一般设置为False。torch.backends.cudnn.benchmark=Truemp_cfg:dict=env_cfg.get('mp_cfg',{})#获取多进程环境配置set_multi_processing(**mp_cfg,distributed=self.distributed)#配置多进程环境#initdistributedenvfirst,sinceloggerdependsonthedistinfo.ifself.distributedandnotis_distributed():dist_cfg:dict=env_cfg.get('dist_cfg',{})#初始化分布式环境，launcher指定加载多进程的方式，#可以是'pytorch','mpi','slurm'和'none'，'none'表示不用分布式训练init_dist(self.launcher,**dist_cfg)self._rank,self._world_size=get_dist_info()#rank：进程序号，world_size:全金进程数timestamp=torch.tensor(time.time(),dtype=torch.float64)#broadcasttimestampfrom0processtootherprocessesbroadcast(timestamp)self._timestamp=time.strftime('%Y%m%d_%H%M%S',time.localtime(timestamp.item()))#https://github.com/pytorch/pytorch/issues/973#setresourcelimitifplatform.system()!='Windows':importresourcerlimit=resource.getrlimit(resource.RLIMIT_NOFILE)base_soft_limit=rlimit[0]hard_limit=rlimit[1]soft_limit=min(max(env_cfg.get('resource_limit',4096),base_soft_limit),hard_limit)resource.setrlimit(resource.RLIMIT_NOFILE,(soft_limit,hard_limit))1、配置多进程环境#Copyright(c)OpenMMLab.Allrightsreserved.importosimportplatformimportwarningsimporttorch.multiprocessingasmpdefset_multi_processing(mp_start_method:str='fork',opencv_num_threads:int=0,distributed:bool=False)-&gt;None:&quot;&quot;&quot;Setmulti-processingrelatedenvironment.Args:mp_start_method(str):Setthemethodwhichshouldbeusedtostartchildprocesses.Defaultsto'fork'.opencv_num_threads(int):Numberofthreadsforopencv.Defaultsto0.distributed(bool):Trueifdistributedenvironment.DefaultstoFalse.&quot;&quot;&quot;#setmulti-processstartmethodas`fork`tospeedupthetrainingifplatform.system()!='Windows':current_method=mp.get_start_method(allow_none=True)#获取当前启动多进程的方法#如果当前启动多进程的方法不为空且与配置文件指定的不一样，输出警告if(current_methodisnotNoneandcurrent_method!=mp_start_method):warnings.warn(f'Multi-processingstartmethod`{mp_start_method}`is'f'differentfromtheprevioussetting`{current_method}`.'f'Itwillbeforcesetto`{mp_start_method}`.Youcan''changethisbehaviorbychanging`mp_start_method`in''yourconfig.')mp.set_start_method(mp_start_method,force=True)#设置为配置文件中的多进程启动方式try:importcv2#disableopencvmultithreadingtoavoidsystembeingoverloadedcv2.setNumThreads(opencv_num_threads)exceptImportError:pass#setupOMPthreads#Thiscodeisreferredfromhttps://github.com/pytorch/pytorch/blob/master/torch/distributed/run.py#noqaif'OMP_NUM_THREADS'notinos.environanddistributed:omp_num_threads=1warnings.warn('SettingOMP_NUM_THREADSenvironmentvariableforeachprocess'f'tobe{omp_num_threads}indefault,toavoidyoursystem''beingoverloaded,pleasefurthertunethevariablefor''optimalperformanceinyourapplicationasneeded.')#os.environ包含跟系统有关的信息，如os.environ['USER']:当前使用用户os.environ['OMP_NUM_THREADS']=str(omp_num_threads)#setupMKLthreadsif'MKL_NUM_THREADS'notinos.environanddistributed:mkl_num_threads=1warnings.warn('SettingMKL_NUM_THREADSenvironmentvariableforeachprocess'f'tobe{mkl_num_threads}indefault,toavoidyoursystem''beingoverloaded,pleasefurthertunethevariablefor''optimalperformanceinyourapplicationasneeded.')os.environ['MKL_NUM_THREADS']=str(mkl_num_threads)","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-pei-zhi-cheng-xu-yun-xing-huan-jing/"},{"title":"mmdetection-3.x学习笔记——MMEngin核心组件分析（一）：Hook","content":"Hook编程是一种编程模式，是指在程序的一个或者多个位置设置位点（挂载点），当程序运行至某个位点时，会自动调用运行时注册到位点的所有方法。在python中由于函数是一等公民，实现hook机制其实只需要传入一个函数即可，在该函数中我们可以获取到内部信息，可以修改或者访问该内容，从而实现一些类似黑科技一般的功能，如下是一个简单的Hook，打印内部变量，defhook(d):print(d)defadd(a,b,c,hook_fn=None)sum1=a+bifhook_fnisnotNone:hook_fn(sum1)returnsum1+c#调用add(1,2,3,hook)Hook配合Runner可以管理整个模型训练流程。在Runner（/mmengine/runner/runner.py）类中，维护了一个Hook池，Runner初始化时会把用到的Hook（default_hooks,custom_hooks）都注册到Hook池中，@RUNNERS.register_module()classRunner:def__init__(self,...,default_hooks:Optional[Dict[str,Union[Hook,Dict]]]=None,custom_hooks:Optional[List[Union[Hook,Dict]]]=None,...):.......self._hooks:List[Hook]=[]#Hook池初始化#registerhooksto`self._hooks`self.register_hooks(default_hooks,custom_hooks)#调用register_hooks()函数把default_hooks,custom_hooks都注册到Hook池中#loghooksinformationself.logger.info(f'Hookswillbeexecutedinthefollowing'f'order:\\n{self.get_hooks_info()}')因此，有default_hooks和custom_hooks两大类，所以用register_hooks()函数再封装一层，register_hooks()函数内部调用register_default_hooks()和register_custom_hooks()函数，defregister_hooks(self,default_hooks:Optional[Dict[str,Union[Hook,Dict]]]=None,custom_hooks:Optional[List[Union[Hook,Dict]]]=None)-&gt;None:&quot;&quot;&quot;Registerdefaulthooksandcustomhooksintohooklist.Args:default_hooks(dict[str,dict]ordict[str,Hook],optional):Hookstoexecutedefaultactionslikeupdatingmodelparametersandsavingcheckpoints.DefaultstoNone.custom_hooks(list[dict]orlist[Hook],optional):Hookstoexecutecustomactionslikevisualizingimagesprocessedbypipeline.DefaultstoNone.&quot;&quot;&quot;self.register_default_hooks(default_hooks)ifcustom_hooksisnotNone:self.register_custom_hooks(custom_hooks)register_default_hooks(）函数定义默认的Hooks，并调用register_hook()函数注册到Hook池中，defregister_default_hooks(self,hooks:Optional[Dict[str,Union[Hook,Dict]]]=None)-&gt;None:&quot;&quot;&quot;Registerdefaulthooksintohooklist.``hooks``willberegisteredintorunnertoexecutesomedefaultactionslikeupdatingmodelparametersorsavingcheckpoints.Defaulthooksandtheirpriorities:+----------------------+-------------------------+|Hooks|Priority|+======================+=========================+|RuntimeInfoHook|VERY_HIGH(10)|+----------------------+-------------------------+|IterTimerHook|NORMAL(50)|+----------------------+-------------------------+|DistSamplerSeedHook|NORMAL(50)|+----------------------+-------------------------+|LoggerHook|BELOW_NORMAL(60)|+----------------------+-------------------------+|ParamSchedulerHook|LOW(70)|+----------------------+-------------------------+|CheckpointHook|VERY_LOW(90)|+----------------------+-------------------------+If``hooks``isNone,abovehookswillberegisteredbydefault::default_hooks=dict(runtime_info=dict(type='RuntimeInfoHook'),timer=dict(type='IterTimerHook'),sampler_seed=dict(type='DistSamplerSeedHook'),logger=dict(type='LoggerHook'),param_scheduler=dict(type='ParamSchedulerHook'),checkpoint=dict(type='CheckpointHook',interval=1),)IfnotNone,``hooks``willbemergedinto``default_hooks``.IfthereareNonevalueindefault_hooks,thecorrespondingitemwillbepoppedfrom``default_hooks``::hooks=dict(timer=None)Thefinalregistereddefaulthookswillbe:obj:`RuntimeInfoHook`,:obj:`DistSamplerSeedHook`,:obj:`LoggerHook`,:obj:`ParamSchedulerHook`and:obj:`CheckpointHook`.Args:hooks(dict[str,Hookordict],optional):Defaulthooksorconfigstoberegistered.&quot;&quot;&quot;default_hooks:dict=dict(#定义默认的Hookruntime_info=dict(type='RuntimeInfoHook'),timer=dict(type='IterTimerHook'),sampler_seed=dict(type='DistSamplerSeedHook'),logger=dict(type='LoggerHook'),param_scheduler=dict(type='ParamSchedulerHook'),checkpoint=dict(type='CheckpointHook',interval=1),)ifhooksisnotNone:forname,hookinhooks.items():ifnameindefault_hooksandhookisNone:#移除不想要的Hook，假如不想要timer这个hook，则把hooks=dict(timer=None)作为参数传入#removehookfrom_default_hooksdefault_hooks.pop(name)else:asserthookisnotNonedefault_hooks[name]=hook#hook不为空，且hook不在default_hooks内，即将该hook添加到defau_hooks中forhookindefault_hooks.values():self.register_hook(hook)#真正的注册函数Hook注册函数，将hook按照优先级插入到Hook池中，defregister_hook(self,hook:Union[Hook,Dict],priority:Optional[Union[str,int,Priority]]=None)-&gt;None:&quot;&quot;&quot;Registerahookintothehooklist.Thehookwillbeinsertedintoapriorityqueue,withthespecifiedpriority(See:class:`Priority`fordetailsofpriorities).Forhookswiththesamepriority,theywillbetriggeredinthesameorderastheyareregistered.Priorityofhookwillbedecidedwiththefollowingpriority:-``priority``argument.If``priority``isgiven,itwillbepriorityofhook.-If``hook``argumentisadictand``priority``init,theprioritywillbethevalueof``hook['priority']``.-If``hook``argumentisadictbut``priority``notinitor``hook``isaninstanceof``hook``,theprioritywillbe``hook.priority``.Args:hook(:obj:`Hook`ordict):Thehooktoberegistered.priority(intorstror:obj:`Priority`,optional):Hookpriority.Lowervaluemeanshigherpriority.&quot;&quot;&quot;ifnotisinstance(hook,(Hook,dict)):raiseTypeError(f'hookshouldbeaninstanceofHookordict,butgot{hook}')_priority=Noneifisinstance(hook,dict):#传入的是hook配置词典if'priority'inhook:_priority=hook.pop('priority')#获取hook配置词典中指定的优先级hook_obj=HOOKS.build(hook)#构建hookelse:hook_obj=hook#传入的是hook对象ifpriorityisnotNone:#调用注册函数时指定优先级hook_obj.priority=priorityelif_priorityisnotNone:hook_obj.priority=_priorityinserted=Falseforiinrange(len(self._hooks)-1,-1,-1):ifget_priority(hook_obj.priority)&gt;=get_priority(self._hooks[i].priority):#比较要注册的hook的优先级与hook池中的优先级，找到正确的位置并插入self._hooks.insert(i+1,hook_obj)inserted=Truebreakifnotinserted:self._hooks.insert(0,hook_obj)#优先级最高Runner在模型的训练、测试流程中插入位点，并在对应位点实现调用hook的抽象逻辑。如在Runner的train函数中插入了'before_run'和'after_run'两个位点，deftrain(self)-&gt;nn.Module:......self.call_hook('before_run')......model=self.train_loop.run()#type:ignoreself.call_hook('after_run')returnmodel在训练循环中插入了'before_train'和'after_train'两个位点，defrun(self)-&gt;torch.nn.Module:&quot;&quot;&quot;Launchtraining.&quot;&quot;&quot;self.runner.call_hook('before_train')whileself._epoch&lt;self._max_epochs:self.run_epoch()self._decide_current_val_interval()if(self.runner.val_loopisnotNoneandself._epoch&gt;=self.val_beginandself._epoch%self.val_interval==0):self.runner.val_loop.run()self.runner.call_hook('after_train')returnself.runner.model在run_epoch()中插入了'before_train_epoch'和'after_train_epoch'两个位点defrun_epoch(self)-&gt;None:&quot;&quot;&quot;Iterateoneepoch.&quot;&quot;&quot;self.runner.call_hook('before_train_epoch')self.runner.model.train()foridx,data_batchinenumerate(self.dataloader):self.run_iter(idx,data_batch)self.runner.call_hook('after_train_epoch')self._epoch+=1在run_iter()中插入了'before_train_iter'和'after_train_iter'两个位点defrun_iter(self,idx,data_batch:Sequence[dict])-&gt;None:&quot;&quot;&quot;Iterateonemin-batch.Args:data_batch(Sequence[dict]):Batchofdatafromdataloader.&quot;&quot;&quot;self.runner.call_hook('before_train_iter',batch_idx=idx,data_batch=data_batch)#Enablegradientaccumulationmodeandavoidunnecessarygradient#synchronizationduringgradientaccumulationprocess.#outputsshouldbeadictofloss.outputs=self.runner.model.train_step(data_batch,optim_wrapper=self.runner.optim_wrapper)self.runner.call_hook('after_train_iter',batch_idx=idx,data_batch=data_batch,outputs=outputs)self._iter+=1Hook中一共有22个位点，在伪代码位置如下defmain():...call_hooks('before_run',hooks)#任务开始前执行的逻辑call_hooks('after_load_checkpoint',hooks)#加载权重后执行的逻辑call_hooks('before_train',hooks)#训练开始前执行的逻辑foriinrange(max_epochs):call_hooks('before_train_epoch',hooks)#遍历训练数据集前执行的逻辑forinputs,labelsintrain_dataloader:call_hooks('before_train_iter',hooks)#模型前向计算前执行的逻辑outputs=net(inputs)loss=criterion(outputs,labels)call_hooks('after_train_iter',hooks)#模型前向计算后执行的逻辑loss.backward()optimizer.step()call_hooks('after_train_epoch',hooks)#遍历完训练数据集后执行的逻辑call_hooks('before_val_epoch',hooks)#遍历验证数据集前执行的逻辑withtorch.no_grad():forinputs,labelsinval_dataloader:call_hooks('before_val_iter',hooks)#模型前向计算前执行outputs=net(inputs)loss=criterion(outputs,labels)call_hooks('after_val_iter',hooks)#模型前向计算后执行call_hooks('after_val_epoch',hooks)#遍历完验证数据集前执行call_hooks('before_save_checkpoint',hooks)#保存权重前执行的逻辑call_hooks('after_train',hooks)#训练结束后执行的逻辑call_hooks('before_test_epoch',hooks)#遍历测试数据集前执行的逻辑withtorch.no_grad():forinputs,labelsintest_dataloader:call_hooks('before_test_iter',hooks)#模型前向计算后执行的逻辑outputs=net(inputs)accuracy=...call_hooks('after_test_iter',hooks)#遍历完成测试数据集后执行的逻辑call_hooks('after_test_epoch',hooks)#遍历完测试数据集后执行call_hooks('after_run',hooks)#任务结束后执行的逻辑在每个位点通过call_hook()调用所有hooks在该位点对应的函数，它会遍历整个hook池，然后执行每个hooks对应该位点的函数，如在'before_run'这个位点，则执行每个hook的before_run()函数。call_hook()的实现逻辑如下，defcall_hook(self,fn_name:str,**kwargs)-&gt;None:&quot;&quot;&quot;Callallhooks.Args:fn_name(str):Thefunctionnameineachhooktobecalled,suchas&quot;before_train_epoch&quot;.**kwargs:Keywordargumentspassedtohook.&quot;&quot;&quot;forhookinself._hooks:#遍历hook池#supportaddingadditionalcustomhookmethodsifhasattr(hook,fn_name):#如果该hook有该位点对应的函数try:getattr(hook,fn_name)(self,**kwargs)#则执行exceptTypeErrorase:raiseTypeError(f'{e}in{hook}')fromNone下面是YOLOXModeSwitchHook，它的作用是训练的最后15个epoch关闭mosaic和mixup数据增广，并启用L1损失函数对回归分支微调，所以在每个epoch开始前需要判断当前是否是最后15个epoch，如果是则执行对应操作。因此，该hook只需要before_train_epoch()这一个函数。#Copyright(c)OpenMMLab.Allrightsreserved.fromtypingimportSequencefrommmengine.hooksimportHookfrommmengine.modelimportis_model_wrapperfrommmdet.registryimportHOOKS@HOOKS.register_module()classYOLOXModeSwitchHook(Hook):&quot;&quot;&quot;SwitchthemodeofYOLOXduringtraining.ThishookturnsoffthemosaicandmixupdataaugmentationandswitchestouseL1lossinbbox_head.Args:num_last_epochs(int):ThenumberoflatterepochsintheendofthetrainingtoclosethedataaugmentationandswitchtoL1loss.Defaultsto15.skip_type_keys(Sequence[str],optional):Sequenceoftypestringtobeskippipeline.Defaultsto('Mosaic','RandomAffine','MixUp').&quot;&quot;&quot;def__init__(self,num_last_epochs:int=15,skip_type_keys:Sequence[str]=('Mosaic','RandomAffine','MixUp'))-&gt;None:self.num_last_epochs=num_last_epochsself.skip_type_keys=skip_type_keysself._restart_dataloader=Falsedefbefore_train_epoch(self,runner)-&gt;None:&quot;&quot;&quot;ClosemosaicandmixupaugmentationandswitchestouseL1loss.&quot;&quot;&quot;epoch=runner.epochtrain_loader=runner.train_dataloadermodel=runner.model#TODO:refactoraftermmengineusingmodelwrapperifis_model_wrapper(model):model=model.moduleif(epoch+1)==runner.max_epochs-self.num_last_epochs:runner.logger.info('Nomosaicandmixupaugnow!')#Thedatasetpipelinecannotbeupdatedwhenpersistent_workers#isTrue,soweneedtoforcethedataloader'smulti-process#restart.Thisisaveryhackyapproach.train_loader.dataset.update_skip_type_keys(self.skip_type_keys)ifhasattr(train_loader,'persistent_workers')andtrain_loader.persistent_workersisTrue:train_loader._DataLoader__initialized=Falsetrain_loader._iterator=Noneself._restart_dataloader=Truerunner.logger.info('AddadditionalL1lossnow!')model.bbox_head.use_l1=Trueelse:#Oncetherestartiscomplete,weneedtorestore#theinitializationflag.ifself._restart_dataloader:train_loader._DataLoader__initialized=True","link":"https://chenjie04.github.io/post/hook/"},{"title":"Transformer阅读笔记","content":"本文研究基于Transformer的目标检测，相关内容参考《ASurveyonVisionTransformer》.一、Transformer-basedsetpredictionmethods1、N.Carionetal.End-to-EndObjectDetectionwithTransformers.InECCV,2020.核心：重新定义目标检测框架，CNN提取特征，将ximg∈R3×H0×W0x_{img}\\inR^{3\\timesH_{0}\\timesW_{0}}ximg​∈R3×H0​×W0​进行32倍下采样得到输出f∈RC×H×Wf\\inR^{C\\timesH\\timesW}f∈RC×H×W（C=2048,H,W=H0/32,W0/32C=2048,H,W=H_{0}/32,W_{0}/32C=2048,H,W=H0​/32,W0​/32），加上位置编码，输入transformer，其中decoder会通过1×11\\times11×1卷积将输出变形压缩为d×HWd\\timesHWd×HW，最后3层FNN和线性映射预测N个boundingbox和类别标签，其中输入到decoder的objectqueries通过学习得到。（一个问题：N应该不等于HW）DETR将目标检测建模为集合预测问题，不需要生成anchor，也不需要非极大值抑制。不需要非极大值抑制的原因是因为DETR直接将非目标的区域直接预测为noobject，然后实验结果表明在decoder的第一层输出加非极大值抑制还有用，再深几层就没啥用了，所以是不需要非极大值抑制的实验发现的，并没有理论证明。Labelassignment：Prediction和Ground-Truth之间就存在一个匹配的问题（labelassignment）。因为DETR采用集合预测（setprediction）的形式，两个集合之间的匹配就需要保证不受集合中元素排列组合影响，那么就变成了求集合间最优匹配的问题。本文采用匈牙利算法（Hungarianalgorithm）求集合最优匹配问题。Lossfunction采用Hungarianloss一个小技巧是在decoder的每一层都附加FNN预测头和loss。缺点：需要非常长的训练过程，小目标检测也很差。（However,thevanillaDETRposesseveralchallenges,specifically,longertrainingscheduleandpoorperformanceforsmallobjects.）（大目标检测准确率高的原因可能是transformer的non-local特性带来的，但是这反而不利于小目标检测）注意：Transformer比传统卷积同等FLOPs下慢至少3倍（Huetal.(2019)admitsuchapproachesaremuchslowerinimplementationthantraditionalconvolutionwiththesameFLOPs(atleast3×slower),duetotheintrinsiclimitationinmemoryaccesspatterns.）2、X.Zhuetal.Deformabledetr:DeformableTransformersforEnd-to-EndObjectDetection.InICLR,2021.DeformableDETR核心：Thedeformableattentionmoduleattendstoasmallsetofkeypositionsaroundareferencepointratherthanlookingatallspatiallocationsonimagefeaturemapsasperformedbytheoriginalmulti-headattentionmechanismintransformer.DETR存在的问题：DETR需要非常长的训练过程才能收敛。原因有二：一是与NLP任务相比，CV任务中注意力机制的元素（elements，一般是图像特征图的像素）量过大，导致过高的计算复杂度和空间复杂度；二是注意力权重Amqk≈1/NkA_{mqk}\\approx1/N_{k}Amqk​≈1/Nk​，当元素的数量NKN_{K}NK​过大时，权重接近于0，因此需要更久的训练才能使注意力集中在特定的元素上。DETR的小目标检测性能较差。原因在于transformer的特长在于捕获全局依赖，这样的非局部性对于检测大目标有效，却不利于小目标检测。解决方案：为了解决问题1，受可变形卷积影响，提出了可变形注意力模块。如上图所示，在注意力机制中，针对每个query，首先经过线性映射得到抽样位置的偏移量以及注意力权重，然后参考点位置加上位置偏移量得到真正的抽样位置，将抽样位置的特征（value）和注意力权重进行整合得到输出。可变形注意力机制同样采用多头注意力的形式，如上图为3头注意力。注意：这种可变形注意力由于不对位置偏移量做任何限制，因此可以关注特征图上的任意位置，而抽样位置为有限的位置，因此同时具备了全局性和局部性。很重要！可变形注意力数学形式定义：给定输入特征图x∈RC×H×W\\mathbf{x}\\inR^{C\\timesH\\timesW}x∈RC×H×W，设qqq是query元素的索引，对应的query的特征向量为zqz_qzq​，二维参考位点为pq=(pqx,pqy)p_{q}=(p_{qx},p_{qy})pq​=(pqx​,pqy​)，可变形注意力机制如下所示：其中，mmm是注意力头的索引；kkk是采样的keys的索引；KKK是采样keys的数量（K≪HWK\\llHWK≪HW）。Δpmqk\\Deltap_{mqk}Δpmqk​和AmqkA_{mqk}Amqk​是zqz_{q}zq​通过线性映射得到的在第mthm^{th}mth个注意力头第kthk^{th}kth个位置的采用偏移量和注意力权重。在实现的时候，query特征向量zqz_{q}zq​通过线性映射网络得到3MK3MK3MK通道，前2MK2MK2MK通道编码了采样偏移量Δpmqk\\Deltap_{mqk}Δpmqk​，剩下的MKMKMK通道通过softmaxsoftmaxsoftmax得到注意力权重AmqkA_{mqk}Amqk​。注意力权重AmqkA_{mqk}Amqk​的范围[0,1][0,1][0,1]，且经过归一化∑k=1KAmqk=1\\sum_{k=1}^{K}A_{mqk}=1∑k=1K​Amqk​=1。采样偏移量Δpmqk\\Deltap_{mqk}Δpmqk​为二维实数，且范围不限，即可以关注特征图上的任意位置。多尺度可变形注意力为了解决小目标检测问题，论文将可变形注意力机制拓展到多尺度特征图，需要注意的是参考点位置需要进行归一化才能在不同尺度的特征图上进行采样，即p^q∈[0,1]2\\hat{p}_{q}\\in[0,1]^{2}p^​q​∈[0,1]2,ϕ(p^q)\\phi(\\hat{p}_{q})ϕ(p^​q​)将参考点位置映射到不同尺度的特征图。讨论：假若将可变形注意力用于自己会议论文提出的spatial-aware和sclae-aware多尺度特征融合，那么单尺度上的可变形注意力便已经可以实现spatial-aware，但是多尺度可变形注意力却并不是scale-aware的，因为它是直接相加，一个简单的方法是增加一个可训练的权重，即：MSDeformAttn(zq,p^q,{xl}l=1L)=∑m=1MWm[∑l=1LWm′∑k=1KAmlqk⋅Wm′′xl(ϕl(p^q)+Δpmlqk]MSDeformAttn(z_{q},\\hat{p}_{q},\\{x^{l}\\}_{l=1}^{L})=\\sum_{m=1}^{M}W_{m}[\\sum_{l=1}^{L}W_{m}^{&#x27;}\\sum_{k=1}^{K}A_{mlqk}\\cdotW_{m}^{&#x27;&#x27;}x^{l}(\\phi_{l}(\\hat{p}_{q})+\\Deltap_{mlqk}]MSDeformAttn(zq​,p^​q​,{xl}l=1L​)=∑m=1M​Wm​[∑l=1L​Wm′​∑k=1K​Amlqk​⋅Wm′′​xl(ϕl​(p^​q​)+Δpmlqk​]或者scaleattention的设置为自注意力权重：整体模型框架如下：优点：deformableattentionmodule同时具备全局性和局部性，可以融合多尺度特征，更低复杂度，更快收敛，更好性能。3、Z.Sunetal.RethinkingTransformer-basedSetPredictionforObjectDetection.InICCV,pp.3611–3620,2021.核心：论文认为基于transformer的目标检测之所以收敛慢，其原因在于decoder中的cross-attentionmodule（就是中间跨越encoder和decoder的attention），因此论文提出了只有encoder的检测模型，而预测过程采用了新的集合预测方法——TSP-FCOS和TSP-RCNN，同时设计了新的bipartitematching方法，获得了更好的稳定性和更快的收敛速度。研究收敛慢的原因1、二部匹配（bipartitematching）的不稳定性导致模型收敛慢？基于匈牙利算法的二部匹配模块是模型中的唯一组件，它的初始化往往是随机的，训练过程中的噪声也容易导致匹配的不稳定性。采用知识蒸馏（用一个预先训练的模型作为二部匹配的Teacher）与原模型比较收敛速度，如下图所示，从图中可以看到，在前15epochs带知识蒸馏的DETR确实收敛更快，但是到了25epochs时优势便没有。所以说二部匹配的不稳定性有影响，但是不大。2、注意力模块是收敛慢的主要原因？其实，经过前面的讨论，人们已经普遍相信注意力模型往往都是需要更长的训练时间。在这里作者更关注encoder和decoder之间的cross-attention模块，这个模块将encoder提取到的特征引入decoder，显然很关键。注意力模型收敛与否的标志是其离散度，注意力图可以解释为概率，所以作者采用负交叉熵作为离散度的度量，从下图中可以看到每一层神经网络的离散度都在上升，到了100epochs都没有收敛，显然这就是导致收敛慢的主要原因。那么，cross-attention模块收敛慢，我们不要行不行？作者直接去掉了decoder，如下图所示，然后，看看结果，如下图所示，可以看到去掉decoder之后AP几乎不变，而且在小目标检测和中等目标检测上还相对更好，尤其是小目标检测，但是大目标检测却面临了准确率下降（很神奇）。作者认为其原因可能是：Apotentialinterpretation,wethink,isthatalargeobjectmayincludetoomanypotentiallymatchablefeaturepoints,whicharediffificultfortheslidingpointschemeintheencoder-onlyDETRtohandle.Anotherpossiblereasonisthatasinglefeaturemapprocessedbyencoderisnotrobustforpredictingobjectsofdifferentscales.所提出的方法：作者找到了他们认为的DETR收敛慢的原因，但是他们提出来的改进方法并不优雅！上面说到直接去掉decoder会导致大目标检测性能下降，说明不能很好应对多尺度目标检测，那怎么办？加FPN，如下图所示。加上FPN之后，考虑到FCOS模型有个特征选择的模块，就是FeatureofInterest，那也加上。得到的特征加上位置编码然后输入encoder中，最后输出预测解决。最后考虑到二部匹配不太稳，那就加上FCOS的匹配规则作为二部匹配的约束条件。这就是TSP-FCOS模型，这么一整，感觉就是FCOS模型换了个检测头。同样，仿照FasterR-CNN加上RPN，同时借鉴FasterR-CNN的匹配规则作为约束条件，这就是TSP-RCNN模型。意义：我觉得这篇文章最大的意义就是证明了去掉decoder对模型性能影响不大，同时证明了FPN对检测不同尺度目标的重要性。但是我想把第一步主干网络特征提取也简化了，不然太不优雅。4、M.Zhengetal.End-to-endobjectdetectionwithadaptiveclusteringtransformer.InBMVC,2021.核心：提出了AdaptiveClusteringTransformer(ACT)替换自注意力模块。ACTadaptivelyclustersthequeryfeaturesusingalocalitysensitivityhashing(LSH)methodandbroadcaststheattentionoutputtothequeriesrepresentedbytheselectedprototypes。可以以轻微性能下降为代价极大降低计算复杂度。同样，这篇文章的主要目的也是提高模型收敛速度。Motivation：1、Encoder中的注意力冗余如下图所示，左边是图中3个不同位置的注意力图，可以发现p0p_0p0​和p1p_1p1​的注意力图很相似，这是因为这两个点都在同一个目标上而且相距较近，与此相反p2p_2p2​的注意力图则完全不同。现p0p_0p0​和p1p_1p1​的注意力图相似恰恰说明encoder中注意力的计算存在大量冗余。我们知道注意力计算的复杂度就是queries×keysqueries\\timeskeysqueries×keys，所以要降低复杂度就要减少queriesqueriesqueries或keyskeyskeys的数量，然后根据刚刚的观察p0p_0p0​和p1p_1p1​的注意力计算存在冗余，那么我们完全可以用p0p_0p0​代替p1p_1p1​从而减少queriesqueriesqueries的数量。具体的做法就是对queriesqueriesqueries做聚类，然后拿每个类的原型（prototypesprototypesprototypes）去做注意力计算，然后将梯度更新广播到类中的每个点上。点评：很有意思的想法，和DeformableDETR形成互补。DeformableDETR认为给定一个queryqueryquery不是对所有的keyskeyskeys都要注意到，其中很多的权重都为零，这就是局部性。所以DeformableDETR通过queryqueryquery预测需要注意的少量的keyskeyskeys，从而降低了计算复杂度。我认为这两者可以结合起来。提出的方法：作者将这种注意力机制称为自适应聚类注意力（AdaptiveClusteringAttention），用于替换encoder中的attention模块，decoder保持不变。自适应聚类注意力的工作原理很简单，如下图左所示，inputfeatures同样是先线性映射成queries、keys、valuesqueries、keys、valuesqueries、keys、values，然后对queriesqueriesqueries进行聚类得到每个类的原型（prototypesprototypesprototypes），将prototypesprototypesprototypes与keyskeyskeys计算attentionmapattention\\mapattentionmap，然后与valuesvaluesvalues相乘，得到输出，最后进行广播。关键在聚类，但是需要注意，每个网络层的特征分布是不一样的，所以需要自适应聚类。文章采用的是ExactEuclideanLocalitySensitiveHashing(E2LSH)进行聚类，它可以让所有距离小于ε的向量以大于p的概率落入同一哈希桶中（letallvectorswithadistancelessthanε\\varepsilonεfallintothesamehashbucketwithaprobabilitygreaterthanppp），哈希函数如下所示：其中，r,a⃗,br,\\vec{a},br,a,b都是超参数，rrr控制着超平面的间距，a⃗={a1,a2,...,ad}\\vec{a}=\\{a_1,a_2,...,a_d\\}a={a1​,a2​,...,ad​}和bbb都是随机变量，aia_iai​满足正态分布ai∼N(0,1)a_i\\simN(0,1)ai​∼N(0,1)，bbb满足均匀分布b∼U(0,r)b\\simU(0,r)b∼U(0,r)。为了增加结果的可信度，迭代计算LLL回哈希值，BBB为常数，B=4B=4B=4。落入同一哈希桶中的向量具有同一哈希值，距离越近的向量落入同一哈希桶的概率越大。至此，我们实现了自适应聚类。聚类完成后计算prototypesprototypesprototypes，记queriesqueriesqueries为Q∈RN×DkQ\\inR^{N\\timesD_{k}}Q∈RN×Dk​,GiG_iGi​为QiQ_iQi​所在的簇的索引，计算某个簇的prototypeprototypeprototype其实就是计算改簇的均值，如计算第jjj个簇的prototypeprototypeprototype如下所示，剩下的注意力计算和广播就比较简单了，如下所示，必须注意的是：虽然极大地降低了计算复杂度，但是性能轻微下降，所以这个方法更应该归类于transformer轻量化研究。为了恢复检测精度，作者采用多任务知识蒸馏，如下图所示，最终，精度损失控制在0.2%0.2\\%0.2%。5、P.Gaoetal.Fastconvergenceofdetrwithspatiallymodulatedco-attention.InICCV,2021.核心：主要提出了SpatiallyModulatedCo-Attention(SMCA)mechanism，其作用是constrainingco-attentionresponsestobehighnearinitiallyestimatedboundingboxlocations.可以加快收敛速度6、Z.Yaoetal.Efficientdetr:Improvingend-to-endobjectdetectorwithdenseprior.arXiv:2104.01318,2021.核心：论文认为DETR收敛慢的原因在于初始化的方法不对，提出通过额外的regionproposalnetwork引进先验知识，（Tothisend,theyproposedtheEfficientDETRtoincorporatethedensepriorintothedetectionpipelineviaanadditionalregionproposalnetwork.）通过这样的初始化，只需要一层decoder就可以达到相当的检测性能。7、Xia,Zhuofanetal.“VisionTransformerwithDeformableAttention.”2022IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)(2022):4784-4793.这篇论文的代码和论文自身讲的对不上，怀疑有问题！二、Transformer-basedbackbonemethod1、J.Bealetal.Towardtransformer-basedobjectdetection.arXiv:2012.09958,2020.核心：将transformer作为主干网络结合到传统目标检测框架中。首先把图片划分为patch，如何输入到transformer提取特征，得到的embedding按照空间信息重新组合然后输入到检测头中进行目标检测。具有代表性的transformer主干网络，如:SwinTransformer2、HassaniA,WaltonS,LiJ,etal.NeighborhoodAttentionTransformer.arXivpreprintarXiv:2204.07143,2022.这是一个值得关注的transformer主干网络！NeighborhoodAttention(NA)给定输出特征图X∈RH×W×dX\\inR^{H\\timesW\\timesd}X∈RH×W×d，首先进行一维展开得到X∈Rn×dX\\inR^{n\\timesd}X∈Rn×d，将XXX进行现象映射得到queryQQQ，keyKKK，valueVVV，为每个query找到它的邻居，记为ρ(i)\\rho(i)ρ(i)，规模为kkk，如下图所示：记第iii个query的第jjj个邻居为ρj(i)\\rho_{j}(i)ρj​(i)，且它们的相对位置偏置为B(i,j)B(i,j)B(i,j)，则注意力权重计算如下所示：同样将对应的value找出来，最后，作注意力计算得最终结果，NeighborhoodAttention和SelfAttention的示意图如下：Note：NeighborhoodAttention的关键在于如何快速找出neighbor，但是论文里并没有过多讲述，而是写成了cuda算子，我看不懂3、Hassani,AliandHumphreyShi.“DilatedNeighborhoodAttentionTransformer”ArXivabs/2209.15001(2022):n.pag.借鉴空洞卷积实现的版本，可以有效捕获长距离依赖！三、Light-Weighttransformer1、HuangT,HuangL,YouS,etal.TowardsLight-WeightConvolution-FreeVisionTransformers.arXivpreprintarXiv:2207.05557,2022.2、ShenZ,ZhangM,ZhaoH,etal.Efficientattention:Attentionwithlinearcomplexities[C]//ProceedingsoftheIEEE/CVFwinterconferenceonapplicationsofcomputervision.2021:3531-3539.3、Lee,S.H.,Lee,S.,&amp;Song,B.C.(2021).VisionTransformerforSmall-SizeDatasets.ArXiv,abs/2112.13492.思考：目前为止，看到的基于transformer的目标检测模型都是考虑如何改进从而提高收敛速度，但是这并不会降低计算复杂度，考虑到海洋机器人有限的计算资源，我们需要的不仅是收敛快，计算速度也要快（即实时计算）。目前位置的transformer模型都需要一个CNN（例如ResNet）先提取特征，那这样看，与其说transformer是检测模型的主干网络，不如说是检测模型的检测头。所以说基于transformer的模型的优势是在检测头考虑了全局依赖（而且这种全局依赖不利于小目标检测）。我希望做到的是先用patchembedding，再用deformabletransformer作为主干网络提取特征。（目前采用NeighborhoodAttention作为主干网络提取特征）特征金字塔对于多尺度目标检测依然十分重要不能丢掉，考虑自己的会议论文提出的方法。（刚刚发现Multi-scaleDeformableSelf-Attention完全符合我会议论文里说的多尺度特征应该spatial-aware和scale-aware）transformer的另外一个优势是端到端的集合预测模式（setprediction）。objectquery可以看作是数据集的统计信息，在YOLOX中丢弃decouplehead，采用decoder作为检测头，引入objectquery，改造成端到端的集合预测，达到NMS-free目的。（可能需要放弃这个，因为这种模式采用one-2-one的labelassignment，会导致正负样本不均衡，不利于小目标检测）","link":"https://chenjie04.github.io/post/transformer-yue-du-bi-ji/"},{"title":"【常用Linux命令】","content":"常用命令Ubuntu系统安装Nvidia驱动并配置CUDA安装Nvidia驱动#打开终端，删除旧的驱动$sudoapt-getpurgenvidia*#禁用自带的nouveaunvidia驱动$sudovi/etc/modprobe.d/blacklist.conf#添加内容$blacklistnouveau$optionsnouveaumodeset=0#更新系统修改$sudoupdate-initramfs-u#然后重启$reboot#验证nouveau是否已经禁用,若无任何输出则禁用成功$lsmod|grepnouveau#英伟达官网下载对应的英伟达显卡驱动#ctrl+alt+f1到6其中一个进入命令行界面,此时需要login：电脑账户名称，password：密码，登录到命令行界面。#关闭图形界面$sudosystemctlstopgdm#或者lightdm，看自己系统装的啥，Ubuntu装的是gdm#卸载系统中存在的显卡驱动$sudoapt-getremovenvidia-*#或者$sudoapt-getpurgenvidia*#给文件权限$sudochmoda+xNVIDIA-Linux-x86_64-xxx.run#运行run文件$sudo./NVIDIA-Linux-x86_64-xxx.run-no-x-check-no-nouveau-check#其中：#-no-x-check：安装驱动时关闭X服务#-no-nouveau-check：安装驱动时禁用nouveau#在安装过程中会出现：#安装过程中出现的提示缺少32位库直接ok#重启图形界面，安装成功后，在命令行输入$sudosystemctlstartgdm#按Ctrl+Alt+F7返回图形界面#检测是否安装成功$nvidia-smi#到此驱动就安装好了。配置CUDA到官网找到对应的cuda，按照教程按照#建议在Windows上用迅雷下载好再拷过去$wgethttps://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux.run#运行$sudoshcuda_11.7.1_515.65.01_linux.run另外，在安装过程中一般选择不安装驱动，安装成功后提示============Summary============Driver:NotSelectedToolkit:Installedin/usr/local/cuda-11.7/Samples:Installedin/home/klchang/,butmissingrecommendedlibrariesPleasemakesurethat-PATHincludes/usr/local/cuda-11.7/bin-LD_LIBRARY_PATHincludes/usr/local/cuda-11.7/lib64,or,add/usr/local/cuda-11.7/lib64to/etc/ld.so.confandrunldconfigasrootTouninstalltheCUDAToolkit,runcuda-uninstallerin/usr/local/cuda-11.7/bin***WARNING:Incompleteinstallation!ThisinstallationdidnotinstalltheCUDADriver.Adriverofversionatleast.00isrequiredforCUDA11.7functionalitytowork.Toinstallthedriverusingthisinstaller,runthefollowingcommand,replacing&lt;CudaInstaller&gt;withthenameofthisrunfile:sudo&lt;CudaInstaller&gt;.run--silent--driverLogfileis/var/log/cuda-installer.log安装结束后，添加环境变量到~/.bashrc文件的末尾，具体添加内容如下：$exportLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64$exportPATH=$PATH:/usr/local/cuda/bin$exportCUDA_HOME=$CUDA_HOME:/usr/local/cuda保存后退出。在Terminal中，激活环境变量命令为source~/.bashrc。测试CUDAToolkit。通过编译自带Samples并执行，以验证是否安装成功。具体命令如下所示：$cd/usr/local/cuda/samples/1_Utilities/deviceQuery$sudomake$./deviceQuery如果安装成功，则输出类似于如下信息：./deviceQueryStarting...CUDADeviceQuery(RuntimeAPI)version(CUDARTstaticlinking)Detected1CUDACapabledevice(s)Device0:&quot;GeForceRTX2070withMax-QDesign&quot;CUDADriverVersion/RuntimeVersion11.7/11.7CUDACapabilityMajor/Minorversionnumber:7.5Totalamountofglobalmemory:7982MBytes(8370061312bytes)(36)Multiprocessors,(64)CUDACores/MP:2304CUDACoresGPUMaxClockrate:1125MHz(1.12GHz)MemoryClockrate:5501MhzMemoryBusWidth:256-bitL2CacheSize:4194304bytesMaximumTextureDimensionSize(x,y,z)1D=(131072),2D=(131072,65536),3D=(16384,16384,16384)MaximumLayered1DTextureSize,(num)layers1D=(32768),2048layersMaximumLayered2DTextureSize,(num)layers2D=(32768,32768),2048layersTotalamountofconstantmemory:65536bytesTotalamountofsharedmemoryperblock:49152bytesTotalnumberofregistersavailableperblock:65536Warpsize:32Maximumnumberofthreadspermultiprocessor:1024Maximumnumberofthreadsperblock:1024Maxdimensionsizeofathreadblock(x,y,z):(1024,1024,64)Maxdimensionsizeofagridsize(x,y,z):(2147483647,65535,65535)Maximummemorypitch:2147483647bytesTexturealignment:512bytesConcurrentcopyandkernelexecution:Yeswith3copyengine(s)Runtimelimitonkernels:YesIntegratedGPUsharingHostMemory:NoSupporthostpage-lockedmemorymapping:YesAlignmentrequirementforSurfaces:YesDevicehasECCsupport:DisabledDevicesupportsUnifiedAddressing(UVA):YesDevicesupportsManagedMemory:YesDevicesupportsComputePreemption:YesSupportsCooperativeKernelLaunch:YesSupportsMultiDeviceCo-opKernelLaunch:YesDevicePCIDomainID/BusID/locationID:0/1/0ComputeMode:&lt;Default(multiplehostthreadscanuse::cudaSetDevice()withdevicesimultaneously)&gt;deviceQuery,CUDADriver=CUDART,CUDADriverVersion=11.7,CUDARuntimeVersion=11.7,NumDevs=1Result=PASS下载并安装cuDNN从NVIDIA官方网址https://developer.nvidia.com/rdp/cudnn-download下载cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz。解压压缩包，并把相应的文件，复制到指定目录即可。如下所示：#UnzipthecuDNNpackage.$tar-xvfcudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz#CopythefollowingfilesintotheCUDAtoolkitdirectory.$sudocpcudnn-*-archive/include/cudnn*.h/usr/local/cuda/include$sudocp-Pcudnn-*-archive/lib/libcudnn*/usr/local/cuda/lib64$sudochmoda+r/usr/local/cuda/include/cudnn*.h/usr/local/cuda/lib64/libcudnn*Ubuntu新建用户并添加管理员权限：#先修改新建用户的目录权限，使其他人无权限进入$sudovim/etc/adduser.conf#找到以下内容#IfDIR_MODEisset,directorieswillbecreatedwiththespecified#mode.Otherwisethedefaultmode0755willbeused.DIR_MODE=0755#将0755改为0750DIR_MODE=0750#新建用户$sudoadduserusername#按照提示输入密码等#如有必要将用户设为管理员（注意只能将有Linux使用经验且可信任的用户设为管理员）$sudoadduserusernamesudo#即将username加入sudo组Tmux简单使用教程起因：以前在训练神经网络时，我喜欢使用nohup命令将程序挂到后台运行，这样即便关闭命令行窗口也不会中断训练，训练过程还可以在自己当前目录下的nohup.out文件中查看，nohup命令如下：#nohup表示不挂断运行命令，&amp;符号表示在后台运行$nohuppythontools/train.pyconfigs/yolox/yolox_s_8x8_300e_VOC.py&amp;后来，在多卡并行训练模型的时候，上面这种方式出现错误，报错如下：WARNING:torch.distributed.elastic.agent.server.api:Received1deathsignal,shuttingdownworkersWARNING:torch.distributed.elastic.multiprocessing.api:Sendingprocess4156332closingsignalSIGHUPWARNING:torch.distributed.elastic.multiprocessing.api:Sendingprocess4156333closingsignalSIGHUPTraceback(mostrecentcalllast):File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/runpy.py&quot;,line193,in_run_module_as_main&quot;main&quot;,mod_spec)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/runpy.py&quot;,line85,in_run_codeexec(code,run_globals)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/launch.py&quot;,line193,inmain()File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/launch.py&quot;,line189,inmainlaunch(args)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/launch.py&quot;,line174,inlaunchrun(args)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/run.py&quot;,line713,inrun)(cmd_args)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/launcher/api.py&quot;,line131,incallreturnlaunch_agent(self._config,self._entrypoint,list(args))File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/launcher/api.py&quot;,line252,inlaunch_agentresult=agent.run()File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/elastic/metrics/api.py&quot;,line125,inwrapperresult=f(args,**kwargs)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py&quot;,line709,inrunresult=self._invoke_run(role)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/elastic/agent/server/api.py&quot;,line843,in_invoke_runtime.sleep(monitor_interval)File&quot;/home/user2/anaconda3/envs/mmlab/lib/python3.7/site-packages/torch/distributed/elastic/multiprocessing/api.py&quot;,line60,in_terminate_process_handlerraiseSignalException(f&quot;Process{os.getpid()}gotsignal:{sigval}&quot;,sigval=sigval)torch.distributed.elastic.multiprocessing.api.SignalException:Process4156314gotsignal:1有人在他的博客nohup训练pytorch模型时的报错以及tmux的简单使用-gy77-博客园中说这是nohup的bug，我们可以使用tmux来替换nohup。那使用tmux吧$sudoapt-getinstalltmux#安装$tmux#进入tmux窗口$exit#推出tmux窗口，或者使用快捷键[Ctrl+d]$tmuxnew-s${session-name}#创建一个会话，并设置绘画名#快捷键[Ctrl+b]是tmux的前缀键，用完前缀键后可以继续按指定键来完成指定命令#[Ctrl+b][d]#将会话与窗口分离，或者[Ctrl+b]tmuxdetach$tmuxls#查看所有会话，或者使用tmuxlist-session$tmuxattach-t${session-name}#根据会话名将terminal窗口接入会话$tmuxkill-session-t${session-name}#根据会话名杀死会话$tmuxswitch-t${session-name}#根据会话名切换会话$tmuxrename-session-t0${session-name}#根据会话名，重命名会话注意：尤其需要注意的是离开会话的时候，可能这时候我们的程序在跑着，没办法输入tmuxdetach命令离开，这时候必须用快捷键[Ctrl+b][d]就是先按Ctrl+b，然后按d键tmux的一个简单使用流程[terminal]:tmuxnew-strain_model#创建一个会话，并设置绘画名:train_model[tmux]:condaactivateenv_name#在tmux会话中，我们激活我们要使用的conda环境[tmux]:pythontrain.py#在tmux会话中，开始训练我们的模型[tmux]:[Ctrl+b][d]#将会话与窗口分离[terminal]:tmuxls#查看我们刚刚创建的会话[terminal]:watch-n1-cgpustat--color#监控我们的gpu信息使用tmux遇到第一个问题：在tmux窗口鼠标不能滚动解决方案：按完前缀ctrl+B后，再按冒号：进入命令行模式，输入以下命令：set-gmouseon#要永久设置就在~/.tmux.conf文件写入该命令使用tmux遇到第二个问题：在tmux窗口不能复制粘贴解决方案：Step1.按住shift，鼠标左键选择内容Step2.Ctrl+Shift+C复制Step3.Ctrl+Vmmdetection运行dist_train.sh文件报Addressalreadyinuse错误其实原因是一台机子上跑了两个mmdetection代码导致节点冲突，我已经运行了一个dist_train任务，启动第二个任务的时候就报错了解决方案：#!/usr/bin/envbashCONFIG=$1GPUS=$2NNODES=${NNODES:-1}NODE_RANK=${NODE_RANK:-0}#将这个默认端口改成一个没有被占用的就行，#比如我现在只运行了一个dist_train任务，只占用了29500，那我就改成29501PORT=${PORT:-29500}MASTER_ADDR=${MASTER_ADDR:-&quot;127.0.0.1&quot;}PYTHONPATH=&quot;$(dirname$0)/..&quot;:$PYTHONPATH\\python-mtorch.distributed.launch\\--nnodes=$NNODES\\--node_rank=$NODE_RANK\\--master_addr=$MASTER_ADDR\\--nproc_per_node=$GPUS\\--master_port=$PORT\\$(dirname&quot;$0&quot;)/train.py\\$CONFIG\\--seed0\\--launcherpytorch${@:3}安装并使用Zsh1.安装$sudoapt-getinstallzsh2.安装oh-my-zsh$wgethttps://ghproxy.com/https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh$shinstall.sh3.设置主题$vim~/.zshrc#找到主题设置ZSH_THEME=&quot;robbyrussell&quot;改为ZSH_THEME=&quot;bira&quot;4.配置自动补全$gitclonehttps://ghproxy.com/https://github.com/zsh-users/zsh-autosuggestions$ZSH_CUSTOM/plugins/zsh-autosuggestions#编辑.zshrc文件（找到plugins=(git)这一行，如果没有添加。更改为如下）plugins=(gitzsh-autosuggestions)5.设置为默认shell$chsh-s/bin/zsh#如果要改回默认：chsh-s/bin/bash6.将tmux窗口的shell也配置为zsh按完前缀ctrl+B后，再按冒号：进入命令行模式，输入以下命令后回车：set-gdefault-command/bin/zsh","link":"https://chenjie04.github.io/post/chang-yong-linux-ming-ling/"},{"title":"（未完成）笔记：Underwater Image Enhancement ","content":"水下图像由于光线的选择性吸收和散射等原因造成了颜色偏差、模糊等退化现象，导致目标检测等水下探测活动效果较差，因此研究水下图像增强成为了一个研究热点。下图是水下图像增强的示例，其中(a)是原图，(b-j)是不同水下图像增强算法的增强效果，水下图像增强就是设计算法，给算法喂入模糊的水下图像得到增强后的图像。1、UncertaintyInspiredUnderwaterImageEnhancement设计深度学习水下图像增强算法最大的难题在于无法获取真正的GroundTruth（也叫referenceimages），所以现有的方法都是先采用一些state-of-the-art的增强算法去生成一些增强后的图片，然后人为主观地去挑选一张最好的作为GroundTruth，但是人为挑选就不可避免地存在不确定性，目前的方法都不能很好地处理这个问题。2、SimpleBaselinesforImageRestoration3、AdaptiveUncertaintyDistributioninDeepLearningforUnsupervisedUnderwaterImageEnhancement","link":"https://chenjie04.github.io/post/bi-ji-underwater-image-enhancement-method-based-on-denoising-diffusion-probabilistic-model/"},{"title":"mmdetection-3.x学习笔记——默认运行时配置（default_runtime.py）","content":"一、学习配置文件MMDetection仓库使用MMEngine的配置文件系统，所有功能的模块都可以通过配置文件进行配置。1、默认运行时配置（configs/base/default_runtime.py）default_scope='mmdet'#默认的注册器域名，默认从此注册器域中寻找模块。default_hooks=dict(#默认hook，大部分定义在mmengin中timer=dict(type='IterTimerHook'),#迭代一次实践统计logger=dict(type='LoggerHook',interval=50),#日志打印param_scheduler=dict(type='ParamSchedulerHook'),#参数（学习率等）调度checkpoint=dict(type='CheckpointHook',interval=1),#保存checkpoint权重sampler_seed=dict(type='DistSamplerSeedHook'),#设置数据采样的随机种子，确保shuffle生效visualization=dict(type='DetVisualizationHook'))#用于可视化验证或测试过程的预测结果，定义在mmdet中env_cfg=dict(cudnn_benchmark=False,#是否启用cudnnbenchmarkmp_cfg=dict(mp_start_method='fork',opencv_num_threads=0),#使用fork来启动多进程。'fork'通常比'spawn'更快，但可能存在隐患。请参考https://github.com/pytorch/pytorch/issues/1355。关闭opencv的多线程以避免系统超负荷dist_cfg=dict(backend='nccl'),##分布式相关设置)#可视化检测结果，在拼接图像中，其中左侧图像是groundtruth，右侧图像是预测。#定义在MMDET.VISUALIZATION.LOCAL_VISUALIZER#-如果show为True，则忽略所有存储后端，图像将显示在本地窗口中。#-如果指定out_file，绘制的图像将保存到out_file,通常在显示器不可用时使用。vis_backends=[dict(type='LocalVisBackend')]#可视化后端visualizer=dict(type='DetLocalVisualizer',vis_backends=vis_backends,name='visualizer')#log_processor=dict(type='LogProcessor',#日志处理器用于处理运行时日志window_size=50,#日志数值的平滑窗口by_epoch=True)#是否使用epoch格式的日志。需要与训练循环的类型保存一致。log_level='INFO'#日志等级load_from=None#从给定路径加载模型检查点作为预训练模型。这不会恢复训练。resume=False#是否从`load_from`中定义的检查点恢复。如果`load_from`为None，它将恢复`work_dir`中的最新检查点。（1）MMEngin核心组件分析（一）：Hook（2）配置程序运行环境","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji/"},{"title":"（未完成）【CTH博士论文】基于强化学习的自动驾驶决策 ","content":"论文地址：https://research.chalmers.se/en/publication/526543","link":"https://chenjie04.github.io/post/cth-bo-shi-lun-wen-ji-yu-qiang-hua-xue-xi-de-zi-dong-jia-shi-jue-ce/"},{"title":"【论文阅读笔记】RTMDet: An Empirical Study of Designing Real-Time Object Detectors","content":"论文地址：RTMDet:AnEmpiricalStudyofDesigningReal-TimeObjectDetectors代码地址：https://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet这是一份关于如何设计实时目标检测器的实验报告，目的在于设计出超越YOLO系列的实时目标检测器，并且易于拓展到旋转目标检测和实例分割任务。一、宏观架构实时目标检测一直是YOLO系列等一阶段检测器的目标，所以设计团队对模型的宏观设计遵循一阶段检测器，如下图所示，主要分为backbone、neck和head。二、模型1、Basicbuildingblack较大的有效感受野有利于像目标检测和语义分割之类的密集预测任务，因为它可以更有效地捕获和建模图像中上下文信息。然而像空洞卷积、非局部注意力这些计算复杂度较高不适合实时目标检测。最近，有研究采用更大的卷积核来增大感受野，配合深度可分离卷积，可以有效控制其计算代价。因此作者在主干网络CSPDarkNet中的基础模块中引入一个5×55\\times55×5的深度可分离卷积，如下图（b）所示。2、Balanceofmodelwidthanddepth采用深度可分离卷积必须额外接一个逐点卷积层（point-wiseconvolution），这样模型的深度增加，因此设计团队适当减少主干网络的每个stage的基础模块并稍微增加模型宽度。3、Balanceofbackboneandneck一个更heavier的颈部网络用于融合多尺度特征对检测不同尺度的目标十分有必要。设计团队提高颈部网络基础模块的膨胀比，以将更多参数和计算放到颈部，提高模型能力。4、Shareddetectionhead不同尺度特征图共享检测头，但是得配备不同的BatchNormalization(BN)层。5、Labelassignmentandlosses基于SimOTA设计动态标签分配策略，以代价函数作为分配标准，代价函数如下，C=λ1Ccls+λ2Creg+λ3Ccenter,C=\\lambda_1C_{cls}+\\lambda_2C_{reg}+\\lambda_3C_{center},C=λ1​Ccls​+λ2​Creg​+λ3​Ccenter​,其中，Ccls,CregC_{cls},C_{reg}Ccls​,Creg​和CcenterC_{center}Ccenter​分别表示分类损失、回归损失以及区域先验损失；λ1=1,λ2=3\\lambda_1=1,\\lambda_2=3λ1​=1,λ2​=3以及λ3=1\\lambda_3=1λ3​=1为默认参数。在分类代价函数中，原来的SimOTA采用二进制标签计算损失，这会导致一个问题：对于某个分类分数高但边界框不正确的预测结果会得到一个较低的分类代价，反之亦然。设计团队提出计算预测结果和真实值之间的IoU作为软标签YsoftY_{soft}Ysoft​，分类代价函数如下，Ccls=CE(P,Ysoft)×(Ysoft−P)2.C_{cls}=CE(P,Y_{soft})\\times(Y_{soft}-P)^2.Ccls​=CE(P,Ysoft​)×(Ysoft​−P)2.回归代价函数中，原来的方法采用GIoU，这样的结果是最好的匹配和最差的匹配相差不过1，很难区分匹配质量，设计团队采用IoU的对数作为回归代价，具体如下，Creg=−log(IoU)C_{reg}=-log(IoU)Creg​=−log(IoU)原来的方法在固定中心区域内选取前kkk个总代价最小作为真实值的正样本，在这里，设计团队加入了区域先验代价CcenterC_{center}Ccenter​，指定了一个软中心区域，区域先验损失如下，Ccenter=α∣xpred−xgt∣−β,C_{center}=\\alpha^{|x_{pred}-x_{gt}|-\\beta},Ccenter​=α∣xpred​−xgt​∣−β,α=10,β=3\\alpha=10,\\beta=3α=10,β=3为默认超参数。接下来选择kkk个总代价最小作为正样本即可。6、CachedMosaicandMixUp采用缓存技术实现Mosaic和MixUp等需要多张图片的数据增广技术，把图片放在缓存中，训练时就不需要加载多张图片就可以实现Mosaic和MixUp，极大地提高数据加载速度。7、Two-stagetraining两段式训练是指将训练过程分成两个阶段，第一阶段采用强数据增广Mosaic和MixUp，第二阶段采用弱数据增广，即不用Mosaic和MixUp。YOLOX还在第二阶段引入L1损失函数来微调回归分支。在这里，设计团队不再引入L1损失函数，将第一阶段数据增广时混合图片数量提高到8，且第一阶段设置为280epochs；在最后20个epochs中采用LargeScaleJittering(LSJ)做为数据增广。两段式训练的原因是强数据增广虽然能够提高模型的泛化性，但是会导致训练数据分布与真实分布不一致，所以需要在第二阶段使用与真实分布更接近的数据进行微调。8、ImplementationDetails9、Experimentalresult","link":"https://chenjie04.github.io/post/bi-ji-rtmdet-an-empirical-study-of-designing-real-time-object-detectors/"},{"title":"【论文阅读笔记】InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions","content":"论文地址：https://arxiv.org/abs/2211.05778代码地址：https://github.com/OpenGVLab/InternImage一、引言现在深度学习都去研究大模型了，不管CV还是NLP，都上大模型。这篇文章也一样，它说现在CV任务里面用的大模型都是基于transformer的，效果相当不错，可以说是横扫一切！但是，作者说卷积神经网络配合相似的算子/架构级别的精心设计，在大模型、大数据的条件下也可以做到类似、甚至更好的性能。就是说你transformer能做到的，我CNN也能做到。这篇文章充分展示了怎么去设计一个神经网络模型！值得学习~为了设计出不弱于transformer的CNN大模型，就得先分析它们的不同之处，（1）从算子层面来看，transformer的基础模块多头注意力具有学习长距离依赖的能力以及可以进行自适应的空间信息聚合的能力（如下图（a）所示）；（2）从架构层面看，transformer除了多头注意力还有其他一些先进的模块，例如LayerNormalization（LN）、Feed-forwardnetwork（FFN）、GELU等。Transformer有优势，也有缺点，就是计算复杂度极高，为了克服这个问题，有人就对多头注意力做一些限制，使它从globalattention变成localattention，这样，复杂度问题解决了，但是却牺牲了长距离依赖的学习能力（如下图（b）所示）。为了给CNN模型引入长距离依赖的学习能力，有些工作[1-2]就采用了超级大的卷积核（比如：31×3131\\times3131×31），但是这种超大卷积核的空间信息聚合能力不能自适应，因为卷积核的权重是固定的（如下图（c）所示）。现有算子中恰好还有可变形卷积，它能够满足我们想要的三个性质：长距离依赖、自适应空间信息聚合以及高计算效率（如下图（d）所示）。首先，可变形卷积的采样偏移量可以灵活地从给定的数据中动态地学习适当的感受野（可以是长距离的或短距离的）；其次，采样偏移量和调制标量都是针对输入数据自适应的；最后，采样常规3×33\\times33×3卷积核，避免了计算复杂度爆炸，提高了计算效率。因此，本文将基于可变形卷积设计大模型。二、InternImage模型1、核心算子长距离依赖。现有研究证明大的有效感受野（长距离依赖）对下游的视觉任务有用，但是常规3×33\\times33×3卷积核的感受野相对很小，即便是很深的模型依然不能捕获类似于transformer的长距离依赖。自适应空间信息聚合。与transformer根据输入动态决定权重不同，常规卷积的权重固定且具有很强的归纳偏置，比如二维局部性、近邻结构性以及平移不变性等。高归纳偏置的模型收敛更快，需要的训练数据更少，但是也限制了模型无法从大规模数据集中学习更加通用和鲁棒的表征。（1）DCNv2回顾同时满足上述两个性质的卷积算子的DCN。给定输入x∈RC×H×W\\mathbf{x}\\in\\mathbb{R}^{C\\timesH\\timesW}x∈RC×H×W和当前像素点p0p_0p0​，DCNv2如下式所示：y(p0)=∑k=1Kwkmkx(p0+pk+Δpk)，\\mathbf{y}(p_0)=\\sum_{k=1}^{K}\\mathbf{w}_{k}\\mathbf{m}_{k}\\mathbf{x}(p_0+p_k+\\Deltap_k)，y(p0​)=k=1∑K​wk​mk​x(p0​+pk​+Δpk​)，其中，KKK表示采样点的数量，如采用3×33\\times33×3卷积核采样点数量则为9，kkk遍历所有采样点。pkp_kpk​表示预定义采样网格{(−1,−1,),(−1,0),...,(0,+1),...,(+1,+1)}\\left\\{(-1,-1,),(-1,0),...,(0,+1),...,(+1,+1)\\right\\}{(−1,−1,),(−1,0),...,(0,+1),...,(+1,+1)}的第kkk个位置；Δpk\\Deltap_kΔpk​表示第kkk个采样网格位置的偏移量，有了这个偏移量，DCN就可以灵活地学习短距离或长距离依赖。mk∈R\\mathbf{m}_{k}\\in\\mathbb{R}mk​∈R表示第kkk个采样点的调制标量，经过sigmoidsigmoidsigmoid函数的标准化。wk∈RC×C\\mathbf{w}_{k}\\in\\mathbb{R}^{C\\timesC}wk​∈RC×C是第kkk个采样点的映射权重。根据输入数据x\\mathbf{x}x可学习的调制标量mk\\mathbf{m}_{k}mk​和位置偏移量Δpk\\Deltap_kΔpk​使得DCN能够进行自适应的空间信息聚合。（2）DCVv2改造为了适应大模型，作者对DCNv2进行改造，1）卷积神经元（一个常规的3×33\\times33×3卷积核具有9个线性映射神经元）之间共享权重。回顾一下原始的DCNv2：对于每个采样点对应的向量x(p0+pk+Δpk)\\mathbf{x}(p_0+p_k+\\Deltap_k)x(p0​+pk​+Δpk​)首先会乘上一个调制标量mk∈R\\mathbf{m}_{k}\\in\\mathbb{R}mk​∈R，然后再拿一个权重向量wk∈RC×C\\mathbf{w}_{k}\\in\\mathbb{R}^{C\\timesC}wk​∈RC×C去乘，进行线性映射。也就是每个采样点都会对应一个不同的映射向量，这样会是得计算复杂度跟采样点数量成线性关系，作者为了提高计算效率，对每个采样点都采用同一个映射权重，即w∈RC×C\\mathbf{w}\\in\\mathbb{R}^{C\\timesC}w∈RC×C与位置kkk无关。（其思想与深度可分离卷积类似，但并不完全一致）2）引入多组机制。Transformer的核心算子是多头注意力，即将输入沿通道分成多个组，在每个组内分别进行运算，这样每个子空间中都可以学习不同的表征，有利于提高模型的表达能力。因此，作者引入多组机制，即分组卷积。3）沿采样点规范化调制标量。DCNv2中每个采样点的调制标量是直接经过sigmoidsigmoidsigmoid缩放到[0−1][0-1][0−1]之间，这样调制标量的总和会在[0−K][0-K][0−K]之间变化，导致训练不稳定，因此作者对所有的调制标量进行softmaxsoftmaxsoftmax规范化，使得调制标量总和为1。基于上述改进，作者提出了DCNv3，如下所示，y(p0)=∑g=1G∑k=1Kwgmgkxg(p0+pk+Δpgk)，\\mathbf{y}(p_0)=\\sum_{g=1}^{G}\\sum_{k=1}^{K}\\mathbf{w}_{g}\\mathbf{m}_{gk}\\mathbf{x}_g(p_0+p_k+\\Deltap_{gk})，y(p0​)=g=1∑G​k=1∑K​wg​mgk​xg​(p0​+pk​+Δpgk​)，其中，GGG表示分组数量，其他符号的意义与原来类似，wg∈RC×C′\\mathbf{w}_{g}\\in\\mathbb{R}^{C\\timesC^{&#x27;}}wg​∈RC×C′表示映射权重，C′=C/GC^{&#x27;}=C/GC′=C/G表示分组后每组的通道维度，mgk∈R\\mathbf{m}_{gk}\\in\\mathbb{R}mgk​∈R表示调制标量。2、模型构建（1）basicblockBasicblock采用transformer常用的结构设置，如下图所示，其中位置偏移量和调制标量通过一个独立卷积层（a3×3depth-wiseconvolutionfollowedbyalinearprojection）来学习。（2）Stem&amp;downsamplinglayersStem和downsamplinglayers的设置如下，（3）StackingrulesStackingrules决定了如何使用基本模块组成整体模型，首先定义几个基本符号：CiC_iCi​:第iii个stage的通道数；GiG_iGi​:第iii个stage的分组数；LiL_iLi​:第iii个stage的basicblock数量；通常一个模型有4个stage，每个stage由上面3个参数决定。为降低搜索空间，作者总结前人经验，提出了4个规则：1）后3个stage的通道数由第一个通道数决定，Ci=2i−1C1C_i=2^{i-1}C_1Ci​=2i−1C1​；2）分组数由通道数决定，Gi=Ci/C′G_i=C_i/C^{&#x27;}Gi​=Ci​/C′；3）第1、2、4stage中的basicblock数量一样，即采用&quot;AABA&quot;的模式，L1=L2=L4L_1=L_2=L_4L1​=L2​=L4​；4）第3个stage的basicblock数量要大于其他stage的数量，L1≤L3L_1\\leL_3L1​≤L3​。这样，一个模型就可以由4个参数决定，分别为(C1,C′,L1,L3)(C_1,C^{&#x27;},L_1,L_3)(C1​,C′,L1​,L3​)，然后对4个参数取不同值进行超参数搜索，最终找到基础模型的最优超参数为(64,16,4,18)(64,16,4,18)(64,16,4,18)。(4)Scalingrules主要是考虑缩放模型的深度和宽度来构建大模型，记模型深度为D=(3L1+L3)D=(3L_1+L_3)D=(3L1​+L3​)，宽度C1C_1C1​，因子α,β,ϕ\\alpha,\\beta,\\phiα,β,ϕ，Scalingrules为D′=αϕD,C1′=βϕC1D^{&#x27;}=\\alpha^{\\phi}D,C_1^{&#x27;}=\\beta^{\\phi}C_1D′=αϕD,C1′​=βϕC1​，其中，α≥1,β≥1,αβ1.99≈2\\alpha\\geq1,\\beta\\geq1,\\alpha\\beta^{1.99}\\approx2α≥1,β≥1,αβ1.99≈2，在这里，1.99是特定于InternImage模型的，通过将模型宽度加倍并保持深度恒定来计算。最终，各个规模的模型配置设置如下，三、实验结果目标检测和语义分割破纪录，COCOtest-dev目标检测数据集达到65.4mAP，ADE20K语义分割数据集达到62.9mIoU。目标检测，用的是DINO检测器，结果如下，四、总结现在transformer说要一统江湖，CNN说廉颇未老尚能再战，其实背后的思想发展脉络已经决定了未来的结局。最开始的深度神经网络是MLP，号称只要模型够大便可以拟合任意函数，只是当时算力不够，标注数据太少且质量太差，训练不出来这样的模型；因此引入归纳偏置，归纳偏置就是人类洞察的关于事物的认知，视觉任务引入高归纳偏置的CNN，语言处理引入RNN。高归纳偏置可以提高模型收敛速度和训练数据的需求。Transformer的兴起是得益于高算力和大数据，直接舍弃部分归纳偏置，直接从数据中学习更通用更鲁棒的知识。Transformer现在面临的问题是计算复杂度高，说到底还是算力不够，所以又考虑重新引入归纳偏置，比如这篇文章。现在一个很现实的考虑就是可以把这篇文章的DCNv3改造成一维卷积，这样便可以同时处理视觉任务和自然语言任务。参考文献：[1]ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie.Aconvnetforthe2020s.arXivpreprintarXiv:2201.03545,2022.2,3,5,6,7,8[2]XiaohanDing,XiangyuZhang,JungongHan,andGuiguangDing.Scalingupyourkernelsto31x31:Revisitinglargekerneldesignincnns.InIEEEConf.Comput.Vis.PatternRecog.,pages11963–11975,2022.2,3,5,6,7,8","link":"https://chenjie04.github.io/post/bi-ji-internimage-exploring-large-scale-vision-foundation-models-with-deformable-convolutions/"},{"title":"WSL 2 配置代理（转载）","content":"原文地址：https://www.cnblogs.com/tuilk/p/16287472.html","link":"https://chenjie04.github.io/post/wsl-2-pei-zhi-dai-li-zhuan-zai/"},{"title":"强化学习入门笔记","content":"——根据台大李宏毅老师的《强化学习》课程笔记而来！一、什么是强化学习当我们谈论什么是强化学习时总会看到上面这张图：强化学习就是智能体（Actor）对环境进行观测，然后根据观测结果（Observation）采取相应的动作（Action），动作作用于环境（Environment）之后，环境就会产生相应的变化，这样智能体就可以再一次得到新的观测，同时环境也会给智能体一个反馈（Reward）表明这个动作是好的还是不好的。强化学习就是想办法找到一个策略能够最大化总的反馈（Totalreward）。以一个打“SpaceInvader”小游戏的例子来进行说明，Actor去打小游戏，那么这里的环境就是游戏机，游戏的画面就是Actor可以观测到画面，Actor根据看到的画面控制己方太空梭左右移动或开火击杀小外星人，比如这里Actor采取的动作的开火，游戏机会根据Actor采取的动作给予反馈，比如这里左右移动没有分，而开火并击杀外星人得5分。那么这里强化学习就是训练Actor使他能够得到最高的分数。上面这些是介绍强化学习都必然提到的基础知识，但是强化学习到底该怎么做呢？还是一头雾水啊！二、怎么训练强化学习之前说过机器学习的三步走很简单：第一步是定义一个带有未知参数的函数（比如神经网络），然后根据训练数据设计损失函数，第三步就是优化。其实强化学习也一样。在强化学习里面，带有未知参数的函数就是我们的Actor或者交PolicyNetwork，它的输入是游戏画面，输出的采取各个动作的概率。其实这跟我们的图像分类问题的一样的，输入图像，输出图像所属的类别，所以说我们甚至可以直接采用图像分类的网络架构。唯一不同的是在强化学习里面不是直接根据预测的概率选择动作，而是在这些动作里再做一次抽样。这一步在强化学习里面很重要，这叫Exploration，它可以增加Actor采取行动的随机性，这样可以探索一些以前没有考虑过的行为，从而增大习得最优策略的可能性。那么我们怎么定义强化学习的损失函数呢？前面说过Actor观测到一个画面就会采取一个动作，同时环境会给它一个反馈rtr_{t}rt​，直到己方的太空梭被摧毁或者杀光外星人，游戏结束。游戏结束后，我们得到一个Totalreward：R=∑t=1TrtR=\\sum^{T}_{t=1}r_{t}R=∑t=1T​rt​，我们的任务就是要最大化Totalreward，那么我们就可以把Totalreward取负号当作我们的损失函数。最后，强化学习该怎么优化？在讨论之前，我们先用另外一幅图示重新审视强化学习。如上图所示，首先Actor从环境环境中得到第一个观测s1s_{1}s1​，根据s1s_{1}s1​采取动作a1a_{1}a1​,a1a_{1}a1​作用于环境之后得到s2s_{2}s2​，Actor根据s2s_{2}s2​采取动作a2a_{2}a2​，如此循环往复直到游戏结束，我们就得到了一条Trajectoryτ={s1,a1,s2,a2,...}\\tau=\\{s_{1},a_{1},s_{2},a_{2},...\\}τ={s1​,a1​,s2​,a2​,...}。然后，我们来看Reward，可以把Reward当作一个函数，判断Actor根据sis_{i}si​采取动作aia_{i}ai​是否合理。当游戏结束，我们就可以得到Totalreward。我们知道Actor是一个神经网络，其实如果环境和Reward都是神经网络的话，整个强化学习模型就像一个RNN，采用RNN的方法就可以解了。但是，问题在于环境和reward都是黑盒，而且带有一定的随机性，比如用强化学习下棋的时候，Actor下完一子之后，它的环境（也就是它的对手——人会采取哪一步是不确定的）。另外，Actor根据观测采取的动作也是具有随机性的，因为action是要随机抽样的。这两个问题让强化学习的训练十分困难，强化学习的核心也就在优化这一步。接下来，我们将一步一步去接这个强化学习的优化问题。首先，我们得研究怎么控制我们的Actor，让它在给定的观测时做或者不做指定的动作。我们可以先把它想象成一个分类问题，假如我们知道给定观测应该采用的动作ground-truth，比如给定观测sss应该采取的动作为a^\\hat{a}a^，我们只需要计算Actor的输出aaa与a^\\hat{a}a^的交叉熵并将它定义为Loss，那么找到一个参数θ\\thetaθ最小化Loss就可以让Actor采取特定的动作。那么我们又是怎么让Actor不要采取指定的动作呢？只需要将Loss定义为负的交叉熵即可。所以说我们是可以控制我们的Actor的，只要我们能够给予合适的标签数据。所以，我们希望可以收集到这样的训练数据，指定在什么样的观测采取什么样的动作，在什么样的观测时不要采取什么动作，这样我们就可以根据训练数据来定义我们的Loss，最后通过最小化Loss来训练我们的强化学习。甚至，我们进一步制定每个动作的重要程度，比如s1s_{1}s1​时比较希望采取动作a1a_{1}a1​，但是在sNs_{N}sN​时绝对不要采取动作aNa_{N}aN​,此时，我们的Loss就变成了L=∑AnenL=\\sumA_{n}e_{n}L=∑An​en​。接下来，我们的重点就是怎么去收集这样的训练数据和怎么定义每个动作的重要程度AnA_{n}An​。Version0Version0是一个简单单不正确的版本，我们就先随机初始化一个Actor，然后就让它去玩几把游戏，然后把{(si,ai)∣i=1...N}\\{(s_{i},a_{i})\\|\\i=1...N\\}{(si​,ai​)∣i=1...N}记录下来就得到了训练数据。另外，在玩游戏的过程中，Actor每采取一个动作就可以得到一个reward，我们把每次的reward都记录下来作为评价action重要程度的指标，这样我们就可以训练强化学习了。但是这样训练出来的Actor是一个短视的Actor，急功近利！比如在spaceinvader游戏里面，左右移动都没有得分，只有开火并击杀外星人才会得分，这样训练出来的Actor，可能就是一个只会盲目开火的废材，因为我们知道有时候我们需要左右移动进行瞄准才会射击得准。换句话说动作a1a_{1}a1​会影响动作a2a_{2}a2​的得分，a2a_{2}a2​会影响a3a_{3}a3​，以此类推，它们是相互关联的，而不是独立的。为了解决Actor这种短视的问题，我们引入Version1.Version1在Version1中，我们不是将每个动作的reward作为评判重要性的指标，而是将当前reward以及往后直到游戏结束的所有reward加起来得到累积奖励，把这个累积奖励作为评判重要性的指标。这样的话，我们的Actor可能就学会做一个长期规划，学会放弃短期利益来获取长期更大的收益。但是，version1也存在问题，那就是当游戏很长的时候，你说a1a_{1}a1​会影响到a3a_{3}a3​的reward我信，但是你说rNr_{N}rN​也是a1a_{1}a1​的功劳，那就不太可能了。换句话说就是当前动作可能会对后续动作产生影响，但是这个影响不可能是无穷的。因此，我们迎来了Version2.Version2Version2中主要是引入一个衰减因子γ&lt;1\\gamma&lt;1γ&lt;1，如上图所示，衰减累积奖励记为Gt′=∑n=tNγn−trnG^{&#x27;}_{t}=\\sum^{N}_{n=t}\\gamma^{n-t}r_{n}Gt′​=∑n=tN​γn−trn​，这样离得越远影响就越小，合情合理（从此之后为了不拗口，衰减累积奖励都简称为累积奖励）。但是这就完美了吗？其实并没有。我们知道动作有好有坏，所以ANA_{N}AN​应该有正有负，但是比如在spaceinvader里面，左右移动得0分，击杀外星人得5分，那么它的衰减累积奖励就全都是正的啊。为了解决这个问题，我们就来到了version3.Version3在version3中考虑的问题是这样，对于那些衰减累积奖励全是正的问题，一个行为的好坏是相对的，比如rn=10r_{n}=10rn​=10，看起来它可能是好的，但是如果所有其他rn≥10r_{n}\\ge10rn​≥10，那么rn=10r_{n}=10rn​=10就是不好的。所以说version3就是在衰减累积奖励的基础上减去一个baseline，衰减累积奖励让累积奖励有正有负，从而更好地判断一个动作是好的还是不好的。这个版本就是我们熟知的PolicyGradient.PolicyGradient接下来，我们就总结一下PolicyGradient这个方法是怎么做的。首先就是初始化actor为θ0\\theta^{0}θ0，然后进入迭代训练，在每次迭代中，首先让之前的actorθi−1\\theta^{i-1}θi−1与环境进行交互，然后收集训练数据τ={(s1,a1),(s2,a2),...,(sN,aN)}\\tau=\\{(s_{1},a_1),(s_2,a_2),...,(s_N,a_N)\\}τ={(s1​,a1​),(s2​,a2​),...,(sN​,aN​)},然后计算{A1,A2,...,AN}\\{A_1,A_2,...,A_N\\}{A1​,A2​,...,AN​}，计算Loss，最后采用梯度下降更新actor。曾经，我有个疑问：为什么在policygradient中是计算所有训练数据的loss再更新actor网络，即收集一遍数据只能更新actor一次，而不是采用深度学习里面的mini-batch的训练方式。一个比较合理的解释是“吾之蜜糖，彼之砒霜”，这里的训练数据是用θi−1\\theta^{i-1}θi−1收集的，一旦你中间对θi−1\\theta^{i-1}θi−1更新了，那么可能剩下的数据可能就不适用于训练θi−1\\theta^{i-1}θi−1了。但是其实这个问题就见仁见智了。On-policyv.sOff-policy其实，像上面这种被训练的Actor和拿去跟环境交互的Actor是同一个的情形，被称为on-policy。On-policy的强化学习最好就是收集一次数据更新一次模型。但是，我们人类可以根据别人的经验学习啊，看着别人去做一件事情，我们也可以学会啊！这种被训练的Actor和拿去跟环境交互的Actor不同的情形，被称为off-policy。Off-policy的优势在于采取特定手段我们就可以收集一次数据更新很多次模型。其中，一个典型的off-policy算法就是ProximalPolicyOptimization(PPO)。ProximalPolicyOptimization(PPO)PPO这个方法它之所以能够从on-policy转到off-policy，主要是基于importancesampling。Importancesampling假设xxx服从未知分布p(x)p(x)p(x),如果我们要求f(x)f(x)f(x)的期望Ex∼p[f(x)]E_{x\\simp}[f(x)]Ex∼p​[f(x)]，那么我们可以从p(x)p(x)p(x)中抽样一堆数据xix^ixi，然后通过求均值来近似f(x)f(x)f(x)的期望，即Ex∼p[f(x)]≈1N∑i=1Nf(xi)E_{x\\simp}[f(x)]\\approx\\frac{1}{N}\\sum^{N}_{i=1}f(x^i)Ex∼p​[f(x)]≈N1​i=1∑N​f(xi)但是有时候我们却无法从p(x)p(x)p(x)中抽样，只能在q(x)q(x)q(x)中抽样。就好像现在我们只有θi−1\\theta^{i-1}θi−1收集到的数据，却没有θi\\theta^iθi收集的数据，θi−1\\theta^{i-1}θi−1收集到的数据只能用于更新θi−1\\theta^{i-1}θi−1，不能用来更新θi\\theta^iθi。off-policy想要达到的目的就是希望可以利用θi−1\\theta^{i-1}θi−1收集到的数据多次更新θiθ^iθi,从而提高效率。回到我们的importancesampling中，现在我们就是希望可以利用部分q(x)q(x)q(x)中抽样到的数据来计算f(x)f(x)f(x)的期望$E_{x\\simp}[f(x)]$。我们将Ex∼p[f(x)]E_{x\\simp}[f(x)]Ex∼p​[f(x)]写成积分的形式，然后上下同乘q(x)q(x)q(x)，然后就可以写成从q(x)q(x)q(x)里面抽样的数据来求期望。Ex∼p[f(x)]=∫f(x)p(x)dx=∫f(x)p(x)q(x)q(x)dx=Ex∼q[f(x)p(x)q(x)]\\begin{aligned}E_{x\\simp}[f(x)]=&amp;\\intf(x)p(x)dx\\\\=&amp;\\intf(x)\\frac{p(x)}{q(x)}q(x)dx\\\\=&amp;E_{x\\simq}[f(x)\\frac{p(x)}{q(x)}]\\end{aligned}Ex∼p​[f(x)]===​∫f(x)p(x)dx∫f(x)q(x)p(x)​q(x)dxEx∼q​[f(x)q(x)p(x)​]​从上面的式子，我们知道其实我们是可以通过一个权重p(x)q(x)\\frac{p(x)}{q(x)}q(x)p(x)​修正两个分布之间的差异。当然，分布p(x)p(x)p(x)和q(x)q(x)q(x)不能差异太大。这就是importancesampling。我们回顾一下，policygradient的目的是最大化期望的reward，根据version3的说法可以写成E(st,at)∼πθi[Aθi(st,at)]E_{(s_t,a_t)\\sim\\pi_{\\theta^{i}}}[A^{\\theta^{i}}(s_t,a_t)]E(st​,at​)∼πθi​​[Aθi(st​,at​)]，根据公式∇f(x)=f(x)∇logf(x)\\nablaf(x)=f(x)\\nablalogf(x)∇f(x)=f(x)∇logf(x),梯度可以写为=E(st,at)∼πθi[Aθi(st,at)∇logpθi(atn∣stn)]=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i}}}[A^{\\theta^{i}}(s_t,a_t)\\nablalogp_{\\theta^{i}}(a_t^n|s_t^n)]=E(st​,at​)∼πθi​​[Aθi(st​,at​)∇logpθi​(atn​∣stn​)]但是我们现在只有从θi−1\\theta^{i-1}θi−1的数据，根据importancesampling，=E(st,at)∼πθi−1[pθi(st,at)pθi−1(st,at)Aθi(st,at)∇logpθi(atn∣stn)]=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i-1}}}[\\frac{p_{\\theta^{i}}(s_t,a_t)}{p_{\\theta^{i-1}}(s_t,a_t)}A^{\\theta^{i}}(s_t,a_t)\\nablalogp_{\\theta^{i}}(a_t^n|s_t^n)]=E(st​,at​)∼πθi−1​​[pθi−1​(st​,at​)pθi​(st​,at​)​Aθi(st​,at​)∇logpθi​(atn​∣stn​)]因为数据是从θi−1\\theta^{i-1}θi−1采样的，所以=E(st,at)∼πθi−1[pθi(st,at)pθi−1(st,at)Aθi−1(st,at)∇logpθi(atn∣stn)]=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i-1}}}[\\frac{p_{\\theta^{i}}(s_t,a_t)}{p_{\\theta^{i-1}}(s_t,a_t)}A^{\\theta^{i-1}}(s_t,a_t)\\nablalogp_{\\theta^{i}}(a_t^n|s_t^n)]=E(st​,at​)∼πθi−1​​[pθi−1​(st​,at​)pθi​(st​,at​)​Aθi−1(st​,at​)∇logpθi​(atn​∣stn​)]将联合概率分布改写成条件概率分布=E(st,at)∼πθi−1[pθi(at∣st)pθi−1(at∣st)pθi(st)pθi−1(st)Aθi−1(st,at)∇logpθi(atn∣stn)]=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i-1}}}[\\frac{p_{\\theta^{i}}(a_t|s_t)}{p_{\\theta^{i-1}}(a_t|s_t)}\\frac{p_{\\theta^{i}}(s_t)}{p_{\\theta^{i-1}}(s_t)}A^{\\theta^{i-1}}(s_t,a_t)\\nablalogp_{\\theta^{i}}(a_t^n|s_t^n)]=E(st​,at​)∼πθi−1​​[pθi−1​(at​∣st​)pθi​(at​∣st​)​pθi−1​(st​)pθi​(st​)​Aθi−1(st​,at​)∇logpθi​(atn​∣stn​)]其中，因为我们的actor是神经网络，我们可以直接把sts_tst​输入，就得到ata_tat​的条件概率，因此pθi−1(at∣st)pθi(at∣st)\\frac{p_{\\theta^{i-1}}(a_t|s_t)}{p_{\\theta^{i}}(a_t|s_t)}pθi​(at​∣st​)pθi−1​(at​∣st​)​很容易求。但是另外一项pθi−1(st)pθi(st)\\frac{p_{\\theta^{i-1}}(s_t)}{p_{\\theta^{i}}(s_t)}pθi​(st​)pθi−1​(st​)​不知道该怎么求，所以我们说服自己每次玩游戏观测到特定画面的概率是一样的，所以这一项不重要，我们把它去掉，那么我们就得到=E(st,at)∼πθi−1[pθi(at∣st)pθi−1(at∣st)Aθi−1(st,at)∇logpθi(atn∣stn)]=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i-1}}}[\\frac{p_{\\theta^{i}}(a_t|s_t)}{p_{\\theta^{i-1}}(a_t|s_t)}A^{\\theta^{i-1}}(s_t,a_t)\\nablalogp_{\\theta^{i}}(a_t^n|s_t^n)]=E(st​,at​)∼πθi−1​​[pθi−1​(at​∣st​)pθi​(at​∣st​)​Aθi−1(st​,at​)∇logpθi​(atn​∣stn​)]相应的，我们的目标函数就是最大化期望累积奖励，Jθi−1(θi)=E(st,at)∼πθi−1[pθi(at∣st)pθi−1(at∣st)Aθi−1(st,at)]J^{\\theta^{i-1}}(\\theta^i)=E_{(s_t,a_t)\\sim\\pi_{\\theta^{i-1}}}[\\frac{p_{\\theta^{i}}(a_t|s_t)}{p_{\\theta^{i-1}}(a_t|s_t)}A^{\\theta^{i-1}}(s_t,a_t)]Jθi−1(θi)=E(st​,at​)∼πθi−1​​[pθi−1​(at​∣st​)pθi​(at​∣st​)​Aθi−1(st​,at​)]Jθi−1(θi)J^{\\theta^{i-1}}(\\theta^i)Jθi−1(θi)中JJJ的上标表示从θi−1\\theta^{i-1}θi−1收集的数据，更新的是θi\\theta^{i}θi的参数。所以说PPO的核心是更新参数的那个actor必须要有意识知道自己和跟环境互动的那个actor是不一样的，为了做区别，我们把目标函数写成Jθ′(θ)=E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)]J^{\\theta^{&#x27;}}(\\theta)=E_{(s_t,a_t)\\sim\\pi_{\\theta^{&#x27;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{&#x27;}}(a_t|s_t)}A^{\\theta^{&#x27;}}(s_t,a_t)]Jθ′(θ)=E(st​,at​)∼πθ′​​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)]拿θ′\\theta^{&#x27;}θ′收集的数据去更新θ\\thetaθ。前面我们讲importancesampling的时候说过，两个分布不能相差得太多，相差太多得到的结果就不可能好。那么，在PPO里就是更新参数的actor和跟环境互动的actor不能差距过大，所以PPO就在目标函数加一个KLKLKL散度正则化项，JPPOθ′(θ)=E(st,at)∼πθ′[pθ(at∣st)pθ′(at∣st)Aθ′(st,at)]−βKL(θ,θ′)J^{\\theta^{&#x27;}}_{PPO}(\\theta)=E_{(s_t,a_t)\\sim\\pi_{\\theta^{&#x27;}}}[\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{&#x27;}}(a_t|s_t)}A^{\\theta^{&#x27;}}(s_t,a_t)]-\\betaKL(\\theta,\\theta^{&#x27;})JPPOθ′​(θ)=E(st​,at​)∼πθ′​​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)]−βKL(θ,θ′)需要注意的是，KLKLKL散度不是对actor的参数做限制，而是对actor的行为（即actor的输出）做限制。现在，我们对PPO做一个总结：首先，初始化策略参数为θ0\\theta^0θ0，然后开始迭代训练，在每一次迭代中，用θk\\theta^kθk去跟环境交互，收集数据{st,at}\\{s_t,a_t\\}{st​,at​}，计算Aθk(st,at)A^{\\theta^{k}}(s_t,a_t)Aθk(st​,at​)，然后根据目标函数优化模型。这里需要注意的是PPO的参数可以更新很多次，充分提高了数据利用效率。最后，还对β\\betaβ做自适应调整，控制更新参数的actor和跟环境互动的actor不会差距过大。PPO还有另外一个比较容易实现的版本：JPPO2θk(θ)≈∑(st,at)(pθ(at∣st)pθk(at∣st)Aθk(st,at),clip(pθ(at∣st)pθk(at∣st),1−ε,1+ε)Aθk(st,at))J^{\\theta^{k}}_{PPO2}(\\theta)\\approx\\sum_{(s_t,a_t)}(\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{k}}(a_t|s_t)}A^{\\theta^{k}}(s_t,a_t),clip(\\frac{p_\\theta(a_t|s_t)}{p_{\\theta^{k}}(a_t|s_t)},1-\\varepsilon,1+\\varepsilon)A^{\\theta^{k}}(s_t,a_t))JPPO2θk​(θ)≈(st​,at​)∑​(pθk​(at​∣st​)pθ​(at​∣st​)​Aθk(st​,at​),clip(pθk​(at​∣st​)pθ​(at​∣st​)​,1−ε,1+ε)Aθk(st​,at​))主要是不用处理KLKLKL散度，比较容易实现，具体就不展开讲了，毕竟现在是入门。Policy-basedv.s.Value-based前面讲的都是怎么训练一个actor，怎么更新actor采用的策略，所以它们是policy-based的方法。现在，我们讲的是另外一个东西叫critic，意为评估者，给定actorθ\\thetaθ，critic评估actor观测到状态sss（并采用动作aaa）究竟好不好，或者说评估actor观测到状态sss之后有可能得到多少的累积奖励。评判好不好，就是评判其价值，所以critic也叫valuefunction（valuefunction在深度强化学习里面也是神经网络）。我们设critic为Vθ(s)V^\\theta(s)Vθ(s)，表示评估actorθ\\thetaθ看到状态sss之后有可能得到的累积奖励。需要强调的是，critic是跟actor绑定的。例如上面的例子，如果是一个很优秀的actor，那么critic看到第一个画面有很多外星人可以杀，那么Vθ(s)V^\\theta(s)Vθ(s)就会很大，看到第二个画面的时候外星人已经没多少了，所以Vθ(s)V^\\theta(s)Vθ(s)就会变小。但是，如果是一个很烂的actor，不管看到什么画面，反正它都杀不了外星人，所以Vθ(s)V^\\theta(s)Vθ(s)就会很小。那么actorθ\\thetaθ看到状态sss之后有可能得到的累积奖励是多少呢？我们当然可以等到游戏结束之后去把奖励都加起来，但是critic的目的就是要学会预测累积奖励！怎么训练critic？基于Monte-Carlo（MC）的方法利用MC训练critic的方法很直接，你就拿actor去跟环境互动，然后收集数据，直接就可以拿收集到的数据去训练critic。比如，在一次互动中，我们看到状态sas_asa​，当游戏结束得到了累积奖励为Ga′G^{&#x27;}_{a}Ga′​，那么我们就得到了一笔训练数据，然后把sas_asa​输入critic输出预测Vθ(sa)V^{\\theta}(s_a)Vθ(sa​)，接着跟ground-truth比较，计算loss，梯度下降训练。同样，当我们观测到状态sbs_bsb​和对应的累积奖励Gb′G^{&#x27;}_{b}Gb′​就又得到另外一笔训练数据。基于Temporal-difference（TD）的方法在MC的方法中，我们是只有玩完整场游戏才能收集训练数据。TD希望了是观测到...st,at,rt,st+1......s_t,a_t,r_t,s_{t+1}......st​,at​,rt​,st+1​...（即在观测到状态sts_tst​，然后actor采取动作ata_tat​，得到奖励rtr_trt​，状态进去st+1s_{t+1}st+1​），这样的序列就可以训练critic。我们来分析一下，Vθ(st)=rt+γrt+1+γ2rt+2+...Vθ(st+1)=rt+1+γrt+2+...V^{\\theta}(s_t)=r_t+\\gammar_{t+1}+\\gamma^{2}r_{t+2}+...\\\\V^{\\theta}(s_{t+1})=r_{t+1}+\\gammar_{t+2}+...Vθ(st​)=rt​+γrt+1​+γ2rt+2​+...Vθ(st+1​)=rt+1​+γrt+2​+...那么，Vθ(st)=γVθ(st+1)+rtV^{\\theta}(s_t)=\\gammaV^{\\theta}(s_{t+1})+r_tVθ(st​)=γVθ(st+1​)+rt​，这就是Temporal-difference。在训练的时候，将sts_tst​和st+1s_{t+1}st+1​分布输入VθV^\\thetaVθ得到输出Vθ(st)V^{\\theta}(s_t)Vθ(st​)和Vθ(st+1)V^{\\theta}(s_{t+1})Vθ(st+1​)，最后计算Vθ(st)−γVθ(st+1)V^{\\theta}(s_t)-\\gammaV^{\\theta}(s_{t+1})Vθ(st​)−γVθ(st+1​)使它跟rtr_trt​越相近越好，这就是TD的训练过程。MCv.s.TDMC和TD都可以用来训练我们critic，但是有时候用这两种方法计算出来的结果却不一样，因为我们观测到的数据指数部分数据而已，结果的不同是抽样导致的。一个简单的例子，假设我们玩了这个简单的小游戏8回，收集的数据如下，我们计算一下，Vθ(sb)=3/4V^{\\theta}(s_b)=3/4Vθ(sb​)=3/4这个很简单，没有问题。但是Vθ(sa)V^{\\theta}(s_a)Vθ(sa​)等于多少呢？根据MC的方法，我们只观测到一次sas_asa​且它的累积奖励为000，那么Vθ(sa)=0V^{\\theta}(s_a)=0Vθ(sa​)=0；但是根据TD的方法，Vθ(sa)=Vθ(sb)+r=3/4+0=3/4V^{\\theta}(s_a)=V^{\\theta}(s_b)+r=3/4+0=3/4Vθ(sa​)=Vθ(sb​)+r=3/4+0=3/4。上面讲的critic是评估actor$\\theta看到状态看到状态看到状态s$之后有可能得到的累积奖励，所以也叫statevaluefunction。现在我们来看另外一个criticQπ(s,a)Q^{\\pi}(s,a)Qπ(s,a)，意思是actorθ\\thetaθ在看到状态sss之后强制采取动作aaa有可能得到的累积奖励，也叫state-actionvaluefunction或者$Q$function。QQQfunction把状态sss和动作aaa作为输入，输出对应的预测累积奖励。但是，对于动作action是离散的问题，我们通常可以将QQQfunction写成下面的形式，输入的是状态sss，输出的不同action对应的累积奖励。Critic的应用我们学习出critic之后，critic该怎么用呢？第一个例子就是Q-learning。Q-learningQ-learning就是让actorπ\\piπ跟环境互动，然后通过TD或MC的方法来学出QfunctionQπ(s,a)Q^{\\pi}(s,a)Qπ(s,a),其实actorπ\\piπ就是根据QfunctionQπ(s,a)Q^{\\pi}(s,a)Qπ(s,a)的输出来选择动作，选择能最大化累积奖励的动作aaaπ′(s)=argmaxaQπ(s,a)\\pi^{&#x27;}(s)=arg\\\\underset{a}{max}\\Q^{\\pi}(s,a)π′(s)=argamax​Qπ(s,a)Q-learning在实践中会用到的技巧1、Targetnetwork第一个会用到的技巧的targetnetwork，它设置两个如下图所示的Qfunction，先固定住其中一个作为targetnetwork，然后采用TD的方式进行训练。把sts_tst​和ata_tat​输入Qfunction中，预测对应的累积奖励Qπ(st,at)Q^\\pi(s_t,a_t)Qπ(st​,at​),把st+1s_{t+1}st+1​和π(st+1)\\pi(s_{t+1})π(st+1​)输入到targetnetwork中预测对应的Qπ(st+1,π(st+1))Q^\\pi(s_{t+1},\\pi(s_{t+1}))Qπ(st+1​,π(st+1​))，根据TD的定义，这两者之间应该相差一个rtr_trt​，然后通过梯度下降最小化Qπ(st,at)Q^\\pi(s_t,a_t)Qπ(st​,at​)和rt+Qπ(st+1,π(st+1))r_t+Q^\\pi(s_{t+1},\\pi(s_{t+1}))rt​+Qπ(st+1​,π(st+1​))进行训练。2、Exploration前面说Qlearning是选择能够最大化期望累积奖励的动作，其实这样不太好，我们讲过强化学习是需要进行探索的，所以说Qlearning也要引入exploration。Qlearning的exploration通常可以有EpsilonGreedy，将动作选取的规则定义为a={argmaxaQπ(s,a),withprobability1−εrandom,otherwisea=\\left\\{\\begin{aligned}&amp;arg\\\\underset{a}{max}\\Q^{\\pi}(s,a),\\\\\\with\\probability\\1-\\varepsilon\\\\&amp;random,\\\\\\otherwise\\end{aligned}\\right.a=⎩⎨⎧​​argamax​Qπ(s,a),withprobability1−εrandom,otherwise​ε\\varepsilonε随着训练递减，因为在训练刚开始时需要更多地探索未知，当策略成熟之后，就应该更多地依赖策略做决策。exploration也可以采用BoltzmannExploration，计算每个动作的条件概率，p(a∣s)=exp(Q(s,a))∑aexp((s,a))p(a|s)=\\frac{exp(Q(s,a))}{\\sum_{a}exp((s,a))}p(a∣s)=∑a​exp((s,a))exp(Q(s,a))​可以选取概率比较高的动作。3.ReplayBufferReplaybuffer就是将训练数据存到缓冲区，这里的每笔数据都是(st,at,rt,st+1)(s_t,a_t,r_t,s_{t+1})(st​,at​,rt​,st+1​)的形式，而且可以是来自不同的策略π\\piπ的数据，训练的时候就从缓冲区中选取一个批次的数据来更新Qfunction。Q-learning整个算法的典型流程总结如下：Q-Learning还有很多高级的技巧，比如DoubleDQN等，但是这里就不展开讲了。Version3.5除了Q-learning之外，critic还有什么用呢？我们在policygradientversion3中说过累积奖励需要减去一个baselinebbb才能更好地判断一个行为究竟是好还是不好。但是，这个baselinebbb该怎么选取呢？其中一个好的办法就是减去Vθ(sN)V^{\\theta}(s_N)Vθ(sN​)。为什么说累积奖励Gt′G^{&#x27;}_{t}Gt′​减去Vθ(st)V^{\\theta}(s_t)Vθ(st​)是更合理的选择呢？在观测到状态sts_tst​之后，如果采取不同的动作，我们可以得到不同的累积奖励，其实Vθ(st)V^{\\theta}(s_t)Vθ(st​)是不同累积奖励的均值，是期望的累积奖励。所以，如果观测到状态sts_tst​之后采取动作ata_tat​得到的累积奖励Gt′G^{&#x27;}_{t}Gt′​，我们计算At=Gt′−Vθ(st)A_t=G^{&#x27;}_{t}-V^{\\theta}(s_t)At​=Gt′​−Vθ(st​)，如果At&gt;0A_t&gt;0At​&gt;0，说明这时候选择动作ata_tat​要比平均值要好，反之，如果At&lt;0A_t&lt;0At​&lt;0，说明选择动作ata_tat​要比平均值要差。Version4其实，version3.5还有一个问题。那就是Gt′G^{&#x27;}_{t}Gt′​只是一次抽样得到的累积奖励，去要拿去减一个均值，所以不太合理，解决方案就是我们的version4。我们观测到的数据是{...st,at,rt,st+1}\\{...s_t,a_t,r_t,s_{t+1}\\}{...st​,at​,rt​,st+1​}，当我们训练好一个critic之后，我们可以求得Vθ(st+1)V^{\\theta}(s_{t+1})Vθ(st+1​)，它是观测到状态st+1s_{t+1}st+1​之后的期望累积奖励。所以说观测到状态sts_tst​之后采取动作ata_tat​得到的累积奖励应该是rt+Vθ(st+1)r_t+V^{\\theta}(s_{t+1})rt​+Vθ(st+1​)。那么，AtA_tAt​的计算方式变成了At=rt+Vθ(st+1)−Vθ(st)A_t=r_t+V^{\\theta}(s_{t+1})-V^{\\theta}(s_t)At​=rt​+Vθ(st+1​)−Vθ(st​)。Version4就是著名的AdvantageActor-Critic。强化学习中其他相关主题RewardShapingRewardshaping就是针对奖励稀疏的强化学习问题，做法就是人为去设置一些奖励来加速学习。InverseReinforcementLearning更进一步，加入奖励已经不是稀疏的问题了，而是根本就没有reward，也不知道该怎么人为去设置reward，这种情况该怎么办呢？这就是inversereinforcementlearning要解决的问题。没有reward，就不知道我们的actor跟环境的互动怎么样，那就没办法通过试错去学习了。那该怎么办呢？虽然没有reward，但是我们有人类的智慧，我们知道该怎么去解决一个问题，那么actor去学习我们怎么解决问题行不行呢？就比如说我们人类会开车，那么我们演示一遍，那actor能不能去模仿学会开车呢？答案是可以的。但是等等，让actor模仿人类的行为，人类的行为不就是ground-truth吗？那这是一个监督学习吗？是的，这是监督学习，也就是所谓的BehaviorCloning。但是BehaviorCloning存在两个问题，第一个就是人类不会演示失败的案例，比如翻车，那么actor也就不会处理失败的情况；第二个就是人类演示可能存在一些个人行为习惯的东西，这些东西跟任务完全无关，但是actor也会照搬过去，这是不合理的。Inversereinforcementlearning应运而生。在这个问题中，我们有人类专家的经验、但是不知道怎么去定义奖励函数，还要避免actor完成照搬人类的行为，那么既然我们不知道奖励函数，能够能不能根据人类专家的经验去用机器学习学出一个奖励函数，然后用这个奖励函数去指导actor学习呢？这就是Inversereinforcementlearning。我们怎么去根据人类专家的经验反推除一个奖励函数呢？其中一个最基本的原则是：老师（人类专家）的行为总是最好的。根据这个原则设计的inversereinforcementlearning过程如下：首先，初始化一个actor，然后让这个actor去跟环境互动，在每一个迭代中，先根据互动收集数据，然后定义一个奖励函数，要求这个奖励函数给老师的分数要高于actor的分数，然后这个actor基于这个奖励函数尝试最大化累积奖励，最终我们就得到了一个奖励函数以及训练好的actor。整个inversereinforcementlearning的框架如下图所示，强化学习处理连续动作问题需要注意的是前面我们讲的例子中都是默认actor可采取的动作空间都是离散的，如在玩spaceinvader这个游戏的时候，可采取的动作只有左、右和开火，如下图所示。然后，我们可以通过a=argmaxaQ(s,a)a=arg\\\\underset{a}{max}\\Q(s,a)a=argamax​Q(s,a)最大化QValueQ\\ValueQValue来选取最优的动作。但是，在实际应用中actor的动作空间却是连续的，例如在自动驾驶中方向盘转向的角度、油门的开度或者机器人关节的方向等。为什么动作空间连续对深度强化学习是一个问题呢？因为在神经网络架构设计中，往往是有多少个离散动作就有多少个输出神经元，但是如果动作空间是连续的，那我们该怎么设计输出神经元呢？今天，我们要考虑的就是怎么处理这样的问题。Solution1：第一个直观方案就是连续问题离散化。这个方案是可行的，因为假设在自动驾驶里面方向盘应该向右打15度不会因为我打了16度而翻车。连续问题离散化方案通过在连续动作空间中抽样一组动作{a1,a2,...,aN}\\{a_1,a_2,...,a_N\\}{a1​,a2​,...,aN​}，例如下图是正态分布，通过抽样从而讲连续问题转换成了离散问题。缺点：抽样的数量NNN需要足够大才会比较准确，但是数量太大会导致计算复杂度上升。Solution2：第二个方案是把它当作回归（regression）问题来解。回归问题就是可以解决连续问题啊，只不过这样的话就需要每个动作设计一个网络，然后用梯度下降来解。Solution3：对于连续动作空间我们将每个动作的概率分布定义为一个高斯概率分布![](file://D:\\华为云盘\\笔记\\ReinforcementLearning\\assets\\2022-10-12-11-30-37-image.png?msec=1675856221583)假设我们有一个机器人，有K条机械腿，控制每条腿的弯曲角度来控制机器人前进或后退或拐弯。（在强化学习的目标就是跑到某一区域）所以此时我们有K个动作。对于这K个动作，存在k个均值和方差，来描述每个动作的概率分布（不同位置的机械腿功能不同自然不等价）。那么如何得到这k组均值和方差呢？依旧交给神经网络来完成。","link":"https://chenjie04.github.io/post/qiang-hua-xue-xi-ru-men-bi-ji/"},{"title":"Google Research, 2022 & Beyond: Language, Vision and Generative Models","content":"——PostedbyJeffDean，谷歌2022年在AI领域的研究进展总结，窥一斑而知全豹原博客地址：https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html主要是看看Google在2022年做了哪些方面的研究，了解一下行业前沿！一、语言模型语言模型规模越大越牛逼！这应该是2022年体现出来的一个大趋势，而且语言模型达到一定规模之后似乎可以触发超能力，下图是Google的PaLM模型，有540B的参数，可以看到PaLM在绝大多数任务上都取得了突破。语言模型还可以用来写代码，例如Google的工作：ML-EnhancedCodeCompletionImprovesDeveloperProductivity。但是，引起更大轰动的是微软的Copilot，可以说相当震撼人心。多步推理学习。人工智能的一个重要关键突破口就是要能够实现多步推理。Google的一项研究正是关注这个：LanguageModelsPerformReasoningviaChainofThought，在教AI问答的时候，不是直接告诉它答案，而是告诉它结题思路，如下图所示，二、计算机视觉计算机视觉的一种重大关注点就是transformer的全面应用。计算机视觉应用transformer时需要同时关注局部和全局注意力，如：MaxViT:Multi-AxisVisionTransformerTransformer还可用用来做目标检测，把目标检测当成翻译任务，如Pix2Seq:ALanguageModelingFrameworkforObjectDetectionGoogle另外一个关注的视觉任务是如何从二维图像理解世界的三维结构，做了不少工作，如：ViewSynthesiswithTransformers三、多模态模型继续推进Pathway，Google认定的下一代人工智能架构，统一的大模型，不仅能同时处理不同模态的数据，还可以做不同的任务，重要的是这个大模型稀疏激活，即在做特定任务时只激活相关的神经元。多模态模型就涉及到该怎么融合数据的问题，Google的一项工作是：Multi-modalBottleneckTransformers多模态融合可以是图像和文字融合，如融合词嵌入做图像分类任务：Locked-imageTuningGoogle设计了PaLI模型，PaLI:ScalingLanguage-ImageLearning,可以完成很多任务。多模态数据除了可以是图像、文字、语音这些相对人类而言的多模态，还是可以是其他传感器的数据，如激光雷达等，融合激光雷达和图像做目标检测，4D-NetforLearningMulti-ModalAlignmentfor3DandImageInputsinTime四、生成模型生成模型最值得关注的就是扩散模型（Diffusionmodel）了。生成模型从2014年的GAN开始，到2022，算是取得了长足的进步了，生成的图像越来越好，扩散模型就是给图像逐渐添加噪声，直到形成完全无序的噪声，这称为扩散过程；然后学习逆扩散过程，这样我们就得到了一个生成模型，能够从噪声生成图像，扩散模型常用在Text-Image任务，根据文字生成图像，DreamBooth:FineTuningText-to-ImageDiffusionModelsforSubject-DrivenGeneration生成图像，ImagenVideo:HighDefinitionVideoGenerationfromDiffusionModels生成语音，AudioLM:aLanguageModelingApproachtoAudioGenerationhttps://google-research.github.io/seanet/audiolm/examples/五、总结最后就是对AI可靠性的关注了，特别是生成模型的发展，谁知道会不会是潘多拉的魔盒了，Google说自己特别关注AI的可靠性，制定了一些AI准则。AI的发展确实振奋人心，NLP领域在2022年的另一个重大事件是ChatGPT的发布，不过它是OpenAI的成果，不是Google的。","link":"https://chenjie04.github.io/post/google-research-2022-and-beyond-language-vision-and-generative-models/"},{"title":"空间注意力机制","content":"注意力机制是由于人类无法同时处理接收到的海量视觉信息而选择性地重点关注或忽略部分信息的信息处理方式。空间注意力是最直接的注意力机制，空间注意力最自然的实现应该就是计算出一个空间注意力权重图然后和特征图相乘，即图片来自博客：热力图与原始图像融合原始空间注意力此种实现的空间注意力机制代表是：CBAM论文地址：Cbam:Convolutionalblockattentionmodule代码地址：CBAM模块包含一个通道注意力模块和一个空间注意力模块，如下图所示，空间注意力模块首先对输入特征图沿着通道方向进行MaxPool和AvgPool操作，得到两个特征图，把两个特征图串联在一起后进行7×77\\times77×7卷积，得到单通道的特征图，经过softmaxsoftmaxsoftmax激活后即为空间注意力权重图，如下图所示，写成公式，可以简单记为：Ms(F)=σ(f7×7([AvgPool(F);MaxPool(F)]))\\textbf{M}_{\\textbf{s}}(\\textbf{F})=\\sigma(f^{7\\times7}([AvgPool(\\textbf{F});MaxPool(\\textbf{F})]))Ms​(F)=σ(f7×7([AvgPool(F);MaxPool(F)]))此种实现方式简单增强重要区域特征并抑制不相干区域特征，然而随着自注意力的爆发，空间注意力也逐渐转变为自注意力模式！自注意力模式的空间注意力个人认为上述的空间注意力侧重与信息的增强或抑制，起到信息过滤的作用，而自注意力则更主要的是特征提取。自注意力爆发于NLP领域，下面举一个NLP的例子，如上图所示，上层的token是下层的所有的token的加权和，不同的token对应的权重不一样，权重沿着整个序列不均匀分布，如上层making主要关注下层的making、more、difficult等。视觉任务的特征图同样可以沿着空间维度展成序列的形式，因此可以基于自注意力构建空间注意力。自注意力最著名的计算方式是《Attentionisallyouneed》，自注意力写成以下形式：Attention(Q,K,V)=softmax(QK⊤dk)VAttention(Q,K,V)=softmax(\\frac{QK^{\\top}}{\\sqrt{d_{k}}})VAttention(Q,K,V)=softmax(dk​​QK⊤​)V其中Q,K,VQ,K,VQ,K,V为输入XXX的线性映射，dkd_{k}dk​为token的维度。自注意力模式的空间注意力机制代表是DualAttentionNetwork。论文地址：DualAttentionNetworkforSceneSegmentation代码地址：https://github.com/junfu1115/DANet/给定输入A∈RC×H×W\\textbf{A}\\in\\mathbb{R}^{C\\timesH\\timesW}A∈RC×H×W，首先将它输入一个卷积层生成两个新的特征图B,C∈RC×H×W\\textbf{B},\\textbf{C}\\in\\mathbb{R}^{C\\timesH\\timesW}B,C∈RC×H×W，接着将B,C\\textbf{B},\\textbf{C}B,C展开成RC×N,N=H×W\\mathbb{R}^{C\\timesN},N=H\\timesWRC×N,N=H×W,然后计算空间注意力图S∈N×N\\textbf{S}\\in\\mathbb{N\\timesN}S∈N×N：S=softmax(B⊤C)\\textbf{S}=softmax(\\textbf{B}^{\\top}\\textbf{C})S=softmax(B⊤C)Sji=exp(Bj⋅Cj)∑i=1Nexp(Bi⋅Cj)\\textbf{S}_{ji}=\\frac{exp(\\textbf{B}_{j}\\cdot\\textbf{C}_{j})}{\\sum_{i=1}^{N}exp(\\textbf{B}_{i}\\cdot\\textbf{C}_{j})}Sji​=∑i=1N​exp(Bi​⋅Cj​)exp(Bj​⋅Cj​)​与此同时AAA输入另外一个卷积层生成新的特征图D∈RC×H×W\\textbf{D}\\in\\mathbb{R}^{C\\timesH\\timesW}D∈RC×H×W并reshape成RC×N\\mathbb{R}^{C\\timesN}RC×N。将D\\textbf{D}D与S\\textbf{S}S(转置)相乘即得空间注意力的最后输出。作者还乘上一个可学习的缩放因子α\\alphaα并加上残差连接，即Ej=α∑i=1N(SjiDi)+Aj\\textbf{E}_{j}=\\alpha\\sum_{i=1}^{N}(\\textbf{S}_{ji}\\textbf{D}_{i})+\\textbf{A}_{j}Ej​=αi=1∑N​(Sji​Di​)+Aj​有人认为实际上起作用的是这个可学习的缩放因子α\\alphaα，有意思！基于自注意力模式的空间注意力更像是一种信息聚合的过程，可视为特征提取，所以说更加强大，最新的研究基本上都是这种模式！但是，我还想研究研究用于信息过滤的空间注意力，将特征提取的任务留给主干网络，各司其职嘛！","link":"https://chenjie04.github.io/post/kong-jian-zhu-yi-li-ji-zhi/"},{"title":"【论文阅读笔记】Efficient Non-Local Contrastive Attention for Image Super-Resolution","content":"论文地址：EfficientNon-LocalContrastiveAttentionforImageSuper-Resolution代码地址：https://github.com/Zj-BinXia/ENLCANon-LocalContrastiveAttention，高效的非局部对比注意力[1]！发表于AAAI2022，属于空间注意力，用于单图像超分辨率任务。Non-LocalAttention，译为非局部注意力，也就是全局注意力，通过利用自然图像中的内在特征相关性可以有效提升超分辨率的性能。但是Non-LocalAttention由于自身的局限性，给图像中的噪声信息也分配了较大的权重，如下图所示，其中，(a)是输入的原图，(b)是图(a)中红点与所有位置的Non-LocalAttention权重，可以看到Non-LocalAttention给不相干的特征也分配了很大的权重；图(c)是另外一种注意力机制Non-LocalSparseAttention（NLSANLSANLSA）[2]分配的注意力权重，NLSANLSANLSA采用局部敏感哈希（LocalitySensitiveHashing(LSH)）将感受野限制在红点周围，可以看到NLSANLSANLSA计算的权重更加准确，但是由于受限的感受野，它忽略了很多重要的特征；图(d)是本文提出的Non-LocalContrastiveAttention计算的权重，可以看到它不仅计算的权重更加精准同时也具有全局的感受野，简直牛逼！Non-LocalAttentionNon-LocalAttention可以通过从整个图像中聚合相关特征来学习表征，一般而言，定义如下：Yi=∑j=1Nexp(Qi⊤Kj)∑j^=1Nexp(Qi⊤Kj^)Vj,\\textbf{Y}_{i}=\\sum_{j=1}^{N}\\frac{exp(\\textbf{Q}_{i}^{\\top}\\textbf{K}_{j})}{\\sum_{\\hat{j}=1}^{N}exp(\\textbf{Q}_{i}^{\\top}\\textbf{K}_{\\hat{j}})}\\textbf{V}_{j},Yi​=j=1∑N​∑j^​=1N​exp(Qi⊤​Kj^​​)exp(Qi⊤​Kj​)​Vj​,Q=θ(X),K=δ(X),V=φ(X),\\textbf{Q}=\\theta(\\textbf{X}),\\textbf{K}=\\delta(\\textbf{X}),\\textbf{V}=\\varphi(\\textbf{X}),Q=θ(X),K=δ(X),V=φ(X),实际上这就是self-attention的一般实现形式，X∈Rc×N\\textbf{X}\\in\\mathbb{R}^{c\\timesN}X∈Rc×N为输入的特征图，其中N=h×wN=h\\timeswN=h×w；θ(.)\\theta(.)θ(.)，δ(.)\\delta(.)δ(.)，φ(.)\\varphi(.)φ(.)为对应的线性映射函数；Qi,Kj∈Rc,Vj∈Rcout\\textbf{Q}_{i},\\textbf{K}_{j}\\in\\mathbb{R}^{c},\\textbf{V}_{j}\\in\\mathbb{R}^{c_{out}}Qi​,Kj​∈Rc,Vj​∈Rcout​则是对应特征图Q,K,V\\textbf{Q},\\textbf{K},\\textbf{V}Q,K,V在位置i,ji,ji,j上的特征。Non-LocalAttention第一个众所周知的缺点是其对输入尺寸二次方的复杂度，所以本文考虑的第一个点就是提升计算效率，作者考虑的是采用用核方法（KernelMethod）去逼近Non-localAttention里面的指数函数。KernelMethod首先，机器学习分类任务中存在这么一个观察：低维空间中线性不可分的点映射到高维空间之后可能就线性可分了，所以按照这个思路，我们就应该把低维空间的点映射到高维空间再进行分类，因为在高维空间中是线性可分的。对于线性模型f(x)=w⊤xf(\\textbf{x})=\\textbf{w}^{\\top}\\textbf{x}f(x)=w⊤x，其最优解是所有数据样本的线性组合w∗=∑i=iNαixi\\textbf{w}^{*}=\\sum^{N}_{i=i}\\alpha_{i}\\textbf{x}_{i}w∗=∑i=iN​αi​xi​，因此模型的最优解表示为：f∗(x)=∑i=1Nαixi⊤xf^{*}(\\textbf{x})=\\sum_{i=1}^{N}\\alpha_{i}\\textbf{x}_{i}^{\\top}\\textbf{x}f∗(x)=i=1∑N​αi​xi⊤​x其实，对核方法理解的难点就在于这里：为什么线性模型的最优解是所有数据样本的线性组合？如果要严格去理论证明这件事可能需要用到表示定理（representertheorem）或者拉格朗日乘子法等知识，这些我已经忘得差不多了，以后再补补吧！在这里，我们举例子简单理解一下，例如在二维空间中，如下图所示有正负两类样本，其中分类边界就是中间的那条线，在线的这边属于一类，另一边属于另一类。显然，这条线是可以有所有数据样本线性组合得到的，分类的关键是怎么对这些数据样本进行线性组合，当然也不是所有的数据样本都对这条分类边界有用。在这里我们不讨论这些，只需要理解分类边界可以由所有数据样本线性组合得到，这样我们就将求权重w∗w^{*}w∗变成了求线性系数α\\alphaα，可以采用梯度下降等方法求解。但是，对于开头提到的低维空间中线性不可分的情况，我们就需要将其映射到高维空间中，即引入映射ϕ:X→H\\phi:\\mathcal{X}\\to\\mathcal{H}ϕ:X→H将样本空间H\\mathcal{H}H变换到高维的特征空间F\\mathcal{F}F，所以模型最优解实际应该写成：f∗(x)=∑i=1Nαiϕ(xi)⊤ϕ(x)f^{*}(\\textbf{x})=\\sum_{i=1}^{N}\\alpha_{i}\\phi(\\textbf{x}_{i})^{\\top}\\phi(\\textbf{x})f∗(x)=i=1∑N​αi​ϕ(xi​)⊤ϕ(x)但是，特征变换映射ϕ(x)\\phi(\\textbf{x})ϕ(x)是怎么样的啊？高维空间中ϕ(xi)⊤ϕ(x)\\phi(\\textbf{x}_{i})^{\\top}\\phi(\\textbf{x})ϕ(xi​)⊤ϕ(x)又该怎么求啊？对于这种需要将低维空间映射到高维空间并求高维空间中内积ϕ(x)⊤ϕ(x′)\\phi(\\textbf{x})^{\\top}\\phi(\\textbf{x}^{&#x27;})ϕ(x)⊤ϕ(x′)的问题，我们就可以采用核方法（kernelmethod）。我们先来看一个简单的例子：还是一个二维映射到三维的例子，我们假设映射函数为：ϕ(x)=ϕ((x1x2))=(x122x1x2x22)\\phi(\\textbf{x})=\\phi(\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix})=\\begin{pmatrix}x_{1}^{2}\\\\\\sqrt{2}x_{1}x_{2}\\\\x_{2}^{2}\\end{pmatrix}ϕ(x)=ϕ((x1​x2​​))=⎝⎛​x12​2​x1​x2​x22​​⎠⎞​那么，假如我们现在要求向量a,b\\textbf{a},\\textbf{b}a,b映射到三维空间后的内积，则ϕ(a)⊤⋅ϕ(b)=(a122a1a2a22)⋅(b122b1b2b22)=a12b12+2a1b1a2b2+a22b22=(a1b1+a2b2)2=((a1a2)⊤⋅(b1b2))2=(a⊤⋅b)2\\phi(\\textbf{a})^{\\top}\\cdot\\phi(\\textbf{b})=\\begin{pmatrix}a_{1}^{2}\\\\\\sqrt{2}a_{1}a_{2}\\\\a_{2}^{2}\\end{pmatrix}\\cdot\\begin{pmatrix}b_{1}^{2}\\\\\\sqrt{2}b_{1}b_{2}\\\\b_{2}^{2}\\end{pmatrix}=a_{1}^{2}b_{1}^{2}+2a_{1}b_{1}a_{2}b_{2}+a_{2}^{2}b_{2}^{2}\\\\=(a_{1}b_{1}+a_{2}b_{2})^{2}=(\\begin{pmatrix}a_{1}\\\\a_{2}\\end{pmatrix}^{\\top}\\cdot\\begin{pmatrix}b_{1}\\\\b_{2}\\end{pmatrix})^{2}=(\\textbf{a}^{\\top}\\cdot\\textbf{b})^{2}ϕ(a)⊤⋅ϕ(b)=⎝⎛​a12​2​a1​a2​a22​​⎠⎞​⋅⎝⎛​b12​2​b1​b2​b22​​⎠⎞​=a12​b12​+2a1​b1​a2​b2​+a22​b22​=(a1​b1​+a2​b2​)2=((a1​a2​​)⊤⋅(b1​b2​​))2=(a⊤⋅b)2也就是说两个向量映射到三维空间后的内积实际上等于其在二维空间中的内积的平方，我们把k(a，b)=ϕ(a)⊤⋅ϕ(b)k(\\textbf{a}，\\textbf{b})=\\phi(\\textbf{a})^{\\top}\\cdot\\phi(\\textbf{b})k(a，b)=ϕ(a)⊤⋅ϕ(b)称为核函数，在这里其等于(a⊤⋅b)2(\\textbf{a}^{\\top}\\cdot\\textbf{b})^{2}(a⊤⋅b)2，也就是说加入我们知道了核函数，那我们压根就不需要先做特征映射再计算高维空间中的内积，而是直接再低维空间中直接就可以求出高维空间中的内积了。核函数定义：设X\\mathcal{X}X是输入空间（欧式空间Rn的子集或离散集合\\mathbf{R}^{n}的子集或离散集合Rn的子集或离散集合），又设H\\mathcal{H}H为特征空间（希尔伯特空间），如果存在一个从X\\mathcal{X}X到H\\mathcal{H}H的映射：ϕ:X→H\\phi:\\mathcal{X}\\to\\mathcal{H}ϕ:X→H使得对所有x,z∈X\\textbf{x},\\textbf{z}\\in\\mathcal{X}x,z∈X，函数k(x,z)k(\\textbf{x},\\textbf{z})k(x,z)满足条件k(x,z)=ϕ(x)⋅ϕ(z)k(\\textbf{x},\\textbf{z})=\\phi(\\textbf{x})\\cdot\\phi(\\textbf{z})k(x,z)=ϕ(x)⋅ϕ(z)则称k(x,z)k(\\textbf{x},\\textbf{z})k(x,z)为核函数，ϕ(x)\\phi(\\textbf{x})ϕ(x)为映射函数，式中ϕ(x)⋅ϕ(z)\\phi(\\textbf{x})\\cdot\\phi(\\textbf{z})ϕ(x)⋅ϕ(z)为ϕ(x)\\phi(\\textbf{x})ϕ(x)和ϕ(z)\\phi(\\textbf{z})ϕ(z)的内积。最后一个问题：核函数怎么来的呢？随便写的吗？有人说核函数的确定并不困难，满足Mercer定理的函数都可以作为核函数，但是我不会。我只知道一些常用的核函数。常见核函数如下：（1）线性核函数：k(x,z)=x⊤zk(\\textbf{x},\\textbf{z})=\\textbf{x}^{\\top}\\textbf{z}k(x,z)=x⊤z（2）多项式核函数：k(x,z)=(ζ+γx⊤z)q,ζ≥0,γ&gt;0k(\\textbf{x},\\textbf{z})=(\\zeta+\\gamma\\textbf{x}^{\\top}\\textbf{z})^{q},\\zeta\\ge0,\\gamma&gt;0k(x,z)=(ζ+γx⊤z)q,ζ≥0,γ&gt;0（3）高斯核函数k(x,z)=exp(−γ∣∣x−z∣∣2)k(\\textbf{x},\\textbf{z})=exp(-\\gamma||\\textbf{x}-\\textbf{z}||^{2})k(x,z)=exp(−γ∣∣x−z∣∣2)（4）拉普拉斯核函数k(x,z)=exp(−∣∣x−z∣∣σ)k(\\textbf{x},\\textbf{z})=exp(-\\frac{||\\textbf{x}-\\textbf{z}||}{\\sigma})k(x,z)=exp(−σ∣∣x−z∣∣​)（5）……对于一个核函数，总能找到一个对应的映射ϕ(⋅)\\phi(\\cdot)ϕ(⋅)，即任何核函数都隐式地定义了一个称为再生核希尔伯特空间(reproducingkernelHilbertspace)的特征空间，通常希望样本在特征空间内是线性可分的，选择核函数相当于选择了特征空间。核方法是一种手段，一种trick，所以也叫kerneltrick。Kerneltrick适合用于解决此类需要将低维输入空间映射到高维特征空间且目标函数包含映射后高维特征向量内积的问题，kerneltrick可以直接用核函数直接替代内积，甚至不用知道映射函数ϕ(x)\\phi(\\textbf{x})ϕ(x)。Kerneltrick可以说是经典机器学习方法中一种非常漂亮的理论了，这里只能给出一个粗浅的理解，深入理解请参考网上的教程[3]。用到kerneltrick的比较经典的机器学习方法就是SVM了，详情请参考大佬的文章[4]。最后提一句深度学习与kernel-based方法的区别，如下图所示，深度学习实际上真的就是把低维空间的点映射到高维空间再进行分类，在EfficientNon-LocalContrastiveAttention这篇文章中作者也是这样用的。EfficientNon-LocalAttentionEfficientNon-localAttention的架构如上图所示，为了给相干的特征更大的权重，忽略不相干特征，作者给输入的Q\\textbf{Q}Q和K\\textbf{K}K乘上一个放大因子kkk（说实在的，我并不理解为啥乘上kkk就能达到目的，可能是比如一个权重的0.10.10.1，一个是0.90.90.9，假如同时乘上6倍，变成了0.6:5.40.6:5.40.6:5.4，虽然比率不变，但是数量级不一样了，算是增强了权重的离散度，也就是归一化（Normalization）的反向运用），在实现上就是在做线性映射的时候给Q\\textbf{Q}Q和K\\textbf{K}K分别乘上一个k\\sqrt{k}k​，即：Q=kθ(X)∣∣θ(X)∣∣,K=kδ(X)∣∣θ(X)∣∣,V=φ(X),\\textbf{Q}=\\sqrt{k}\\frac{\\theta(\\textbf{X})}{||\\theta(\\textbf{X})||},\\textbf{K}=\\sqrt{k}\\frac{\\delta(\\textbf{X})}{||\\theta(\\textbf{X})||},\\textbf{V}=\\varphi(\\textbf{X}),Q=k​∣∣θ(X)∣∣θ(X)​,K=k​∣∣θ(X)∣∣δ(X)​,V=φ(X),作者考虑用核方法（KernelMethod）去逼近Non-localAttention里面的指数函数。那么设核函数为：K(Qi,Kj)=exp(Qi⊤Kj)=exp((∣∣Qi+Kj∣∣2−∣Qi∣∣2−∣∣Kj∣∣2)/2)=exp(−(∣∣Qi∣∣2+∣∣Kj∣∣2)/2)exp(∣∣Qi+Kj∣∣2/2)\\begin{aligned}&amp;\\K(\\textbf{Q}_{i},\\textbf{K}_{j})=exp(\\textbf{Q}_{i}^{\\top}\\textbf{K}_{j})\\\\&amp;=exp((||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}-|\\textbf{Q}_{i}||^{2}-||\\textbf{K}_{j}||^{2})/2)\\\\&amp;=exp(-(||\\textbf{Q}_{i}||^{2}+||\\textbf{K}_{j}||^{2})/2)exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)\\\\\\end{aligned}​K(Qi​,Kj​)=exp(Qi⊤​Kj​)=exp((∣∣Qi​+Kj​∣∣2−∣Qi​∣∣2−∣∣Kj​∣∣2)/2)=exp(−(∣∣Qi​∣∣2+∣∣Kj​∣∣2)/2)exp(∣∣Qi​+Kj​∣∣2/2)​定义高斯随机样本为f∈Rc\\textbf{f}\\in\\mathbb{R}^{c}f∈Rc且f∼N(0c,Ic)\\textbf{f}\\sim\\mathcal{N}(\\textbf{0}_{c},\\textbf{I}_{c})f∼N(0c​,Ic​)，则对任意Qi,Kj∈Rc\\textbf{Q}_{i},\\textbf{K}_{j}\\in\\mathcal{R}^{c}Qi​,Kj​∈Rc有(2π)−c/2∫exp(−∣∣f−(Qi+Kj)∣∣2/2)df=1(2\\pi)^{-c/2}\\intexp(-||\\textbf{f}-(\\textbf{Q}_{i}+\\textbf{K}_{j})||^{2}/2)d\\textbf{f}=1(2π)−c/2∫exp(−∣∣f−(Qi​+Kj​)∣∣2/2)df=1(这个公式我没看懂！)，根据该公式，核函数中的exp(∣∣Qi+Kj∣∣2/2)exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)exp(∣∣Qi​+Kj​∣∣2/2)可以近似为：exp(∣∣Qi+Kj∣∣2/2)=(2π)−c/2exp(∣∣Qi+Kj∣∣2/2)⋅∫exp(−∣∣f−(Qi+Kj)∣∣2/2)df=(2π)−c/2∫exp(−∣∣f∣∣2/2+f⊤(Qi+Kj)−∣∣Qi+Kj∣∣2/2+∣∣Qi+Kj∣∣2/2)df=(2π)−c/2∫exp(−∣∣f∣∣2/2+f⊤(Qi+Kj))df=(2π)−c/2∫exp(−∣∣f∣∣2/2)⋅exp(f⊤(Qi+Kj))df=Ef∼N(0c,Ic)[exp(f⊤(Qi+Kj))]\\begin{aligned}&amp;exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)=(2\\pi)^{-c/2}exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)\\cdot\\intexp(-||\\textbf{f}-(\\textbf{Q}_{i}+\\textbf{K}_{j})||^{2}/2)d\\textbf{f}\\\\&amp;=(2\\pi)^{-c/2}\\intexp(-||\\textbf{f}||^{2}/2+\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j})-||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2+||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)d\\textbf{f}\\\\&amp;=(2\\pi)^{-c/2}\\intexp(-||\\textbf{f}||^{2}/2+\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j}))d\\textbf{f}\\\\&amp;=(2\\pi)^{-c/2}\\intexp(-||\\textbf{f}||^{2}/2)\\cdotexp(\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j}))d\\textbf{f}\\\\&amp;=\\mathbb{E}_{f\\sim\\mathcal{N}(\\textbf{0}_{c},\\textbf{I}_{c})}[exp(\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j}))]\\end{aligned}​exp(∣∣Qi​+Kj​∣∣2/2)=(2π)−c/2exp(∣∣Qi​+Kj​∣∣2/2)⋅∫exp(−∣∣f−(Qi​+Kj​)∣∣2/2)df=(2π)−c/2∫exp(−∣∣f∣∣2/2+f⊤(Qi​+Kj​)−∣∣Qi​+Kj​∣∣2/2+∣∣Qi​+Kj​∣∣2/2)df=(2π)−c/2∫exp(−∣∣f∣∣2/2+f⊤(Qi​+Kj​))df=(2π)−c/2∫exp(−∣∣f∣∣2/2)⋅exp(f⊤(Qi​+Kj​))df=Ef∼N(0c​,Ic​)​[exp(f⊤(Qi​+Kj​))]​那么，核函数最终为：K(Qi,Kj)=exp(Qi⊤Kj)=exp(−(∣∣Qi∣∣2+∣∣Kj∣∣2)/2)exp(∣∣Qi+Kj∣∣2/2)=Ef∼N(0c,Ic)exp(f⊤(Qi+Kj)−∣∣Qi∣∣2+∣∣Kj∣∣22)=ϕ(Qi)⊤ϕ(Kj)\\begin{aligned}&amp;\\K(\\textbf{Q}_{i},\\textbf{K}_{j})=exp(\\textbf{Q}_{i}^{\\top}\\textbf{K}_{j})\\\\&amp;=exp(-(||\\textbf{Q}_{i}||^{2}+||\\textbf{K}_{j}||^{2})/2)exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2}/2)\\\\&amp;=\\mathbb{E}_{f\\sim\\mathcal{N}(\\textbf{0}_{c},\\textbf{I}_{c})}exp(\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j})-\\frac{||\\textbf{Q}_{i}||^{2}+||\\textbf{K}_{j}||^{2}}{2})\\\\&amp;=\\phi(\\textbf{Q}_{i})^{\\top}\\phi(\\textbf{K}_{j})\\end{aligned}​K(Qi​,Kj​)=exp(Qi⊤​Kj​)=exp(−(∣∣Qi​∣∣2+∣∣Kj​∣∣2)/2)exp(∣∣Qi​+Kj​∣∣2/2)=Ef∼N(0c​,Ic​)​exp(f⊤(Qi​+Kj​)−2∣∣Qi​∣∣2+∣∣Kj​∣∣2​)=ϕ(Qi​)⊤ϕ(Kj​)​在实作上，作者用了mmm个高斯随机样本，f1,⋯,fm∼i.i.dN(0c,Ic)\\textbf{f}_{1},\\cdots,\\textbf{f}_{m}\\overset{i.i.d}{\\sim}\\mathcal{N}(\\textbf{0}_{c},\\textbf{I}_{c})f1​,⋯,fm​∼i.i.dN(0c​,Ic​)并拼接成高斯随机矩阵F∈Rm×c\\textbf{F}\\in\\mathbb{R}^{m\\timesc}F∈Rm×c。根据上述核函数可知，指数函数exp(Qi⊤Kj)exp(\\textbf{Q}_{i}^{\\top}\\textbf{K}_{j})exp(Qi⊤​Kj​)可由映射后的向量内积ϕ(Qi⊤)ϕ(Kj)\\phi(\\textbf{Q}_{i}^{\\top})\\phi(\\textbf{K}_{j})ϕ(Qi⊤​)ϕ(Kj​)近似，而映射函数为：ϕ(u)=1mexp(−∣∣u∣∣2/2)exp(Fu)\\phi(\\textbf{u})=\\frac{1}{\\sqrt{m}}exp(-||\\textbf{u}||^{2}/2)exp(\\textbf{F}\\textbf{u})ϕ(u)=m​1​exp(−∣∣u∣∣2/2)exp(Fu)基于上述分解和矩阵结合律，EfficientNon-LocalAttention可以表示为：Y^=D−1(ϕ(Q)⊤(ϕ(K)V⊤)),D=diag[ϕ(Q)⊤(ϕ(K)1N)]\\hat{\\textbf{Y}}=\\textbf{D}^{-1}(\\phi(\\textbf{Q})^{\\top}(\\phi(\\textbf{K})\\textbf{V}^{\\top})),\\\\\\textbf{D}=diag[\\phi(\\textbf{Q})^{\\top}(\\phi(\\textbf{K})\\textbf{1}_{N})]Y^=D−1(ϕ(Q)⊤(ϕ(K)V⊤)),D=diag[ϕ(Q)⊤(ϕ(K)1N​)]整个EfficientNon-LocalAttention模块的实现师姐可以参考上图。经过上述改造，整个模块的复杂度为O(2mNc+2mNcout+mN)\\mathcal{O}(2mNc+2mNc_{out}+mN)O(2mNc+2mNcout​+mN)。EfficientNon-LocalAttention还存在一个问题，即随着核函数K(Qi,Kj)K(\\textbf{Q}_{i},\\textbf{K}_{j})K(Qi​,Kj​)的增长，ϕ(Qi)⊤ϕ(Kj)\\phi(\\textbf{Q}_{i})^{\\top}\\phi(\\textbf{K}_{j})ϕ(Qi​)⊤ϕ(Kj​)的方差成指数增长，Var(ϕ(Qi)⊤ϕ(Kj))=1mexp(−(∣∣Qi∣∣2+∣∣Kj∣∣2))Var(exp(f⊤(Qi+Kj)))=1mK2(Qi,Kj∣)(exp(∣∣Qi+Kj∣∣2)−1)\\begin{aligned}Var(\\phi(\\textbf{Q}_{i})^{\\top}\\phi(\\textbf{K}_{j}))&amp;=\\frac{1}{m}exp(-(||\\textbf{Q}_{i}||^{2}+||\\textbf{K}_{j}||^{2}))Var(exp(\\textbf{f}^{\\top}(\\textbf{Q}_{i}+\\textbf{K}_{j})))\\\\&amp;=\\frac{1}{m}K^{2}(\\textbf{Q}_{i},\\textbf{K}_{j}|)(exp(||\\textbf{Q}_{i}+\\textbf{K}_{j}||^{2})-1)\\end{aligned}Var(ϕ(Qi​)⊤ϕ(Kj​))​=m1​exp(−(∣∣Qi​∣∣2+∣∣Kj​∣∣2))Var(exp(f⊤(Qi​+Kj​)))=m1​K2(Qi​,Kj​∣)(exp(∣∣Qi​+Kj​∣∣2)−1)​所以放大因子kkk不能太大，不然近似误差就会变大。但是为了进一步增加相干特征和不相干特征的差距(tofurtherseparaterelevantandirrelevantfeatures)，作者采用对比学习(contrastivelearning)拉大相干特征与不相干特征的距离。ContrastiveLearningforSparseAggragation整个对比学习方案如下图所示：对于Query矩阵QQQ和Key矩阵KKK，首先度量两两之间的相似度，Ti,j=kQi⊤∣∣Qi∣∣Kj∣∣Kj∣∣,k&gt;1,Ti,j∈T,\\textbf{T}_{i,j}=k\\frac{\\textbf{Q}_{i}^{\\top}}{||\\textbf{Q}_{i}||}\\frac{\\textbf{K}_{j}}{||\\textbf{K}_{j}||},k&gt;1,\\textbf{T}_{i,j}\\in\\textbf{T},Ti,j​=k∣∣Qi​∣∣Qi⊤​​∣∣Kj​∣∣Kj​​,k&gt;1,Ti,j​∈T,然后对相似度向量Ti\\textbf{T}_{i}Ti​的元素进行排序，即从最相干到最不相干排序Ti′=sort(Ti,Descending),Ti′∈T′,Ti∈T\\textbf{T}_{i}^{&#x27;}=sort(\\textbf{T}_{i},Descending),\\textbf{T}_{i}^{&#x27;}\\in\\textbf{T}^{&#x27;},\\textbf{T}_{i}\\in\\textbf{T}Ti′​=sort(Ti​,Descending),Ti′​∈T′,Ti​∈T最后，设计对比学习的损失函数Lcl\\mathcal{L}_{cl}Lcl​为：Lcl=1N∑i=1N−log∑j=1n1Nexp(Ti,j′)/n1N∑n2N(n1+n2)Nexp(Ti,j′)/n1N+b,\\mathcal{L}_{cl}=\\frac{1}{N}\\sum_{i=1}^{N}-log\\frac{\\sum_{j=1}^{n_{1}N}exp(\\textbf{T}_{i,j}^{&#x27;})/n_{1}N}{\\sum_{n_{2}N}^{(n_{1}+n_{2})N}exp(\\textbf{T}_{i,j}^{&#x27;})/n_{1}N}+b,Lcl​=N1​i=1∑N​−log∑n2​N(n1​+n2​)N​exp(Ti,j′​)/n1​N∑j=1n1​N​exp(Ti,j′​)/n1​N​+b,n1n_{1}n1​为选取的相干特征或不相干特征占的百分比，n2n_{2}n2​为不相干特征的起始索引。通过最小化该损失函数就可以拉开相干特征与不相干特征的权重差距。实验结果EfficientNon-LocalContrastiveAttention的消融实验如下图所示，将NLA替换为ENLA提升0.16，引入放大因子kkk则再增加0.7，使用对比学习增加0.4。可视化效果如下图所示，参考文献[1]XiaB,HangY,TianY,etal.EfficientNon-LocalContrastiveAttentionforImageSuper-Resolution[J].arXivpreprintarXiv:2201.03794,2022.[2]MeiY,FanY,ZhouY.Imagesuper-resolutionwithnon-localsparseattention[C]//ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2021:3517-3526.[3]https://0809zheng.github.io/2021/07/23/kernel.html[4]https://blog.csdn.net/v_july_v/article/details/7624837","link":"https://chenjie04.github.io/post/bi-ji-efficient-non-local-contrastive-attention-for-image-super-resolution/"},{"title":"扩散模型（Diffusion Model）学习笔记","content":"一、先验知识1、条件概率的一般形式P(A,B,C)=P(C∣B,A)P(B,A)=P(C∣B,A)P(B∣A)P(A)P(A,B,C)=P(C|B,A)P(B,A)=P(C|B,A)P(B|A)P(A)P(A,B,C)=P(C∣B,A)P(B,A)=P(C∣B,A)P(B∣A)P(A)P(B,C∣A)=P(B∣A)P(C∣A,B)P(B,C|A)=P(B|A)P(C|A,B)P(B,C∣A)=P(B∣A)P(C∣A,B)证明：左边：P(B,C∣A)=P(A,B,C)/P(A)P(B,C|A)=P(A,B,C)/P(A)P(B,C∣A)=P(A,B,C)/P(A)右边：P(B∣A)=P(A,B)/P(A)P(B|A)=P(A,B)/P(A)P(B∣A)=P(A,B)/P(A);而P(C∣A,B)=P(A,B,C)/P(A,B)P(C|A,B)=P(A,B,C)/P(A,B)P(C∣A,B)=P(A,B,C)/P(A,B);两项相乘得：P(A,B,C)/P(A)P(A,B,C)/P(A)P(A,B,C)/P(A)2、基于马尔可夫假设的条件概率如果满足马尔可夫链关系A-&gt;B-&gt;C，那么有P(A,B,C)=P(C∣B,A)P(B,A)=P(C∣B)P(B∣A)P(A)P(B,C∣A)=P(B∣A)P(C∣B)P(A,B,C)=P(C|B,A)P(B,A)=P(C|B)P(B|A)P(A)\\\\P(B,C|A)=P(B|A)P(C|B)P(A,B,C)=P(C∣B,A)P(B,A)=P(C∣B)P(B∣A)P(A)P(B,C∣A)=P(B∣A)P(C∣B)因为马尔可夫的性质就是当前时刻的状态只与前一时刻有关，即C只与B相关，B只与A相关3、高斯分布的KL散度公式对于两个单一变量的高斯分布p和q而言，它们的KL散度为KL(p,q)=logσ2σ1+σ12+(μ1−μ2)22σ22−12KL(p,q)=log\\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{\\sigma^{2}_{1}+(\\mu_{1}-\\mu_{2})^{2}}{2\\sigma^{2}_{2}}-\\frac{1}{2}KL(p,q)=logσ1​σ2​​+2σ22​σ12​+(μ1​−μ2​)2​−21​4、重参数技巧(很重要)若希望从高斯分布N(μ,σ2)N(\\mu,\\sigma^{2})N(μ,σ2)中采样，可以先从标准正态分布N(0,1)N(0,1)N(0,1)采样出ϵ\\epsilonϵ，再通过σ∗ϵ+μ\\sigma\\ast\\epsilon+\\muσ∗ϵ+μ得到采样结果。这样做的好处的将随机性转移到了ϵ\\epsilonϵ这个常量上，而σ\\sigmaσ和μ\\muμ则当作仿射变换网络的一部分。二、DiffusionModel如下图所示，扩散模型定义了一个马尔可夫链：在扩散过程(x0→xT)(\\mathbf{x}_0\\rightarrow\\mathbf{x}_T)(x0​→xT​)中，慢慢地给原始图片（或者叫原始数据分布）添加高斯噪声，x0\\mathbf{x}_0x0​表示从真实数据集中采样得到的一张图片，对x0\\mathbf{x}_0x0​添加TTT次噪声，随着噪声的不断添加，图片逐渐变得模糊，当TTT足够大时，最终数据分布就变成了一个各项独立的高斯分布，即xT\\mathbf{x}_TxT​为标准正态分布。在训练过程中，每次添加的噪声是已知的，即q(xt∣xt−1)q(\\mathbf{x}_t|\\mathbf{x}_{t-1})q(xt​∣xt−1​)是已知的，根据马尔科夫链的性质，可以递归得到q(xt∣x0)q(\\mathbf{x}_t|\\mathbf{x}_{0})q(xt​∣x0​)，即扩散过程是已知的。我们需要学习的是逆扩散过程(xT→x0)(\\mathbf{x}_T\\rightarrow\\mathbf{x}_0)(xT​→x0​)，从噪声中构建出原来的图片。假如我们能够在给定xt\\mathbf{x}_txt​的条件下计算出xt−1\\mathbf{x}_{t-1}xt−1​，即知道q(xt−1∣xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)q(xt−1​∣xt​)，那我们就能够从任意一张噪声图片中经过一次次的采样得到一张图片而达成图片生成的目的。不幸的是我们很难估计q(xt−1∣xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)q(xt−1​∣xt​)，因为它需要用整个数据集来估计，所以我们希望能够用一个神经网络pθ(xt−1∣xt)p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)来近似这个逆扩散过程q(xt−1∣xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)q(xt−1​∣xt​)。下面进行扩散过程与逆扩散过程的推导。1、扩散过程给定初始数据分布x0∼q(x)\\mathbf{x}_{0}\\simq(\\mathbf{x})x0​∼q(x)，我们定义一个马尔科夫链的前向扩散过程，该过程中的每个时间步ttt我们慢慢地向分布中添加高斯噪声，该噪声的标准差是由固定值βt\\beta_{t}βt​确定，均值由βt\\beta_{t}βt​和当前时刻的数据xt\\mathbf{x}_{t}xt​决定。βt\\beta_{t}βt​定义为(0,1)(0,1)(0,1)的小数，即{βt∈(0,1)}t=1t\\{\\beta_{t}\\in(0,1)\\}_{t=1}^{t}{βt​∈(0,1)}t=1t​，那么这个扩散过程可以记为q(xt∣xt−1)=N(xt;1−βtxt−1,βtI)q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\textbf{I})q(xt​∣xt−1​)=N(xt​;1−βt​​xt−1​,βt​I)根据基于马尔可夫假设的条件概率公式，有q(x1:T∣x0)=∏t=1Tq(xt∣xt−1)q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})=\\prod_{t=1}^{T}q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})q(x1:T​∣x0​)=t=1∏T​q(xt​∣xt−1​)随着时间ttt逐渐增大，初始数据分布逐渐失去其具有判别性的特征；当T→∞T\\to\\inftyT→∞，最终数据分布xT\\mathbf{x}_{T}xT​就等价于一个各向独立的高斯分布。在扩散过程中，一般而言是一步一步迭代来算出每一步的噪声样本，即根据x0\\mathbf{x}_{0}x0​计算x1\\mathbf{x}_{1}x1​，根据x1\\mathbf{x}_{1}x1​计算x2\\mathbf{x}_{2}x2​，……，一直迭代直到xT\\mathbf{x}_{T}xT​。但是，实际上利用重参数技巧，我们完全可以基于x0\\mathbf{x}_{0}x0​和βt\\beta_{t}βt​计算出任意时刻的xt\\mathbf{x}_{t}xt​，而不需要做迭代。要求噪声样本xt\\mathbf{x}_{t}xt​，就得从分布N(1−βtxt−1,βtI)\\mathcal{N}(\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\textbf{I})N(1−βt​​xt−1​,βt​I)中抽样，根据重参数技巧，我们可以先从标准正态分布N(0,I)\\mathcal{N}(\\textbf{0},\\textbf{I})N(0,I)中采样出ϵt−1\\epsilon_{t-1}ϵt−1​，然后根据重参数技巧得xt=1−βtxt−1+βtϵt−1\\mathbf{x}_{t}=\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1}+\\beta_{t}\\epsilon_{t-1}xt​=1−βt​​xt−1​+βt​ϵt−1​，为简化公式，记αt=1−βt\\alpha_{t}=1-\\beta_{t}αt​=1−βt​，则有xt=αtxt−1+1−αtϵt−1;whereϵt−1∼N(0,I)\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}}\\mathbf{x}_{t-1}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1};\\\\\\\\\\\\where\\\\epsilon_{t-1}\\sim\\mathcal{N}(\\textbf{0},\\textbf{I})xt​=αt​​xt−1​+1−αt​​ϵt−1​;whereϵt−1​∼N(0,I)同理，xt−1=αt−1xt−2+1−αt−1ϵt−2;whereϵt−2∼N(0,I)\\mathbf{x}_{t-1}=\\sqrt{\\alpha_{t-1}}\\mathbf{x}_{t-2}+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{t-2};\\\\\\\\\\\\where\\\\epsilon_{t-2}\\sim\\mathcal{N}(\\textbf{0},\\textbf{I})xt−1​=αt−1​​xt−2​+1−αt−1​​ϵt−2​;whereϵt−2​∼N(0,I)把xt−1\\mathbf{x}_{t-1}xt−1​代入上式，则有xt=αtαt−1xt−2+αt(1−αt−1)ϵt−2+1−αtϵt−1;\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}\\alpha_{t-1}}\\mathbf{x}_{t-2}+\\sqrt{\\alpha_{t}(1-\\alpha_{t-1})}\\epsilon_{t-2}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1};xt​=αt​αt−1​​xt−2​+αt​(1−αt−1​)​ϵt−2​+1−αt​​ϵt−1​;注意，两个正态分布x∼N(μ1,σ12)\\mathbf{x}\\sim\\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})x∼N(μ1​,σ12​)和Y∼N(μ2,σ22)Y\\sim\\mathcal{N}(\\mu_{2},\\sigma_{2}^{2})Y∼N(μ2​,σ22​)叠加后的分布ax+bYa\\mathbf{x}+bYax+bY的均值为μ=aμ1+bμ2\\mu=a\\mu_{1}+b\\mu_{2}μ=aμ1​+bμ2​，方差为σ2=a2σ12+b2σ22\\sigma^{2}=a^{2}\\sigma_{1}^{2}+b^{2}\\sigma_{2}^{2}σ2=a2σ12​+b2σ22​。由于ϵt−1\\epsilon_{t-1}ϵt−1​和ϵt−2\\epsilon_{t-2}ϵt−2​是标准正态分布，即N(0,I)\\mathcal{N}(\\textbf{0},\\textbf{I})N(0,I)，因此αt(1−αt−1)ϵt−2+1−αtϵt−1∼N(0,(1−αtαt−1)I)\\sqrt{\\alpha_{t}(1-\\alpha_{t-1})}\\epsilon_{t-2}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1}\\sim\\mathcal{N}(\\textbf{0},(1-\\alpha_{t}\\alpha_{t-1})\\textbf{I})αt​(1−αt−1​)​ϵt−2​+1−αt​​ϵt−1​∼N(0,(1−αt​αt−1​)I)，根据重参数技巧，我们重新在标准正态分布中采样一个新的噪声ϵˉt−2\\bar{\\epsilon}_{t-2}ϵˉt−2​，则有αt(1−αt−1)ϵt−2+1−αtϵt−1=(1−αtαt−1)ϵˉt−2\\sqrt{\\alpha_{t}(1-\\alpha_{t-1})}\\epsilon_{t-2}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t-1}=\\sqrt{(1-\\alpha_{t}\\alpha_{t-1})}\\bar{\\epsilon}_{t-2}αt​(1−αt−1​)​ϵt−2​+1−αt​​ϵt−1​=(1−αt​αt−1​)​ϵˉt−2​也即是xt=αtαt−1xt−2+(1−αtαt−1)ϵˉt−2;\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}\\alpha_{t-1}}\\mathbf{x}_{t-2}+\\sqrt{(1-\\alpha_{t}\\alpha_{t-1})}\\bar{\\epsilon}_{t-2};xt​=αt​αt−1​​xt−2​+(1−αt​αt−1​)​ϵˉt−2​;继续迭代下去，不难看出xt=αtαt−1αt−2xt−3+(1−αtαt−1αt−2)ϵˉt−3;…\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}\\alpha_{t-1}\\alpha_{t-2}}\\mathbf{x}_{t-3}+\\sqrt{(1-\\alpha_{t}\\alpha_{t-1}\\alpha_{t-2})}\\bar{\\epsilon}_{t-3};\\\\\\dotsxt​=αt​αt−1​αt−2​​xt−3​+(1−αt​αt−1​αt−2​)​ϵˉt−3​;…我们记αˉt=∏i=1Tαi\\bar{\\alpha}_{t}=\\prod_{i=1}^{T}\\alpha_{i}αˉt​=∏i=1T​αi​，另外，由于ϵˉt−2,ϵˉt−3,…\\bar{\\epsilon}_{t-2},\\bar{\\epsilon}_{t-3},\\dotsϵˉt−2​,ϵˉt−3​,…均为服从标准正态分布的噪声可简单记为ϵ\\epsilonϵ，最终可得出xt=αˉtx0+(1−αˉtϵ;\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{(1-\\bar{\\alpha}_{t}}\\epsilon;\\\\xt​=αˉt​​x0​+(1−αˉt​​ϵ;即可以基于x0\\mathbf{x}_{0}x0​和βt\\beta_{t}βt​计算出任意时刻的xt\\mathbf{x}_{t}xt​，q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0},(1-\\bar{\\alpha}_{t})\\textbf{I})q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I)至此，Diffusionmodel的扩散过程结束，另外，一般来说对于超参数βt\\beta_{t}βt​的设置一开始时设置得比较小，不要一下子就完全打乱原始数据的分布，当数据分布越来越接近高斯噪声时，我们就可以把βt\\beta_{t}βt​设置得大一些，即β1&lt;β2&lt;⋯&lt;βT\\beta_{1}&lt;\\beta_{2}&lt;\\dots&lt;\\beta_{T}β1​&lt;β2​&lt;⋯&lt;βT​。2、逆扩散过程（ReverseProcess）正如前面提到逆扩散过程正是要求q(xt−1∣xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)q(xt−1​∣xt​)，但是很难，因此采用神经网络pθ(xt−1∣xt)p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)来近似。实际上，pθ(xt−1∣xt)p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)也是一个从高斯分布中采样的过程，该高斯分布的均值与噪声样本xt\\mathbf{x}_txt​以及实践ttt相关，记为μθ(xt,t)\\mu_{\\theta}(\\mathbf{x}_t,t)μθ​(xt​,t)，方差同理，记为Σθ(xt,t)\\Sigma_{\\theta}(\\mathbf{x}_t,t)Σθ​(xt​,t)，则pθ(xt−1∣xt)=N(xt−1;μθ(xt,t),Σθ(xt,t))p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)=\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_{\\theta}(\\mathbf{x}_t,t),\\Sigma_{\\theta}(\\mathbf{x}_t,t))pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t))但是，我们还是不知道该怎么求。巧妙的是，虽然我们不知道q(xt−1∣xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)q(xt−1​∣xt​)，但是q(xt−1∣xt,x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q(xt−1​∣xt​,x0​)是可知的，可以用q(xt∣x0)q(\\mathbf{x}_t|\\mathbf{x}_0)q(xt​∣x0​)，q(xt−1∣x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)q(xt−1​∣x0​)和q(xt∣xt−1)q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})q(xt​∣xt−1​)表示，也就是说引入x0\\mathbf{x}_0x0​这个问题就可解了，因此考虑用q(xt−1∣xt,x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q(xt−1​∣xt​,x0​)监督pθ(xt−1∣xt)p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)的学习。首先来看q(xt−1∣xt,x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q(xt−1​∣xt​,x0​)的推导，根据条件概率公式有q(xt−1∣xt,x0)=q(x0xt−1xt)q(x0xt)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)=\\frac{q(\\mathbf{x}_0\\mathbf{x}_{t-1}\\mathbf{x}_t)}{q(\\mathbf{x}_0\\mathbf{x}_t)}q(xt−1​∣xt​,x0​)=q(x0​xt​)q(x0​xt−1​xt​)​上下同乘q(x0xt−1)q(\\mathbf{x}_0\\mathbf{x}_{t-1})q(x0​xt−1​)得=q(x0xt−1xt)q(x0xt−1)⋅q(x0xt−1)q(x0xt)=\\frac{q(\\mathbf{x}_0\\mathbf{x}_{t-1}\\mathbf{x}_t)}{q(\\mathbf{x}_0\\mathbf{x}_{t-1})}\\cdot\\frac{q(\\mathbf{x}_0\\mathbf{x}_{t-1})}{q(\\mathbf{x}_0\\mathbf{x}_{t})}=q(x0​xt−1​)q(x0​xt−1​xt​)​⋅q(x0​xt​)q(x0​xt−1​)​第一项等于q(xt∣xt−1x0)q(\\mathbf{x}_t|\\mathbf{x}_{t-1}\\mathbf{x}_0)q(xt​∣xt−1​x0​)，又因为扩散过程是马尔可夫过程，所以q(xt∣xt−1x0)=q(xt∣xt−1)q(\\mathbf{x}_t|\\mathbf{x}_{t-1}\\mathbf{x}_0)=q(\\mathbf{x}_t|\\mathbf{x}_{t-1})q(xt​∣xt−1​x0​)=q(xt​∣xt−1​)；第二项上下同除q(x0)q(\\mathbf{x}_0)q(x0​)，同样根据条件概率公式有，q(x0xt−1)q(x0xt)=q(x0xt−1)/q(x0)q(x0xt)/q(x0)=q(xt−1∣x0)q(xt∣x0)\\frac{q(\\mathbf{x}_0\\mathbf{x}_{t-1})}{q(\\mathbf{x}_0\\mathbf{x}_{t})}=\\frac{q(\\mathbf{x}_0\\mathbf{x}_{t-1})/q(\\mathbf{x}_0)}{q(\\mathbf{x}_0\\mathbf{x}_{t})/q(\\mathbf{x}_0)}=\\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}{q(\\mathbf{x}_t|\\mathbf{x}_0)}q(x0​xt​)q(x0​xt−1​)​=q(x0​xt​)/q(x0​)q(x0​xt−1​)/q(x0​)​=q(xt​∣x0​)q(xt−1​∣x0​)​，因此，q(xt−1∣xt,x0)=q(xt∣xt−1)⋅q(xt−1∣x0)q(xt∣x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)=q(\\mathbf{x}_t|\\mathbf{x}_{t-1})\\cdot\\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}{q(\\mathbf{x}_t|\\mathbf{x}_0)}q(xt−1​∣xt​,x0​)=q(xt​∣xt−1​)⋅q(xt​∣x0​)q(xt−1​∣x0​)​可以看到这三项都是扩散过程，根据前面扩散过程的推导以及高斯分布的概率密度函数有，q(xt∣xt−1)=N(xt;1−βtxt−1,βtI)=1σ2π(1−αt)e−12(xt−αtxt−1)21−αt;whereαt=1−βtq(\\mathbf{x}_t|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\textbf{I})=\\frac{1}{\\sigma\\sqrt{2\\pi(1-\\alpha_t)}}e^{-\\frac{1}{2}\\frac{(\\mathbf{x}_t-\\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^2}{1-\\alpha_t}};\\\\where\\\\alpha_t=1-\\beta_{t}q(xt​∣xt−1​)=N(xt​;1−βt​​xt−1​,βt​I)=σ2π(1−αt​)​1​e−21​1−αt​(xt​−αt​​xt−1​)2​;whereαt​=1−βt​q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)=1σ2π(1−αˉt)e−12(xt−αˉtx0)21−αˉt;whereαˉt=∏i=1Tαiq(\\mathbf{x}_t|\\mathbf{x}_0)=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0},(1-\\bar{\\alpha}_{t})\\textbf{I})=\\frac{1}{\\sigma\\sqrt{2\\pi(1-\\bar{\\alpha}_{t})}}e^{-\\frac{1}{2}\\frac{(\\mathbf{x}_t-\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_{t}}};\\\\where\\\\bar{\\alpha}_{t}=\\prod_{i=1}^{T}\\alpha_{i}q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I)=σ2π(1−αˉt​)​1​e−21​1−αˉt​(xt​−αˉt​​x0​)2​;whereαˉt​=i=1∏T​αi​q(xt−1∣x0)=N(xt−1;αˉt−1x0,(1−αˉt−1)I)=1σ2π(1−αˉt−1)e−12(xt−1−αˉt−1x0)21−αˉt−1;q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)=\\mathcal{N}(\\mathbf{x}_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0},(1-\\bar{\\alpha}_{t-1})\\textbf{I})=\\frac{1}{\\sigma\\sqrt{2\\pi(1-\\bar{\\alpha}_{t-1})}}e^{-\\frac{1}{2}\\frac{(\\mathbf{x}_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_{0})^2}{1-\\bar{\\alpha}_{t-1}}};q(xt−1​∣x0​)=N(xt−1​;αˉt−1​​x0​,(1−αˉt−1​)I)=σ2π(1−αˉt−1​)​1​e−21​1−αˉt−1​(xt−1​−αˉt−1​​x0​)2​;注：若随机变数x\\mathbf{x}x服从一个均值为μ\\muμ、标准差为σ\\sigmaσ的正态分布，记为：x∼N(μ,σ2),f(x)=1σ2πe−(x−μ)22σ2\\mathbf{x}\\sim\\mathcal{N}(\\mu,\\sigma^{2}),f(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}x∼N(μ,σ2),f(x)=σ2π​1​e−2σ2(x−μ)2​因此，q(xt−1∣xt,x0)=q(xt∣xt−1)⋅q(xt−1∣x0)q(xt∣x0)∝exp(−12((xt−αtxt−1)2βt+(xt−1−αˉt−1x0)21−αˉt−1+(xt−αˉtx0)21−αˉt))=exp(−12(xt2−2αtxtxt−1+αtxt−12βt+xt−12−2αˉt−1x0xt−1+αˉt−1x021−αˉt−1−(xt−αˉtx0)21−αˉt))=exp(−12((αtβt+11−αˉt−1)xt−12−2(αtβtxt+αˉt−11−αˉt−1x0)xt−1+C(xt,x0)))\\begin{aligned}q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)&amp;=q(\\mathbf{x}_t|\\mathbf{x}_{t-1})\\cdot\\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}{q(\\mathbf{x}_t|\\mathbf{x}_0)}\\\\&amp;\\proptoexp(-\\frac{1}{2}(\\frac{(\\mathbf{x}_t-\\sqrt{\\alpha_t}\\mathbf{x}_{t-1})^2}{\\beta_t}+\\frac{(\\mathbf{x}_{t-1}-\\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_0)^2}{1-\\bar{\\alpha}_{t-1}}+\\frac{(\\mathbf{x}_t-\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t}))\\\\&amp;=exp(-\\frac{1}{2}(\\frac{\\mathbf{x}_t^2-2\\sqrt{\\alpha_t}\\mathbf{x}_t{\\color{red}{\\mathbf{x}_{t-1}}}+\\alpha_t{\\color{green}{\\mathbf{x}_{t-1}^2}}}{\\beta_t}+\\frac{{\\color{green}{\\mathbf{x}_{t-1}^2}}-2\\sqrt{\\bar{\\alpha}_{t-1}}\\mathbf{x}_0{\\color{red}{\\mathbf{x}_{t-1}}}+\\bar{\\alpha}_{t-1}\\mathbf{x}_0^2}{1-\\bar{\\alpha}_{t-1}}-\\frac{(\\mathbf{x}_t-\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t}))\\\\&amp;=exp(-\\frac{1}{2}({\\color{green}{(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}})\\mathbf{x}_{t-1}^{2}}}-{\\color{red}{2(\\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0)\\mathbf{x}_{t-1}}}+C(\\mathbf{x}_t,\\mathbf{x}_0)))\\end{aligned}q(xt−1​∣xt​,x0​)​=q(xt​∣xt−1​)⋅q(xt​∣x0​)q(xt−1​∣x0​)​∝exp(−21​(βt​(xt​−αt​​xt−1​)2​+1−αˉt−1​(xt−1​−αˉt−1​​x0​)2​+1−αˉt​(xt​−αˉt​​x0​)2​))=exp(−21​(βt​xt2​−2αt​​xt​xt−1​+αt​xt−12​​+1−αˉt−1​xt−12​−2αˉt−1​​x0​xt−1​+αˉt−1​x02​​−1−αˉt​(xt​−αˉt​​x0​)2​))=exp(−21​((βt​αt​​+1−αˉt−1​1​)xt−12​−2(βt​αt​​​xt​+1−αˉt−1​αˉt−1​​​x0​)xt−1​+C(xt​,x0​)))​其中，C(xt,x0)C(\\mathbf{x}_t,\\mathbf{x}_0)C(xt​,x0​)是不包含xt−1\\mathbf{x}_{t-1}xt−1​的函数，因此细节可以忽略。然后将式子中的每一项除以(αtβt+11−αˉt−1)(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}})(βt​αt​​+1−αˉt−1​1​)构造成正态分布密度函数形式，可得=exp(−12(xt−1−(αtβtxt+αˉt−11−αˉt−1x0)/(αtβt+11−αˉt−1))21/(αtβt+11−αˉt−1))=exp(-\\frac{1}{2}\\frac{(\\mathbf{x}_{t-1}-(\\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0)/(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}}))^2}{1/(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}})})=exp(−21​1/(βt​αt​​+1−αˉt−1​1​)(xt−1​−(βt​αt​​​xt​+1−αˉt−1​αˉt−1​​​x0​)/(βt​αt​​+1−αˉt−1​1​))2​)所以，q(xt−1∣xt,x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q(xt−1​∣xt​,x0​)服从正态分布，方差与均值如下所示(注意：αt=1−βt,αˉt=∏i=1Tαi\\alpha_t=1-\\beta_t,\\bar{\\alpha}_t=\\prod_{i=1}^{T}\\alpha_{i}αt​=1−βt​,αˉt​=∏i=1T​αi​)，q(xt−1∣xt,x0)=N(xt−1;μ~(xt,x0),β~tI)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)=\\mathcal{N}(\\mathbf{x}_{t-1};\\tilde{\\mu}(\\mathbf{x}_t,\\mathbf{x}_0),\\tilde{\\beta}_t\\textbf{I})q(xt−1​∣xt​,x0​)=N(xt−1​;μ~​(xt​,x0​),β~​t​I)β~t=1/(αtβt+11−αˉt−1)=1/(αt−αˉt+βtβt(1−αˉt−1))=1−αˉt−11−αˉt⋅βt\\tilde{\\beta}_t=1/(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}})=1/(\\frac{\\alpha_t-\\bar{\\alpha}_t+\\beta_t}{\\beta_t(1-\\bar{\\alpha}_{t-1})})={\\color{magenta}{\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\cdot\\beta_t}}β~​t​=1/(βt​αt​​+1−αˉt−1​1​)=1/(βt​(1−αˉt−1​)αt​−αˉt​+βt​​)=1−αˉt​1−αˉt−1​​⋅βt​μ~(xt,x0)=(αtβtxt+αˉt−11−αˉt−1x0)/(αtβt+11−αˉt−1)=(αtβtxt+αˉt−11−αˉt−1x0)⋅1−αˉt−11−αˉt⋅βt=αt(1−αˉt−1)1−αˉtxt+αˉt−1βt1−αˉtx0\\begin{aligned}\\tilde{\\mu}(\\mathbf{x}_t,\\mathbf{x}_0)&amp;=(\\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0)/(\\frac{\\alpha_t}{\\beta_t}+\\frac{1}{1-\\bar{\\alpha}_{t-1}})\\\\&amp;=(\\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0)\\cdot{\\color{magenta}{\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\cdot\\beta_t}}\\\\&amp;=\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}\\mathbf{x}_0\\end{aligned}μ~​(xt​,x0​)​=(βt​αt​​​xt​+1−αˉt−1​αˉt−1​​​x0​)/(βt​αt​​+1−αˉt−1​1​)=(βt​αt​​​xt​+1−αˉt−1​αˉt−1​​​x0​)⋅1−αˉt​1−αˉt−1​​⋅βt​=1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​αˉt−1​​βt​​x0​​由扩散过程可知xt=αˉtx0+(1−αˉtϵt\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{(1-\\bar{\\alpha}_{t}}\\epsilon_txt​=αˉt​​x0​+(1−αˉt​​ϵt​，即x0=1αˉt(xt−(1−αˉtϵt)\\mathbf{x}_{0}=\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}(\\mathbf{x}_{t}-\\sqrt{(1-\\bar{\\alpha}_{t}}\\epsilon_t)x0​=αˉt​​1​(xt​−(1−αˉt​​ϵt​)，代入上式可得（注意：αˉt=αˉt−1⋅αt\\bar{\\alpha}_t=\\bar{\\alpha}_{t-1}\\cdot\\alpha_tαˉt​=αˉt−1​⋅αt​）μ~(xt,x0)=αt(1−αˉt−1)1−αˉtxt+αˉt−1βt1−αˉt1αˉt(xt−(1−αˉtϵt)=1αt(xt−1−αt1−αˉtϵt)\\begin{aligned}\\tilde{\\mu}(\\mathbf{x}_t,\\mathbf{x}_0)&amp;=\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\mathbf{x}_t+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t}\\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}}(\\mathbf{x}_{t}-\\sqrt{(1-\\bar{\\alpha}_{t}}\\epsilon_t)\\\\&amp;=\\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_t)\\end{aligned}μ~​(xt​,x0​)​=1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​αˉt−1​​βt​​αˉt​​1​(xt​−(1−αˉt​​ϵt​)=αt​​1​(xt​−1−αˉt​​1−αt​​ϵt​)​因此，q(xt−1∣xt,x0)=N(xt−1;1αt(xt−1−αt1−αˉtϵt),1−αˉt−11−αˉt⋅βtI)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)=\\mathcal{N}(\\mathbf{x}_{t-1};\\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_t),\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\cdot\\beta_t\\textbf{I})q(xt−1​∣xt​,x0​)=N(xt−1​;αt​​1​(xt​−1−αˉt​​1−αt​​ϵt​),1−αˉt​1−αˉt−1​​⋅βt​I)接下来，我们就可以利用q(xt−1∣xt,x0)q(\\mathbf{x}_{t-1}|\\mathbf{x}_t,\\mathbf{x}_0)q(xt−1​∣xt​,x0​)来监督pθ(xt−1∣xt)p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)的训练了！3、损失函数要优化模型pθ(xt−1∣xt)p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)pθ​(xt−1​∣xt​)可以考虑最大化对数似然概率，即最小化负的对数似然概率−logpθ(x0)-logp\\theta(\\mathbf{x}_0)−logpθ(x0​)（为什么是x0\\mathbf{x}_0x0​？因为扩散过程的马尔科夫链），但是由于很难对噪声空间进行积分，因此直接优化−logpθ(x0)-logp\\theta(\\mathbf{x}_0)−logpθ(x0​)是困难的，因此，向VAE一样，转而去优化它的变分下界LVLBL_{VLB}LVLB​，我们知道KL散度是两个概率分布差别的非对称性度量，即KL散度大于等于零，因此有−logpθ(x0)≤−logpθ(x0)+DKL(q(x1:T∣x0)∣∣pθ(x1:T∣x0))=−logpθ(x0)+Ex1:T∼q(x1:T∣x0)[logq(x1:T∣x0)pθ(x0:T)/pθ(x0)]=−logpθ(x0)+Eq[logq(x1:T∣x0)pθ(x0:T)+logpθ(x0)]=Eq[logq(x1:T∣x0)pθ(x0:T)]\\begin{aligned}-logp\\theta(\\mathbf{x}_0)&amp;\\le-logp\\theta(\\mathbf{x}_0)+D_{KL}(q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)||p\\theta(\\mathbf{x}_{1:T}|\\mathbf{x}_0))\\\\&amp;=-logp\\theta(\\mathbf{x}_0)+\\mathbb{E}_{\\mathbf{x}_{1:T}\\simq(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}[log\\frac{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})/p_{\\theta}(\\mathbf{x}_0)}]\\\\&amp;=-logp\\theta(\\mathbf{x}_0)+\\mathbb{E}_q[log\\frac{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}+logp_{\\theta}(\\mathbf{x}_0)]\\\\&amp;=\\mathbb{E}_q[log\\frac{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}]\\end{aligned}−logpθ(x0​)​≤−logpθ(x0​)+DKL​(q(x1:T​∣x0​)∣∣pθ(x1:T​∣x0​))=−logpθ(x0​)+Ex1:T​∼q(x1:T​∣x0​)​[logpθ​(x0:T​)/pθ​(x0​)q(x1:T​∣x0​)​]=−logpθ(x0​)+Eq​[logpθ​(x0:T​)q(x1:T​∣x0​)​+logpθ​(x0​)]=Eq​[logpθ​(x0:T​)q(x1:T​∣x0​)​]​其中，Eq[logq(x1:T∣x0)pθ(x0:T)]\\mathbb{E}_q[log\\frac{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}]Eq​[logpθ​(x0:T​)q(x1:T​∣x0​)​]就是负对数似然的变分下界。或者从另外一个角度，我们可以优化真实分布与预测分布的交叉熵（crossentropy），但是由于同样的原因，优化很困难，因此求变分下界，LCE=−Eq(x0)logpθ(x0)=−Eq(x0)log(∫pθ(x0:T)dx1:T)=−Eq(x0)log(∫q(x1:T∣x0)pθ(x0:T)q(x1:T∣x0)dx1:T)=−Eq(x0)log(Eq(x1:T∣x0)pθ(x0:T)q(x1:T∣x0))≤−Eq(x0:T)logpθ(x0:T)q(x1:T∣x0)=Eq(x0:T)logq(x1:T∣x0)pθ(x0:T)\\begin{aligned}L_{CE}&amp;=-\\mathbb{E}_{q(\\mathbf{x}_0)}logp_{\\theta}(\\mathbf{x}_0)\\\\&amp;=-\\mathbb{E}_{q(\\mathbf{x}_0)}log(\\intp_{\\theta}(\\mathbf{x}_{0:T})d_{\\mathbf{x}_{1:T}})\\\\&amp;=-\\mathbb{E}_{q(\\mathbf{x}_0)}log(\\intq(\\mathbf{x}_{1:T}|\\mathbf{x}_0)\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}d_{\\mathbf{x}_{1:T}})\\\\&amp;=-\\mathbb{E}_{q(\\mathbf{x}_0)}log(\\mathbb{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)})\\\\&amp;\\le-\\mathbb{E}_{q(\\mathbf{x}_{0:T})}log\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\\\&amp;=\\mathbb{E}_{q(\\mathbf{x}_{0:T})}log\\frac{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}{p_{\\theta}(\\mathbf{x}_{0:T})}\\end{aligned}LCE​​=−Eq(x0​)​logpθ​(x0​)=−Eq(x0​)​log(∫pθ​(x0:T​)dx1:T​​)=−Eq(x0​)​log(∫q(x1:T​∣x0​)q(x1:T​∣x0​)pθ​(x0:T​)​dx1:T​​)=−Eq(x0​)​log(Eq(x1:T​∣x0​)​q(x1:T​∣x0​)pθ​(x0:T​)​)≤−Eq(x0:T​)​logq(x1:T​∣x0​)pθ​(x0:T​)​=Eq(x0:T​)​logpθ​(x0:T​)q(x1:T​∣x0​)​​为了方程中的每个项可解析计算，可以进一步将目标重写为几个KL散度和熵项的组合，LVLB=Eq(x0T)[log⁡q(x1:T∣x0)pθ(x0:T)]=Eq[log⁡∏t=1Tq(xt∣xt−1)pθ(xT)∏t=1Tpθ(xt−1∣xt)]=Eq[−log⁡pθ(xT)+∑t=1Tlog⁡q(xt∣xt−1)pθ(xt−1∣xt)]=Eq[−log⁡pθ(xT)+∑t=2Tlog⁡q(xt∣xt−1)pθ(xt−1∣xt)+log⁡q(x1∣x0)pθ(x0∣x1)]=Eq[−log⁡pθ(xT)+∑t=2Tlog⁡(q(xt−1∣xt,x0)pθ(xt−1∣xt)⋅q(xt∣x0)q(xt−1∣x0))+log⁡q(x1∣x0)pθ(x0∣x1)]=Eq[−log⁡pθ(xT)+∑t=2Tlog⁡q(xt−1∣xt,x0)pθ(xt−1∣xt)+∑t=2Tlog⁡q(xt∣x0)q(xt−1∣x0)+log⁡q(x1∣x0)pθ(x0∣x1)]=Eq[−log⁡pθ(xT)+∑t=2Tlog⁡q(xt−1∣xt,x0)pθ(xt−1∣xt)+log⁡q(xT∣x0)q(x1∣x0)+log⁡q(x1∣x0)pθ(x0∣x1)]=Eq[log⁡q(xT∣x0)pθ(xT)+∑t=2Tlog⁡q(xt−1∣xt,x0)pθ(xt−1∣xt)−log⁡pθ(x0∣x1)]=Eq[DKL(q(xT∣x0)∥pθ(xT))⎵LT+∑t=2TDKL(q(xt−1∣xt,x0)∥pθ(xt−1∣xt))⎵Lt−1−log⁡pθ(x0∣x1)⎵L0]\\begin{aligned}&amp;L_{\\mathrm{VLB}}=\\mathbb{E}_{q(\\mathbf{x}0T)}\\left[\\log\\frac{q\\left(\\mathbf{x}_{1:T}\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_{0:T}\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[\\log\\frac{\\prod_{t=1}^Tq\\left(\\mathbf{x}_t\\mid\\mathbf{x}_{t-1}\\right)}{p_\\theta\\left(\\mathbf{x}_T\\right)\\prod_{t=1}^Tp_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[-\\logp_\\theta\\left(\\mathbf{x}_T\\right)+\\sum_{t=1}^T\\log\\frac{q\\left(\\mathbf{x}_t\\mid\\mathbf{x}_{t-1}\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[-\\logp_\\theta\\left(\\mathbf{x}_T\\right)+\\sum_{t=2}^T\\log\\frac{q\\left(\\mathbf{x}_t\\mid\\mathbf{x}_{t-1}\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}+\\log\\frac{q\\left(\\mathbf{x}_1\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[-\\logp_\\theta\\left(\\mathbf{x}_T\\right)+\\sum_{t=2}^T\\log\\left(\\frac{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t,\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}\\cdot\\frac{q\\left(\\mathbf{x}_t\\mid\\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_0\\right)}\\right)+\\log\\frac{q\\left(\\mathbf{x}_1\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[-\\logp_\\theta\\left(\\mathbf{x}_T\\right)+\\sum_{t=2}^T\\log\\frac{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t,\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}+\\sum_{t=2}^T\\log\\frac{q\\left(\\mathbf{x}_t\\mid\\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_0\\right)}+\\log\\frac{q\\left(\\mathbf{x}_1\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[-\\logp_\\theta\\left(\\mathbf{x}_T\\right)+\\sum_{t=2}^T\\log\\frac{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t,\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}+\\log\\frac{q\\left(\\mathbf{x}_T\\mid\\mathbf{x}_0\\right)}{q\\left(\\mathbf{x}_1\\mid\\mathbf{x}_0\\right)}+\\log\\frac{q\\left(\\mathbf{x}_1\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)}\\right]\\\\&amp;=\\mathbb{E}_q\\left[\\log\\frac{q\\left(\\mathbf{x}_T\\mid\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_T\\right)}+\\sum_{t=2}^T\\log\\frac{q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t,\\mathbf{x}_0\\right)}{p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)}-\\logp_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)\\right]\\\\&amp;=\\mathbb{E}_q[\\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_T\\mid\\mathbf{x}_0\\right)\\|p_\\theta\\left(\\mathbf{x}_T\\right)\\right)}_{L_T}+\\sum_{t=2}^T\\underbrace{D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t,\\mathbf{x}_0\\right)\\|p_\\theta\\left(\\mathbf{x}_{t-1}\\mid\\mathbf{x}_t\\right)\\right)}_{L_{t-1}}-\\underbrace{\\logp_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)}_{L_0}]\\\\&amp;\\end{aligned}​LVLB​=Eq(x0T)​[logpθ​(x0:T​)q(x1:T​∣x0​)​]=Eq​[logpθ​(xT​)∏t=1T​pθ​(xt−1​∣xt​)∏t=1T​q(xt​∣xt−1​)​]=Eq​[−logpθ​(xT​)+t=1∑T​logpθ​(xt−1​∣xt​)q(xt​∣xt−1​)​]=Eq​[−logpθ​(xT​)+t=2∑T​logpθ​(xt−1​∣xt​)q(xt​∣xt−1​)​+logpθ​(x0​∣x1​)q(x1​∣x0​)​]=Eq​[−logpθ​(xT​)+t=2∑T​log(pθ​(xt−1​∣xt​)q(xt−1​∣xt​,x0​)​⋅q(xt−1​∣x0​)q(xt​∣x0​)​)+logpθ​(x0​∣x1​)q(x1​∣x0​)​]=Eq​[−logpθ​(xT​)+t=2∑T​logpθ​(xt−1​∣xt​)q(xt−1​∣xt​,x0​)​+t=2∑T​logq(xt−1​∣x0​)q(xt​∣x0​)​+logpθ​(x0​∣x1​)q(x1​∣x0​)​]=Eq​[−logpθ​(xT​)+t=2∑T​logpθ​(xt−1​∣xt​)q(xt−1​∣xt​,x0​)​+logq(x1​∣x0​)q(xT​∣x0​)​+logpθ​(x0​∣x1​)q(x1​∣x0​)​]=Eq​[logpθ​(xT​)q(xT​∣x0​)​+t=2∑T​logpθ​(xt−1​∣xt​)q(xt−1​∣xt​,x0​)​−logpθ​(x0​∣x1​)]=Eq​[LT​DKL​(q(xT​∣x0​)∥pθ​(xT​))​​+t=2∑T​Lt−1​DKL​(q(xt−1​∣xt​,x0​)∥pθ​(xt−1​∣xt​))​​−L0​logpθ​(x0​∣x1​)​​]​变分下界可以简单表示如下：LVLB=LT+LT−1+⋯+L0whereLT=DKL(q(xT∣x0)∥pθ(xT))Lt=DKL(q(xt∣xt+1,x0)∥pθ(xt∣xt+1))for1≤t≤T−1L0=−log⁡pθ(x0∣x1)\\begin{aligned}L_{\\mathrm{VLB}}&amp;=L_T+L_{T-1}+\\cdots+L_0\\\\\\text{where}L_T&amp;=D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_T\\mid\\mathbf{x}_0\\right)\\|p_\\theta\\left(\\mathbf{x}_T\\right)\\right)\\\\L_t&amp;=D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_t\\mid\\mathbf{x}_{t+1},\\mathbf{x}_0\\right)\\|p_\\theta\\left(\\mathbf{x}_t\\mid\\mathbf{x}_{t+1}\\right)\\right)\\text{for}1\\leqt\\leqT-1\\\\L_0&amp;=-\\logp_\\theta\\left(\\mathbf{x}_0\\mid\\mathbf{x}_1\\right)\\end{aligned}LVLB​whereLT​Lt​L0​​=LT​+LT−1​+⋯+L0​=DKL​(q(xT​∣x0​)∥pθ​(xT​))=DKL​(q(xt​∣xt+1​,x0​)∥pθ​(xt​∣xt+1​))for1≤t≤T−1=−logpθ​(x0​∣x1​)​其中，因为qqq没有参数且xT\\mathbf{x}_TxT​是高斯噪声，所以LTL_TLT​是常数，再训练的时候可以省略；另外，DenoisingDiffusionProbabilisticModels（DDPM）使用一个单独的解码器N(x0;μθ(x1,1),Σθ(x1,1))\\mathcal{N}(\\mathbf{x}_0;\\mu_\\theta(\\mathbf{x}_1,1),\\Sigma_\\theta(\\mathbf{x}_1,1))N(x0​;μθ​(x1​,1),Σθ​(x1​,1))来建模L0L_0L0​，而不是计算KL散度。4、损失函数的重参数化总的来说，我们希望可以学习一个神经网络可以近似逆扩散过程的条件概率分布，该神经网络表示为pθ(xt−1∣xt)=N(xt−1;μθ(xt,t),Σθ(xt,t))p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)=\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_\\theta(\\mathbf{x}_t,t),\\Sigma_\\theta(\\mathbf{x}_t,t))pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t))。我们希望训练μθ\\mu_\\thetaμθ​来预测μ~t=1αt(xt−1−αt1−αˉtϵt)\\tilde{\\mu}_t=\\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_t)μ~​t​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵt​)。因为，xt\\mathbf{x}_txt​在训练的过程中是已知的，所以我们的神经网络可以直接基于xt\\mathbf{x}_txt​在时间步ttt预测高斯噪声ϵt\\epsilon_tϵt​，μθ(xt,t)=1αt(xt−1−αt1−αˉtϵθ(xt,t))Thusxt−1=N(xt−1;1αt(xt−1−αt1−αˉtϵθ(xt,t)),Σθ(xt,t))\\begin{aligned}\\boldsymbol{\\mu}_\\theta\\left(\\mathbf{x}_t,t\\right)&amp;=\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta\\left(\\mathbf{x}_t,t\\right)\\right)\\\\\\text{Thus}\\mathbf{x}_{t-1}&amp;=\\mathcal{N}\\left(\\mathbf{x}_{t-1};\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right),\\mathbf{\\Sigma}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right)\\end{aligned}μθ​(xt​,t)Thusxt−1​​=αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​,t))=N(xt−1​;αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​,t)),Σθ​(xt​,t))​损失函数LtL_tLt​最终可以重参数化来最小化均值μ~\\tilde{\\mu}μ~​的差异,Lt=Ex0,ϵ[12∥Σθ(xt,t)∥22∥μ~t(xt,x0)−μθ(xt,t)∥2]=Ex0,ϵ[12∥Σθ∥22∥1αt(xt−1−αt1−αˉtϵt)−1αt(xt−1−αt1−αˉtϵθ(xt,t))∥2]=Ex0,ϵ[(1−αt)22αt(1−αˉt)∥Σθ∥22∥ϵt−ϵθ(xt,t)∥2]=Ex0,ϵ[(1−αt)22αt(1−αˉt)∥Σθ∥22∥ϵt−ϵθ(αˉtx0+1−αˉtϵt,t)∥2]\\begin{aligned}L_t&amp;=\\mathbb{E}_{\\mathbf{x}_{0,}\\epsilon}\\left[\\frac{1}{2\\left\\|\\mathbf{\\Sigma}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right\\|_2^2}\\left\\|\\tilde{\\boldsymbol{\\mu}}_t\\left(\\mathbf{x}_t,\\mathbf{x}_0\\right)-\\boldsymbol{\\mu}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right\\|^2\\right]\\\\&amp;=\\mathbb{E}_{\\mathbf{x}_{0,}\\epsilon}\\left[\\frac{1}{2\\left\\|\\mathbf{\\Sigma}_\\theta\\right\\|_2^2}\\left\\|\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\boldsymbol{\\epsilon}_t\\right)-\\frac{1}{\\sqrt{\\alpha_t}}\\left(\\mathbf{x}_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right)\\right\\|^2\\right]\\\\&amp;=\\mathbb{E}_{\\mathbf{x}0,\\epsilon}\\left[\\frac{\\left(1-\\alpha_t\\right)^2}{2\\alpha_t\\left(1-\\bar{\\alpha}_t\\right)\\left\\|\\mathbf{\\Sigma}_\\theta\\right\\|_2^2}\\left\\|\\epsilon_t-\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right\\|^2\\right]\\\\&amp;=\\mathbb{E}_{\\mathbf{x}_{0,}\\epsilon}\\left[\\frac{\\left(1-\\alpha_t\\right)^2}{2\\alpha_t\\left(1-\\bar{\\alpha}_t\\right)\\left\\|\\mathbf{\\Sigma}_\\theta\\right\\|_2^2}\\left\\|\\epsilon_t-\\boldsymbol{\\epsilon}_\\theta\\left(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon_t,t\\right)\\right\\|^2\\right]\\end{aligned}Lt​​=Ex0,​ϵ​[2∥Σθ​(xt​,t)∥22​1​∥μ~​t​(xt​,x0​)−μθ​(xt​,t)∥2]=Ex0,​ϵ​[2∥Σθ​∥22​1​∥∥∥∥​αt​​1​(xt​−1−αˉt​​1−αt​​ϵt​)−αt​​1​(xt​−1−αˉt​​1−αt​​ϵθ​(xt​,t))∥∥∥∥​2]=Ex0,ϵ​[2αt​(1−αˉt​)∥Σθ​∥22​(1−αt​)2​∥ϵt​−ϵθ​(xt​,t)∥2]=Ex0,​ϵ​[2αt​(1−αˉt​)∥Σθ​∥22​(1−αt​)2​∥∥​ϵt​−ϵθ​(αˉt​​x0​+1−αˉt​​ϵt​,t)∥∥​2]​另外，DDPM的作者发现，省去损失函数中的加权项效果更好，Ltsimple=Et∼[1,T],x0,ϵt[∥ϵt−ϵθ(xt,t)∥2]=Et∼[1,T],x0,,ϵt[∥ϵt−ϵθ(αˉtx0+1−αˉtϵt,t)∥2]\\begin{aligned}L_t^{\\text{simple}}&amp;=\\mathbb{E}_{t\\sim[1,T],\\mathbf{x}_0,\\epsilon_t}\\left[\\left\\|\\boldsymbol{\\epsilon}_t-\\boldsymbol{\\epsilon}_\\theta\\left(\\mathbf{x}_t,t\\right)\\right\\|^2\\right]\\\\&amp;=\\mathbb{E}_{t\\sim[1,T],\\mathbf{x}_{0,},\\epsilon_t}\\left[\\left\\|\\boldsymbol{\\epsilon}_t-\\boldsymbol{\\epsilon}_\\theta\\left(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0+\\sqrt{1-\\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t,t\\right)\\right\\|^2\\right]\\end{aligned}Ltsimple​​=Et∼[1,T],x0​,ϵt​​[∥ϵt​−ϵθ​(xt​,t)∥2]=Et∼[1,T],x0,​,ϵt​​[∥∥​ϵt​−ϵθ​(αˉt​​x0​+1−αˉt​​ϵt​,t)∥∥​2]​最终的目标函数记为：Lsimple=Ltsimple+CL_{simple}=L_{t}^{simple}+CLsimple​=Ltsimple​+C其中，CCC是权重无关的常数。5、小结扩散模型DDPM的训练和采样算法如下图所示，三、ExtractingTrainingDatafromDiffusionModels论文地址：ExtractingTrainingDatafromDiffusionModels这篇论文的意思就是扩散模型在近两年的表现牛逼上天了，但是它是不是就完美了呢？没有！他们发现扩散模型可以记住训练集中的样本，并在生成过程中进行复现。扩散模型容易受到记忆攻击，从而导致抄袭训练数据集的行为！参考文献[1]https://lilianweng.github.io/posts/2021-07-11-diffusion-models/[2]https://blog.csdn.net/Little_White_9/article/details/124435560[3]https://www.bilibili.com/video/BV1b541197HX/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&amp;vd_source=7b8e971b43a022cac5ad76d689c0c177","link":"https://chenjie04.github.io/post/xia-xie-zuo-ce-shi/"}]}