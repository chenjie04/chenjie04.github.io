<!DOCTYPE html>
<html>
<head>
<link rel="shortcut icon" href="https://chenjie04.github.io/favicon.ico" type="image/x-icon" /><meta name="viewport"content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"/><meta name="apple-mobile-web-app-capable"content="yes"/><meta name="apple-mobile-web-app-status-bar-style"content="black"/><meta name="format-detection"content="telephone=no"/><meta name="renderer"content="webkit"><meta name="description"content="温故而知新"><meta charset="UTF-8"><title>Google Research, 2022 &amp; Beyond: Language, Vision and Generative Models | chenjie04&#39;s blog</title>
<link href="https://chenjie04.github.io/styles/main.css" type="text/css" rel="stylesheet" /><link href="https://at.alicdn.com/t/font_1621793_zatzzgvf30g.css" type="text/css" rel="stylesheet" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css"><script async src="https://cdn.jsdelivr.net/npm/busuanzi@2.3.0/bsz.pure.mini.min.js"></script><script src="https://chenjie04.github.io/media/js/magnify.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
<script type="text/javascript">function btn_toggle(){document.getElementById("hn").classList.contains("no-js")?document.getElementById("hn").classList.remove("no-js"):document.getElementById("hn").classList.add("no-js")}</script>

<link rel="canonical" href="https://chenjie04.github.io/post/google-research-2022-and-beyond-language-vision-and-generative-models/" />
</head>
<body>
<div class="progress"></div><style>.progress{background:linear-gradient(to right,#87ceeb var(--scroll),transparent 0);background-repeat:no-repeat;position:fixed;width:100%;height:4px;z-index:1}</style><div class="darkmode-background"></div><div class="darkmode-layer"></div>
<noscript><p class="warn" >本页面需要浏览器支持（启用）JavaScript</p></noscript><div class="header"><div class="logo_title"><div class="title animated fadeInDown"><a href="https://chenjie04.github.io"><img alt="logo" style="display:inline-block;" src="https://chenjie04.github.io/images/avatar.png"/></a><h1 title="chenjie04&#39;s blog" class="weaklink"><a  href="/">chenjie04&#39;s blog</a></h1>

<div class="navbar weaklink">
<div class="normal_nav">
<div class="bitcron_nav_container"><div class="bitcron_nav"><div class="bitcron_nav"><div style="display:flex;justify-content:center;"><nav class="mixed_site_nav_wrap site_nav_wrap"><ul class="mixed_site_nav site_nav sm sm-base">	<li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >归档</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >标签</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/post/about" class="selected active current nav__item" >关于</a></li></ul></nav>
<div style="float:right;margin-top:1em"><form id="gridea-search-form" data-update="1578893743252" action="/search/index.html"><input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="Search..."></form></div><div style="margin-left:0.5em;margin-top:1.2em"><input id="switch_default" onclick="mobileBtn()" type="checkbox" class="switch_default"><label for="switch_default" class="toggleBtn"></label></div></div>
<div class="clear clear_nav_inline_end"></div></div></div><div class="clear clear_nav_end"></div></div></div><div class="hamberger" href="javascript:void(0)" onclick="btn_toggle();"><i class="iconfont icon-category"></i></div></div></div></div>
<div id="hn" class="no-js hidden_nav animated fadeInDown"><div class="bitcron_nav_container"><div class="bitcron_nav"><nav class="mixed_site_nav_wrap site_nav_wrap"><ul class="mixed_site_nav site_nav sm sm-base">	<li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/" class="selected active current nav__item" >首页</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/archives" class="selected active current nav__item" >归档</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/tags" class="selected active current nav__item" >标签</a></li><li><a id="d2ef19af68cc211e98f8a0242ac110003" href="/post/about" class="selected active current nav__item" >关于</a></li></ul><div class="clear clear_nav_inline_end"></div></nav></div><div class="clear clear_nav_end"></div></div>
<div style="display:flex;justify-content:center;inline-block;text-align:center;margin-top:7%"><div><form id="gridea-search-form" data-update="1690208408768" action="/search/index.html"><input class="search-input" autocomplete="off" spellcheck="false" name="q"  placeholder="Search..." /></form></div><div style="margin-left:0.5em"><input id="switch_default_h" onclick="mobileBtn()" type="checkbox" class="switch_default"><label for="switch_default" class="toggleBtn"></label></div></div>
</div></div>
<script>function enableDarkmode(){document.body.classList.add("darkmode"),document.getElementById("switch_default").checked=1,document.getElementById("switch_default_h").checked=1}function removeDarkmode(){document.body.classList.remove("darkmode"),document.getElementById("switch_default").checked=0,document.getElementById("switch_default_h").checked=0}function getCookie(a){var b,c=new RegExp("(^| )"+a+"=([^;]*)(;|$)");return(b=document.cookie.match(c))?unescape(b[2]):null}cookie=getCookie("darkmode"),"enable"==cookie&&enableDarkmode(),window.matchMedia("(prefers-color-scheme: dark)").matches&&"disable"!==cookie&&(enableDarkmode(),document.cookie="darkmode=enable; path=/");var mobileBtn=function(){1==document.getElementById("switch_default").checked?(enableDarkmode(),document.cookie="darkmode=enable; path=/"):(removeDarkmode(),document.cookie="darkmode=disable; path=/")};</script>

<div class="main"><div class="main-inner"><div class="content">
<article class="post">
<h2 class="post_title sm_margin"><a>Google Research, 2022 &amp; Beyond: Language, Vision and Generative Models</a></h2>
<script>function lan(){if(document.getElementById("lan").innerText=="繁"){var s=document.getElementById("tongwenlet_cn");if(s!=null){document.body.removeChild(s)}var s=document.createElement("script");s.language="javascript";s.type="text/javascript";s.src="https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_tw.js";s.id="tongwenlet_cn";document.body.appendChild(s);document.getElementById("lan").innerHTML="简"}else{if(document.getElementById("lan").innerText=="簡"){var s=document.getElementById("tongwenlet_cn");if(s!=null){document.body.removeChild(s)}var s=document.createElement("script");s.language="javascript";s.type="text/javascript";s.src="https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_cn.js";s.id="tongwenlet_cn";document.body.appendChild(s);document.getElementById("lan").innerHTML="繁"}}};</script>
<section class="post_details"><i class="iconfont icon-calendar"></i><span style="margin-right:15px"> 2023-01-25</span><i class="iconfont icon-browse"></i><span style="margin-right:15px"> <span id="busuanzi_value_page_pv"></span> Views</span><i class="iconfont icon-category"></i><span class="weaklink" style="margin-right:15px"></span><i class="iconfont icon-caret-down"></i><span style="margin-right:15px">932字</span><i class="iconfont icon-naozhong"></i><span style="margin-right:15px">4 min read</span><a id="lan" href="javascript:void(0);"onclick="lan();"title="调整简繁体" style="margin-right:15px;">繁</a>
</section>

<div style="display:flex">
<div class="md_block" id="md_block">
<div class="round-shape-one"></div>
<p>—— Posted by Jeff Dean，谷歌2022年在AI领域的研究进展总结，窥一斑而知全豹</p>
<p>原博客地址：<a href="https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html">https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html</a></p>
<p>主要是看看Google在2022年做了哪些方面的研究，了解一下行业前沿！</p>
<h2 id="一-语言模型">一、语言模型</h2>
<ol>
<li>语言模型规模越大越牛逼！这应该是2022年体现出来的一个大趋势，而且语言模型达到一定规模之后似乎可以触发超能力，下图是Google的PaLM模型，有540B的参数，可以看到PaLM在绝大多数任务上都取得了突破。</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhitNX_wuzSqxFIGyod7aTZ38aFakHbGIfPNDaQmFaZFR_5lIuCd9xpDEeKI_7H6hKsz84SP4efhLoD1ma77tA8K9J-uSH1b5M2hKO6TdeIsyF47oT42xJwC8TF_FgVrqx7glSXn4qPz-f0NdmYA38ID3O6m7hgPTbGeuYHFTMKf7A3yhs52vX1AzjnqA/s1559/image3.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>
<p>语言模型还可以用来写代码，例如Google的工作：<a href="https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html">ML-Enhanced Code Completion Improves Developer Productivity</a>。但是，引起更大轰动的是微软的Copilot，可以说相当震撼人心。</p>
</li>
<li>
<p>多步推理学习。人工智能的一个重要关键突破口就是要能够实现多步推理。Google的一项研究正是关注这个：<a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html">Language Models Perform Reasoning via Chain of Thought</a>，在教AI问答的时候，不是直接告诉它答案，而是告诉它结题思路，如下图所示，</p>
</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhUgLsMjIc6PzC4lU94CFtyZ5GGXmO8RpzsqZu_0ucTrcD2KBLyyw5xG_6i3dfKG6RBYRjXvikpzvVWiISnQ-8spvBkmmiBK7dgW-Bgp9jdL5IGzNVQEx_TI6nqa2CDRif5V8N35ip8o1_zbcfBh1GhVZ66Q5tJvJf2SPuikUfGSCuphuhwEh-yrfargA/s16000/image13.png" alt="" loading="lazy"></figure>
<h2 id="二-计算机视觉">二、计算机视觉</h2>
<ol>
<li>计算机视觉的一种重大关注点就是transformer的全面应用。计算机视觉应用transformer时需要同时关注局部和全局注意力，如：<a href="https://ai.googleblog.com/2022/09/a-multi-axis-approach-for-vision.html">MaxViT: Multi-Axis Vision Transformer</a></li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgtJW_zDbtGJZFN0EYYWYxrTMzedWNJ7sMycBCdf6Pm0TqY5SEkris6ZOm7zoVO53KfPzkcEG_SaBwRtmFjRJjZHuF6Eu0Szq1TZH9pn960DkhSeq1p3XdyOzaLILRdTmMc0L8pBKFlXBvVCcbWiNluaLGlyukeEXhfXgojvSFQ8QKElWyiVW9YHMsgvg/s16000/image8.png" alt="" loading="lazy"></figure>
<p>Transformer还可用用来做目标检测，把目标检测当成翻译任务，如<a href="https://ai.googleblog.com/2022/04/pix2seq-new-language-interface-for.html">Pix2Seq: A Language Modeling Framework for Object Detection</a></p>
<figure data-type="image" tabindex="4"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg2ffEMkLQWUNmwdp8OlhQX10OeCgQa10buWM4whcE9lglPNI5o7DHpUvrwKaqnc_NVywLs9PmwyhHl7BlRNTtTTJPV77bo66T1dXl64xOoPPkkspEzyY0TyRXwuhosCmz1RfrnBrXekqRbVyzZtBnFAY0ZZCVWjOeTQht_1rW2w-XJoICNmIAHnSIHpw/s16000/image15.gif" alt="" loading="lazy"></figure>
<ol start="2">
<li>Google另外一个关注的视觉任务是如何从二维图像理解世界的三维结构，做了不少工作，如：<a href="https://ai.googleblog.com/2022/09/view-synthesis-with-transformers.html">View Synthesis with Transformers</a></li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKnLtwlg_ZTnmQQw0jbuda-b701YtRaTnSGBcDpYeB48ovECFvmca2liHhWakta_gbMLjQLqd1YCgJ3bV8PmpJwUeSKgK9SHjrORLoWJGPKCwxMaGQlp5R8FcGl4qd2kfXZvKwlPMoy8KyZ3VOFbARVIr95fK6g9GNu4VKuewORVOH7Gsktjqe8t72rQ/s16000/image2.gif" alt="" loading="lazy"></figure>
<h2 id="三-多模态模型">三、多模态模型</h2>
<ol>
<li>继续推进Pathway，Google认定的下一代人工智能架构，统一的大模型，不仅能同时处理不同模态的数据，还可以做不同的任务，重要的是这个大模型稀疏激活，即在做特定任务时只激活相关的神经元。</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhEmB6YH9ml3kjklEBjfgi92kUewP8JtSt-V_N7gbVKIoWUdZgWEwkjtqLbB_M1T2K7btC6R3hy6kWqal8UzMFV-xY-5PFvvGlNprHsuU8HzKmdrJ90PZcA-07Gc04tnEDmT4OYOtnLn66TF1loR479ZkK0Bwmqv7fQ0XS97ZmekwdqSJzZ3wAjlBkAsg/s16000/image5.gif" alt="" loading="lazy"></figure>
<p>多模态模型就涉及到该怎么融合数据的问题，Google的一项工作是：<a href="https://ai.googleblog.com/2022/03/multimodal-bottleneck-transformer-mbt.html">Multi-modal Bottleneck Transformers</a></p>
<figure data-type="image" tabindex="7"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhY2Lh5QtFkc3NrkZ_Yg7SqtKGQNtxTUDboes-HhNE_8K3G1T8e8xML7rCfEWp590t5G2D_v4-gDfmbbcOj82vNJoz6WfvmaEzsAUvHvMjbZGCJIy_8qbiAwevudTfiknG3D_6d7q2LZy_2qbZ_exeGRwuYtHj3m0lUib52N2VXavOA5GWDkNtFypBzHQ/s16000/image4.gif" alt="" loading="lazy"></figure>
<p>多模态融合可以是图像和文字融合，如融合词嵌入做图像分类任务：<a href="https://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html">Locked-image Tuning</a></p>
<figure data-type="image" tabindex="8"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgJsfV__cpKjHIByCb_D9qnkj9oePP-0OzCmY4M3b5HCpBEL1XOTOiQhVs34rEv8cRgc1ggFI1cDArE5eRzRQIys8Wid2Y1w3CRvLOHM8JA4F0TiM96eU82jiVrUfkPJJTTX7HQoxH6Qiarxi4ZHC-7o-4Jb0xtIc8yjvj9xPFumnIrYtuhGit9xiGu3g/s16000/image6.gif" alt="" loading="lazy"></figure>
<p>Google设计了PaLI模型，<a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html">PaLI: Scaling Language-Image Learning</a> ,可以完成很多任务。</p>
<figure data-type="image" tabindex="9"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhm0bIDaQpKkW7VPxbmNT1J4A-KjInU043jfUb2P3rUMBJKgOj2w5Sx9RDDzEfUKOmMrIugl39QjmL73ux-Kk7u-iBERIVhK3dbxrwQh2prwbZjIIvUePGzMuGSuhPDKaPQXNXkfNUcCFvL_FItDyycWSchAgFGWF8FJmoa-fTiWARqYoN05FPU7aMAng/s16000/image14.gif" alt="" loading="lazy"></figure>
<p>多模态数据除了可以是图像、文字、语音这些相对人类而言的多模态，还是可以是其他传感器的数据，如激光雷达等，融合激光雷达和图像做目标检测，<a href="https://ai.googleblog.com/2022/02/4d-net-learning-multi-modal-alignment.html">4D-Net for Learning Multi-Modal Alignment for 3D and Image Inputs in Time</a></p>
<figure data-type="image" tabindex="10"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjY7pKJ1tolaR7XaRDdEqfOwuGsBk9BvVjKahC5NX918OfKJm_DheZ_puyz1v8NPgf9S4r2JK0QXRnEh7SA6Ft_tdpivywVb_EGexEOm4Uw3163gthGCP1dD99-dd3VTUQSj4KojC0kPhz9S_AauP7QGEnww66BlG_Q-VRJIlEigfSgov0kp6XbgWGS8Q/s16000/4d-net.gif" alt="" loading="lazy"></figure>
<h2 id="四-生成模型">四、生成模型</h2>
<ol>
<li>生成模型最值得关注的就是扩散模型（Diffusion model）了。生成模型从2014年的GAN开始，到2022，算是取得了长足的进步了，生成的图像越来越好，</li>
</ol>
<figure data-type="image" tabindex="11"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQCHnADUlh0lqkrs3nhbNanfpmx4AI7FnYs_4nJC9fRAA_zlWc9DH3ci_vyhWcgJfwotwmrjmfZyNZX5AnqqXEQ6MawLSsHggMrWuIXkkVEV8Ea01DsbL45GVM4QUdJe6qhWfR9ylAVRCahItLQZHhDaYTVBzjcE_tYWNNBpWF0y96Zmx6KOttSPHB0A/w640-h182/image10.png" alt="" loading="lazy"></figure>
<p>扩散模型就是给图像逐渐添加噪声，直到形成完全无序的噪声，这称为扩散过程；然后学习逆扩散过程，这样我们就得到了一个生成模型，能够从噪声生成图像，</p>
<figure data-type="image" tabindex="12"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXwQpq0QCgRiobjgw8HDnE43QD2MtxtwmnWT-lOw9TOQuSu7hVwwBNUTEklRvhqapcKyi-ROL9NHsTyyIT2wQ1C2aSrqvavsitTitxI8B9MYBcN-kE464WsRCXTLC_qafURR36Zaq9sUs-i2MZwyKYQ2JiqFmBpLmLkekOn6UThgQcjy_5DbqF2Ga80g/s16000/image1.png" alt="" loading="lazy"></figure>
<p>扩散模型常用在Text-Image任务，根据文字生成图像，<a href="https://arxiv.org/abs/2208.12242">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></p>
<figure data-type="image" tabindex="13"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjTqLRW73sRDgcvRc1J-1D2mq3M29_VSCZ0la33x6tFCdTFkiR3Dn2zTixy1Xw_KsHev68RFd9IpfMwlOjsOao_s1hrGcJxx4pHpQVe8wnFlUw4GupznqK17dShSgFAh27rCZHgjnxCp6DY-7l_kycCShLlbWZ_Y93qZE-AnphIIwqseagb0L--zjYnQg/s16000/teaser_static.jpg" alt="" loading="lazy"></figure>
<p>生成图像，<a href="https://imagen.research.google/video/paper.pdf">Imagen Video: High Definition Video Generation from Diffusion Models</a></p>
<p><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijaIgVhtv3V4XXUUGFbgUHn3sIHnetWFLz_7Qfn6rKbj2DmyQ5zEz0FeOu7L4HZm-Lgi2ZHOCfT3SVhvkS-gzCNg6nKbNl11M03mjeA_q-xsWobIM0P3l-TRkDesM8kcJqnioYy4Xo3MSxLgMQt5rDbh1dnvFobKlqaeCa6bCUGKOfEigtJQyPRIsNog/s320/Imagen1.gif" alt="" loading="lazy"><br>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3p_64A50y_wE_IiL6mElau5bk_jYljF9Ob5yUx-U_r0EJEfpkU8RtkBJJ1T1jYwWqjeuvN72V-8CrJVEdrfeJ2BeHNOmJe_pJgfYKSIQI9PKVJM5BlSkOSeNHlPUlINdnhwyhR64UwzoQnuZNb-KUiwyJ_BcfSMofUbsdEk90p4ciTT9tdSXF3fnr9A/s320/Imagen2.gif" alt="" loading="lazy"><br>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqfznnhH8hGvhxp59z2clElg-j5-GqiDMsS1jHHhmgyetR3mx4lj9y0HKDdzwlBonPUJjbD6Cwws0ZgBUjxMY6-Y1MttuH9pT2kp090V7l5TgMe4qhavDpe6_dwJltjKyDt3-MFVpclb1_-WH_b0yJLlkniiHC2kPVVAL0aMGp06-ZHWrh5DD5RdEEjw/s320/Imagen3.gif" alt="" loading="lazy"></p>
<p>生成语音，<a href="https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html">AudioLM: a Language Modeling Approach to Audio Generation</a></p>
<p><a href="https://youtu.be/_xkZwJ0H9IU">https://google-research.github.io/seanet/audiolm/examples/</a></p>
<h2 id="五-总结">五、总结</h2>
<p>最后就是对AI可靠性的关注了，特别是生成模型的发展，谁知道会不会是潘多拉的魔盒了，Google说自己特别关注AI的可靠性，制定了一些<a href="https://ai.google/static/documents/ai-principles-2022-progress-update.pdf">AI准则</a>。</p>
<p>AI的发展确实振奋人心，NLP领域在2022年的另一个重大事件是<a href="https://openai.com/blog/chatgpt/">ChatGPT</a> 的发布，不过它是OpenAI的成果，不是Google的。</p>

<span id="footnote"></span>
<div id = "warn"></div>
</div>
<div class="toc-container"><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E4%B8%80-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">一、语言模型</a></li>
<li><a href="#%E4%BA%8C-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">二、计算机视觉</a></li>
<li><a href="#%E4%B8%89-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B">三、多模态模型</a></li>
<li><a href="#%E5%9B%9B-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">四、生成模型</a></li>
<li><a href="#%E4%BA%94-%E6%80%BB%E7%BB%93">五、总结</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="fullPage"><canvas id="canvas"></canvas></div>
</article>
<div id="eof"><span>EOF</span></div><div class="round-shape-one"></div>
<section>
<div class="doc_comments">

	  
		<div id="gitalk-container"></div>
		<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/qyxtim/Static@3.8/gitalk.min.js"></script><link href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"rel="stylesheet"><script>var gitalk=new Gitalk({clientID:"e92872ae2da2976f32ca",clientSecret:"966ed19d803936e67d1eff80ec616cb4f6c2d57d",repo:"chenjie04.github.io",owner:"chenjie04",admin:["chenjie04"],id:(location.pathname).substring(0, 49),distractionFreeMode:false});gitalk.render("gitalk-container");</script>
	  
		  
	  

</div></section>
</div></div></div><script>
"use strict";!function(){for(var n=document.getElementsByTagName("pre"),e=n.length,s=0;s<e;s++){n[s].innerHTML='<span class="line-number"></span>'+n[s].innerHTML+'<span class="cl"></span>';for(var a=n[s].innerHTML.split(/\n/).length,r=0;r<a-1;r++){n[s].getElementsByTagName("span")[0].innerHTML+="<span>"+(r+1)+"</span>"}}}();
let mainNavLinks=document.querySelectorAll(".markdownIt-TOC a");window.addEventListener("scroll",event=>{let fromTop=window.scrollY;mainNavLinks.forEach((link,index)=>{let section=document.getElementById(decodeURI(link.hash).substring(1));let nextSection=null
if(mainNavLinks[index+1]){nextSection=document.getElementById(decodeURI(mainNavLinks[index+1].hash).substring(1));}
if(section.offsetTop<=fromTop){if(nextSection){if(nextSection.offsetTop>fromTop){link.classList.add("currentToc");}else{link.classList.remove("currentToc");}}else{link.classList.add("currentToc");}}else{link.classList.remove("currentToc");}});});
var h=document.documentElement,b=document.body,st="scrollTop",sh="scrollHeight",progress=document.querySelector(".progress"),scroll;document.addEventListener("scroll",function(){scroll=(h[st]||b[st])/((h[sh]||b[sh])-h.clientHeight)*100;progress.style.setProperty("--scroll",scroll+"%")});
var wxScale=new WxScale({fullPage:document.querySelector("#fullPage"),canvas:document.querySelector("#canvas")});var imgBox=document.querySelectorAll("#md_block img");for(var i=0;i<imgBox.length;i++){imgBox[i].onclick=function(e){wxScale.start(this)}};
</script>
<a id="scrollUp" href="#top" style="position: fixed; z-index: 2147483647; display: block;"></a><div class="footer animated fadeInDown"><div class="site_footer"><div class="mysocials"><div class="my_socials"></div></div><div class="copyright"id="copyright">Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>Copyright © 2018-2020 <a href="https://chenjie04.github.io" style="margin:0;">chenjie04&#39;s blog</a>.</div>
<span style="display: inline;margin-right:15px;">👁<strong><span id="busuanzi_value_site_uv"></span></strong></span><span id="busuanzi_container_page_pv" style="display: inline;"><span>📚<strong>31</strong> posts</span></div></div>
<script>
console.log("\n %c \u26a1Theme: Bitcron-pro Author's Blog:https://blog.blinkstar.cn  Writen By Serence  \n\n", "color: #ffffff; background: rgba(49, 49, 49, 0.85); padding:5px 0;border-radius:5px;", );
</script>
<script src="https://cdn.jsdelivr.net/npm/instant.page@3.0.0/instantpage.min.js" type="module" defer></script>
<script type="text/javascript" async src="https://chenjie04.github.io/media/js/prism.js"></script>
</body>
</html>