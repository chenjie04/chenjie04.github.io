{"posts":[{"fileName":"shuang-xian-xing-cha-zhi-fa-de-zhi-guan-li-jie","abstract":"","description":"在学习可变形卷积的过程中，涉及到给采样点加上偏移量进行卷积感受野变形的操作，但是偏移量是浮点数，偏移后一般不能保证刚好落到某个像素点上，多是落到类似（10.3，25.7）这样的虚拟像素点上，这样我们就需要用到双线性插值法来计算出虚拟像素点的...","title":"双线性插值法的直观理解","tags":[],"feature":"","link":"https://chenjie04.github.io/post/shuang-xian-xing-cha-zhi-fa-de-zhi-guan-li-jie/","stats":{"text":"4 min read","time":198000,"words":709,"minutes":4},"isTop":false,"toc":"","date":"2023-07-24 21:23:44","dateFormat":"2023-07-24"},{"fileName":"juan-ji-shu-chu-da-xiao-de-ji-suan-xiang-jie","abstract":"","description":" 一直以来都没有详细计算过卷积输出大小的过程，都是需要的时候上网查，网上的教程都比较乱或者直接给出公式，不够形象，因此自己记录一下。 考虑上图的通用卷积场景：输入大小为9×99 \\times 99×9，卷积核大小为3×33 \\times 3...","title":"卷积输出大小的计算详解","tags":[],"feature":"","link":"https://chenjie04.github.io/post/juan-ji-shu-chu-da-xiao-de-ji-suan-xiang-jie/","stats":{"text":"3 min read","time":120000,"words":489,"minutes":3},"isTop":false,"toc":"","date":"2023-07-19 10:00:25","dateFormat":"2023-07-19"},{"fileName":"ros2-kai-fa-bi-ji-launch-xi-tong-liu","abstract":"","description":"在前面启动某个节点时，往往都是使用ros2 run命令，这样美启动一个节点都需要打开一个新的终端，显然不科学。那么，我们是如何管理我们的节点呢？答案就是用Launch系统。使用Launch系统可以同时启动多个节点，还可以配置节点的参数（pa...","title":"ROS2开发笔记——Launch系统（六）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-launch-xi-tong-liu/","stats":{"text":"3 min read","time":163000,"words":599,"minutes":3},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-launch%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BD%BF%E7%94%A8\">一、Launch系统的使用</a></li>\n</ul>\n","date":"2023-05-21 21:43:46","dateFormat":"2023-05-21"},{"fileName":"ros2-kai-fa-bi-ji-dong-zuo-actionyang-li","abstract":"","description":"Action是基于topic和service之上的通信方式，使用于那些需要较长时间执行的任务。Action通信示例如下， 下面，我们将通过一个计算斐波那契 (Fibonacci)数列的例子演示action通信。在斐波那契数列中第一和第二个...","title":"ROS2开发笔记——动作（action）样例（五）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-dong-zuo-actionyang-li/","stats":{"text":"5 min read","time":279000,"words":970,"minutes":5},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-action%E9%80%9A%E4%BF%A1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84\">一、Action通信数据结构</a></li>\n<li><a href=\"#%E4%BA%8C-action%E9%80%9A%E4%BF%A1%E6%A0%B7%E4%BE%8B\">二、Action通信样例</a>\n<ul>\n<li><a href=\"#1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87\">1、环境准备</a></li>\n<li><a href=\"#2-%E7%BC%96%E5%86%99actionserver\">2、编写ActionServer</a></li>\n<li><a href=\"#3-%E7%BC%96%E5%86%99actionclient\">3、编写ActionClient</a></li>\n<li><a href=\"#4-%E6%9B%B4%E6%96%B0%E4%BE%9D%E8%B5%96%E4%B8%8E%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E5%85%A5%E5%8F%A3\">4、更新依赖与程序执行入口</a></li>\n<li><a href=\"#5-%E7%BC%96%E8%AF%91%E4%B8%8E%E8%BF%90%E8%A1%8C\">5、编译与运行</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-05-21 11:12:45","dateFormat":"2023-05-21"},{"fileName":"ros2-kai-fa-bi-ji-zi-ding-yi-shu-ju-jie-gou-msgsrv","abstract":"","description":"目前，我们学习的topic和service两种通信方式用的都是ROS预定义的数据结构，但是有时候为了满足特殊需求就得自定义数据结构。 一般而言，好的开发习惯是将数据结构放到独立的包中方便管理，也有数据结构的功能代码放到同一个包中的，例如官方...","title":"ROS2开发笔记——自定义数据结构（msg、srv）（四）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-zi-ding-yi-shu-ju-jie-gou-msgsrv/","stats":{"text":"8 min read","time":443000,"words":1534,"minutes":8},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E6%96%B0%E5%BB%BA%E5%8C%85\">一、新建包</a></li>\n<li><a href=\"#%E4%BA%8C-%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84\">二、定义数据结构</a></li>\n<li><a href=\"#%E4%B8%89-%E6%9B%B4%E6%96%B0%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6\">三、更新配置文件</a>\n<ul>\n<li><a href=\"#31-cmakeliststxt\">3.1、CMakeLists.txt</a></li>\n<li><a href=\"#32-packagexml\">3.2、package.xml</a></li>\n</ul>\n</li>\n<li><a href=\"#%E5%9B%9B-%E7%BC%96%E8%AF%91%E4%B8%8E%E9%AA%8C%E8%AF%81\">四、编译与验证</a>\n<ul>\n<li><a href=\"#41-%E5%9C%A8%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E8%80%85%E9%80%9A%E4%BF%A1%E4%B8%AD%E9%AA%8C%E8%AF%81\">4.1、在发布/订阅者通信中验证</a></li>\n<li><a href=\"#42-%E5%9C%A8%E6%9C%8D%E5%8A%A1%E9%80%9A%E4%BF%A1%E4%B8%AD%E9%AA%8C%E8%AF%81\">4.2、在服务通信中验证</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-05-17 10:05:25","dateFormat":"2023-05-17"},{"fileName":"ros2-kai-fa-bi-ji-service-yang-li","abstract":"","description":" 一、创建包 服务的通信方式就是客户端发起其服务请求，服务端收到请求后响应。在这里，我们通过一个加法求解的样例演示怎么进行服务开发，客户端发起请求——“我想知道a+b=？”，服务端收到a和b这两个数后把它们相加并返回给客户端。根据问题的定义...","title":"ROS2开发笔记——service样例（三）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros2-kai-fa-bi-ji-service-yang-li/","stats":{"text":"4 min read","time":233000,"words":862,"minutes":4},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E5%88%9B%E5%BB%BA%E5%8C%85\">一、创建包</a></li>\n<li><a href=\"#%E4%BA%8C-%E7%BC%96%E5%86%99%E6%9C%8D%E5%8A%A1%E7%AB%AF%E8%8A%82%E7%82%B9\">二、编写服务端节点</a></li>\n<li><a href=\"#%E4%B8%89-%E7%BC%96%E5%86%99%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%8A%82%E7%82%B9\">三、编写客户端节点</a></li>\n<li><a href=\"#%E5%9B%9B-%E7%BC%96%E8%AF%91%E4%B8%8E%E8%BF%90%E8%A1%8C\">四、编译与运行</a></li>\n</ul>\n","date":"2023-05-17 08:49:32","dateFormat":"2023-05-17"},{"fileName":"ros-kai-fa-bi-ji","abstract":"","description":"一、新建工作空间 ROS采用工作空间的概念，在工作空间目录下一般设src目录存放源代码。ROS采用包的形式管理项目，所有工作空间src目录下一般有多个包。ROS可以采用python或C++进行开发，也可以同时采用这两种语言混合开发，因此一个...","title":"ROS2开发笔记——Topic样例（二）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros-kai-fa-bi-ji/","stats":{"text":"11 min read","time":601000,"words":2299,"minutes":11},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E6%96%B0%E5%BB%BA%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4\">一、新建工作空间</a></li>\n<li><a href=\"#%E4%BA%8C-%E6%96%B0%E5%BB%BA%E5%8C%85\">二、新建包</a>\n<ul>\n<li><a href=\"#21-packagexml\">2.1、package.xml</a></li>\n<li><a href=\"#22-setuppy\">2.2、setup.py</a></li>\n<li><a href=\"#23-setupcfg\">2.3、setup.cfg</a></li>\n<li><a href=\"#24-%E5%85%B6%E4%BB%96\">2.4、其他</a></li>\n</ul>\n</li>\n<li><a href=\"#%E4%B8%89-%E7%BC%96%E5%86%99publisher%E8%8A%82%E7%82%B9\">三、编写publisher节点</a></li>\n<li><a href=\"#%E5%9B%9B-%E7%BC%96%E5%86%99subscriber%E8%8A%82%E7%82%B9\">四、编写subscriber节点</a></li>\n<li><a href=\"#%E4%BA%94-%E7%BC%96%E8%AF%91%E4%B8%8E%E8%BF%90%E8%A1%8C\">五、编译与运行</a></li>\n</ul>\n","date":"2023-05-16 16:13:33","dateFormat":"2023-05-16"},{"fileName":"ros2-gai-nian-xue-xi-bi-ji","abstract":"","description":"一、节点（Node） 节点是ROS的最小执行单元，每个节点应该只负责单一一个目的。节点之间通过主题（topic）、服务（services）、动作（actions）或参数（parameters）等方式进行通信。 ROS系统就是由许许多多的节...","title":"ROS2概念学习笔记（一）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/ros2-gai-nian-xue-xi-bi-ji/","stats":{"text":"16 min read","time":914000,"words":3328,"minutes":16},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E8%8A%82%E7%82%B9node\">一、节点（Node）</a></li>\n<li><a href=\"#%E4%BA%8C-%E4%B8%BB%E9%A2%98topic\">二、主题（topic）</a></li>\n<li><a href=\"#%E4%B8%89-%E6%9C%8D%E5%8A%A1services\">三、服务（services）</a></li>\n<li><a href=\"#%E5%9B%9B-%E5%8A%A8%E4%BD%9Caction\">四、动作（action）</a></li>\n<li><a href=\"#%E4%BA%94-%E5%8F%82%E6%95%B0parameter\">五、参数（parameter）</a></li>\n<li><a href=\"#%E5%85%AD-%E6%9F%A5%E9%98%85%E6%97%A5%E5%BF%97\">六、查阅日志</a></li>\n<li><a href=\"#%E4%B8%83-%E8%AE%B0%E5%BD%95%E5%B9%B6%E5%9B%9E%E6%94%BE%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE\">七、记录并回放实验数据</a></li>\n</ul>\n","date":"2023-05-11 20:48:16","dateFormat":"2023-05-11"},{"fileName":"simple-baseline-for-image-restoration","abstract":"","description":"配置文件 default_scope = 'mmedit' save_dir = './work_dirs/' default_hooks = dict( timer=dict(type='IterTimerHook'), ...","title":"Simple Baseline for Image Restoration","tags":[],"feature":"","link":"https://chenjie04.github.io/post/simple-baseline-for-image-restoration/","stats":{"text":"4 min read","time":191000,"words":512,"minutes":4},"isTop":false,"toc":"","date":"2023-04-22 16:09:36","dateFormat":"2023-04-22"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-jian-ce-tou-yuan-ma-yue-du","abstract":"","description":"RTMDet模型的检测头同样基于YOLOX Head，采用分类分支和回归分支解耦的形式，目的就是缓解分类和回归两个任务之间的冲突。不同的是YOLOX 在3个尺度特征图上都共享同一个检测头以降低参数量，但是这也会削弱模型的能力。大多数模型为了...","title":"mmdetection-3.x学习笔记——RTMDet模型检测头源码阅读","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-jian-ce-tou-yuan-ma-yue-du/","stats":{"text":"52 min read","time":3119000,"words":10093,"minutes":52},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#rtmdetsepbnhead%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84\">RTMDetSepBNHead的网络结构</a></li>\n<li><a href=\"#1-rtmdetsepbnhead%E7%9A%84%E8%BE%93%E5%87%BA\">1、RTMDetSepBNHead的输出</a></li>\n<li><a href=\"#2-%E8%8E%B7%E5%BE%97points%E5%85%88%E9%AA%8C\">2、获得points先验</a>\n<ul>\n<li><a href=\"#valid_flags\">valid_flags</a></li>\n</ul>\n</li>\n<li><a href=\"#3-%E5%B0%86%E5%88%86%E7%B1%BB%E5%88%86%E6%94%AF%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%B1%95%E6%88%90%E4%BB%A5%E4%B8%BA%E5%B9%B6%E5%B0%86%E5%90%8C%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E4%B8%B2%E8%81%94%E8%B5%B7%E6%9D%A5\">3、将分类分支的预测结果展成以为并将同一张图片的预测结果串联起来</a></li>\n<li><a href=\"#4-%E5%B0%86%E5%9B%9E%E5%BD%92%E5%88%86%E6%94%AF%E9%A2%84%E6%B5%8B%E7%9A%84%E4%B8%AD%E5%BF%83%E7%82%B9%E5%88%B0%E5%9B%9B%E8%BE%B9%E7%9A%84%E8%B7%9D%E7%A6%BB%E8%A7%A3%E7%A0%81%E6%88%90bbox%E5%B9%B6%E5%B0%86%E5%90%8C%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%89%87%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E4%B8%B2%E8%81%94%E8%B5%B7%E6%9D%A5\">4、将回归分支预测的中心点到四边的距离解码成bbox并将同一张图片的预测结果串联起来</a></li>\n<li><a href=\"#5-%E5%B0%86%E5%B1%95%E5%BC%80%E5%90%8E%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C-pointsanchors%E5%85%88%E9%AA%8C-ground-truth%E7%AD%89%E4%BC%A0%E5%85%A5selfget_targets%E5%87%86%E5%A4%87%E8%BF%9B%E8%A1%8C%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E5%88%86%E9%85%8D\">5、将展开后的预测结果、points（anchors）先验、Ground Truth等传入self.get_targets()准备进行正负样本分配</a></li>\n<li><a href=\"#6-%E5%9C%A8self_get_targets_single%E5%87%BD%E6%95%B0%E5%86%85%E9%83%A8%E5%8E%BB%E6%8E%89%E8%B6%85%E5%87%BA%E5%9B%BE%E7%89%87%E8%8C%83%E5%9B%B4%E7%9A%84anchors%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%92%8Canchors%E4%B8%80%E8%B5%B7%E7%94%A8%E6%8A%BD%E8%B1%A1%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3instancedata%E5%B0%81%E8%A3%85%E7%84%B6%E5%90%8E%E9%80%81%E5%85%A5%E6%A0%B7%E6%9C%AC%E5%88%86%E9%85%8D%E5%99%A8%E8%BF%9B%E8%A1%8C%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%88%86%E9%85%8D\">6、在self._get_targets_single()函数内部，去掉超出图片范围的anchors，对模型的预测结果和anchors一起用抽象数据接口InstanceData封装，然后送入样本分配器进行正负样本的分配</a></li>\n<li><a href=\"#7-dynamicsoftlabelassigner%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E5%88%86%E9%85%8D\">7、DynamicSoftLabelAssigner正负样本分配</a></li>\n<li><a href=\"#8-%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E9%87%87%E6%A0%B7\">8、正负样本采样</a></li>\n<li><a href=\"#9-%E6%A0%B9%E6%8D%AE%E6%8A%BD%E6%A0%B7%E7%BB%93%E6%9E%9C%E6%9E%84%E5%BB%BA%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97loss%E7%9A%84target\">9、根据抽样结果构建用于计算loss的target</a></li>\n<li><a href=\"#10-%E8%AE%A1%E7%AE%97loss\">10、计算loss</a>\n<ul>\n<li><a href=\"#%E8%AE%A1%E7%AE%97%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1qualityfocalloss\">计算分类损失QualityFocalLoss</a></li>\n<li><a href=\"#%E8%AE%A1%E7%AE%97%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1giouloss\">计算回归损失GIoULoss</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2023-03-28 19:13:29","dateFormat":"2023-03-28"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-pei-zhi-wen-jian-rtmdet_l_8xb32-300e_cocopy","abstract":"","description":"RTMDet 是基于YOLOX再次开发的检测器，检测速度更快，精度更高，还可以做实例分割和旋转目标检测。 _base_ = [ # 继承默认配置 '../_base_/default_runtime.py', '../_base_/...","title":"mmdetection-3.x学习笔记——RTMDet模型配置文件（rtmdet_l_8xb32-300e_coco.py）","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-rtmdet-mo-xing-pei-zhi-wen-jian-rtmdet_l_8xb32-300e_cocopy/","stats":{"text":"7 min read","time":361000,"words":1316,"minutes":7},"isTop":false,"toc":"","date":"2023-03-28 16:01:47","dateFormat":"2023-03-28"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-ce-shi-shi-zeng-qiang-xiang-guan-pei-zhi-rtmdet_ttapy","abstract":"","description":"测试时增强（Test time augmentation，后文简称 TTA）是一种测试阶段的数据增强策略，旨在测试过程中，对同一张图片做翻转、缩放等各种数据增强，并在增强后的图像上进行测试，最后将增强后每张图片预测的结果还原到原始尺寸并做融...","title":"mmdetection-3.x学习笔记——测试时增强相关配置（rtmdet_tta.py）","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-ce-shi-shi-zeng-qiang-xiang-guan-pei-zhi-rtmdet_ttapy/","stats":{"text":"3 min read","time":134000,"words":561,"minutes":3},"isTop":false,"toc":"","date":"2023-03-28 14:57:10","dateFormat":"2023-03-28"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-shu-ju-ji-xiang-guan-pei-zhi-schedule_1xpy","abstract":"","description":"默认采用coco数据集，结构如下 ├── data │ ├── coco │ │ ├── annotations │ │ │ │--- instances_train2017.json │ │ │ │--...","title":"mmdetection-3.x学习笔记——数据集相关配置（coco_detection.py）","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-shu-ju-ji-xiang-guan-pei-zhi-schedule_1xpy/","stats":{"text":"6 min read","time":319000,"words":1289,"minutes":6},"isTop":false,"toc":"","date":"2023-03-28 11:15:31","dateFormat":"2023-03-28"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-ce-shi-he-xun-lian-xiang-guan-pei-zhi-schedule_1xpy","abstract":"","description":"MMEngine 的 Runner 使用 Loop 来控制训练，验证和测试过程。 用户可以使用这些字段设置最大训练轮次和验证间隔。 # training schedule for 1x train_cfg = dict( type=...","title":"mmdetection-3.x学习笔记——测试和训练相关配置（schedule_1x.py）","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-ce-shi-he-xun-lian-xiang-guan-pei-zhi-schedule_1xpy/","stats":{"text":"3 min read","time":166000,"words":680,"minutes":3},"isTop":false,"toc":"","date":"2023-03-28 10:42:14","dateFormat":"2023-03-28"},{"fileName":"wei-wan-cheng-deformable-convolution-network-v3-cuda-suan-zi-kai-fa","abstract":"","description":"参考教程：https://zhuanlan.zhihu.com/p/595851188 img2col算法：https://inria.hal.science/file/index/docid/112631/filename/p103811...","title":"（未完成）Deformable convolution network v3 cuda算子开发","tags":[],"feature":"","link":"https://chenjie04.github.io/post/wei-wan-cheng-deformable-convolution-network-v3-cuda-suan-zi-kai-fa/","stats":{"text":"1 min read","time":7000,"words":24,"minutes":1},"isTop":false,"toc":"","date":"2023-03-07 22:06:32","dateFormat":"2023-03-07"},{"fileName":"bi-ji-deformable-convnets-v2-more-deformable-better-results","abstract":"","description":"论文地址：https://arxiv.org/abs/1811.11168v2 代码地址：python 代码 https://github.com/4uiiurz1/pytorch-deform-conv-v2/blob/master/de...","title":"（未完成）【论文阅读笔记】Deformable ConvNets v2: More Deformable, Better Results","tags":[],"feature":"","link":"https://chenjie04.github.io/post/bi-ji-deformable-convnets-v2-more-deformable-better-results/","stats":{"text":"1 min read","time":9000,"words":29,"minutes":1},"isTop":false,"toc":"","date":"2023-02-25 20:31:11","dateFormat":"2023-02-25"},{"fileName":"mmdetection-3x-xue-xi-bi-ji-pei-zhi-cheng-xu-yun-xing-huan-jing","abstract":"","description":"mmdetection会在Runner类初始化时调用setup_env(env_cfg)函数来配置程序的运行环境，包括多线程和分布式等环境信息， def setup_env(self, env_cfg: Dict) -&gt; None: ...","title":"mmdetection-3.x学习笔记——配置程序运行环境","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji-pei-zhi-cheng-xu-yun-xing-huan-jing/","stats":{"text":"4 min read","time":228000,"words":730,"minutes":4},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-%E9%85%8D%E7%BD%AE%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%8E%AF%E5%A2%83\">1、配置多进程环境</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-02-24 16:02:08","dateFormat":"2023-02-24"},{"fileName":"hook","abstract":"","description":"Hook编程是一种编程模式，是指在程序的一个或者多个位置设置位点（挂载点），当程序运行至某个位点时，会自动调用运行时注册到位点的所有方法。在 python 中由于函数是一等公民，实现 hook 机制其实只需要传入一个函数即可，在该函数中我们...","title":"mmdetection-3.x学习笔记——MMEngin核心组件分析（一）：Hook","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/hook/","stats":{"text":"12 min read","time":687000,"words":2222,"minutes":12},"isTop":false,"toc":"","date":"2023-02-24 15:44:52","dateFormat":"2023-02-24"},{"fileName":"transformer-yue-du-bi-ji","abstract":"","description":"本文研究基于Transformer的目标检测，相关内容参考《A Survey on Vision Transformer》. 一、Transformer-based set prediction methods 1、N. Carion ...","title":"Transformer阅读笔记","tags":[],"feature":"","link":"https://chenjie04.github.io/post/transformer-yue-du-bi-ji/","stats":{"text":"20 min read","time":1141000,"words":4694,"minutes":20},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-transformer-based-set-prediction-methods\"><strong>一、Transformer-based set prediction methods</strong></a>\n<ul>\n<li><a href=\"#1-n-carion-et-alend-to-end-object-detection-with-transformers-in-eccv-2020\"><strong>1、N. Carion et al.End-to-End Object Detection with Transformers. In ECCV, 2020.</strong></a></li>\n<li><a href=\"#2-x-zhu-et-al-deformable-detr-deformable-transformers-for-end-to-end-object-detection-in-iclr-2021\"><strong>2、X. Zhu et al. Deformable detr: Deformable Transformers for End-to-End Object Detection. In ICLR, 2021.</strong></a></li>\n<li><a href=\"#3-z-sun-et-al-rethinking-transformer-based-set-prediction-for-object-detection-in-iccv-pp-36113620-2021\"><strong>3、Z. Sun et al. Rethinking Transformer-based Set Prediction for Object Detection. In ICCV, pp. 3611–3620, 2021.</strong></a></li>\n<li><a href=\"#4-m-zheng-et-al-end-to-end-object-detection-with-adaptive-clustering-transformer-in-bmvc-2021\"><strong>4、M. Zheng et al. End-to-end object detection with adaptive clustering transformer. In BMVC, 2021.</strong></a></li>\n<li><a href=\"#5-p-gao-et-al-fast-convergence-of-detr-with-spatially-modulated-co-attention-in-iccv-2021\"><strong>5、P. Gao et al. Fast convergence of detr with spatially modulated co-attention. In ICCV, 2021.</strong></a></li>\n<li><a href=\"#6-z-yao-et-al-efficient-detr-improving-end-to-end-object-detector-with-dense-priorarxiv210401318-2021\"><strong>6、Z. Yao et al. Efficient detr: Improving end-to-end object detector with dense prior.arXiv:2104.01318, 2021.</strong></a></li>\n<li><a href=\"#7-xia-zhuofan-et-al-vision-transformer-with-deformable-attention-2022-ieeecvf-conference-on-computer-vision-and-pattern-recognition-cvpr-2022-4784-4793\"><strong>7、Xia, Zhuofan et al. “Vision Transformer with Deformable Attention.” <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2022): 4784-4793.</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#%E4%BA%8C-transformer-based-backbone-method\"><strong>二、Transformer-based backbone method</strong></a>\n<ul>\n<li><a href=\"#1-j-beal-et-al-toward-transformer-based-object-detectionarxiv201209958-2020\"><strong>1、J. Beal et al. Toward transformer-based object detection.arXiv:2012.09958, 2020.</strong></a></li>\n<li><a href=\"#2-hassani-a-walton-s-li-j-et-al-neighborhood-attention-transformer-arxiv-preprint-arxiv220407143-2022\"><strong>2、Hassani A, Walton S, Li J, et al. Neighborhood Attention Transformer. arXiv preprint arXiv:2204.07143, 2022.</strong></a>\n<ul>\n<li><a href=\"#neighborhood-attention-na\">Neighborhood Attention (NA)</a></li>\n</ul>\n</li>\n<li><a href=\"#3-hassani-ali-and-humphrey-shi-dilated-neighborhood-attention-transformer-arxiv-abs220915001-2022-n-pag\"><strong>3、Hassani, Ali and Humphrey Shi. “Dilated Neighborhood Attention Transformer” <em>ArXiv</em> abs/2209.15001 (2022): n. pag.</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#%E4%B8%89-light-weight-transformer\"><strong>三、Light-Weight transformer</strong></a>\n<ul>\n<li><a href=\"#1-huang-t-huang-l-you-s-et-al-towards-light-weight-convolution-free-vision-transformers-arxiv-preprint-arxiv220705557-2022\"><strong>1、Huang T, Huang L, You S, et al. Towards Light-Weight Convolution-Free Vision Transformers. arXiv preprint arXiv:2207.05557, 2022.</strong></a></li>\n<li><a href=\"#2-shen-z-zhang-m-zhao-h-et-al-efficient-attention-attention-with-linear-complexitiescproceedings-of-the-ieeecvf-winter-conference-on-applications-of-computer-vision-2021-3531-3539\"><strong>2、Shen Z, Zhang M, Zhao H, et al. Efficient attention: Attention with linear complexities[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2021: 3531-3539.</strong></a></li>\n<li><a href=\"#3-lee-sh-lee-s-song-bc-2021-vision-transformer-for-small-size-datasets-arxiv-abs211213492\"><strong>3、Lee, S.H., Lee, S., &amp; Song, B.C. (2021). Vision Transformer for Small-Size Datasets. ArXiv, abs/2112.13492.</strong></a></li>\n</ul>\n</li>\n<li><a href=\"#%E6%80%9D%E8%80%83\"><strong>思考：</strong></a></li>\n</ul>\n","date":"2023-02-23 14:50:54","dateFormat":"2023-02-23"},{"fileName":"chang-yong-linux-ming-ling","abstract":"","description":"常用命令 Ubuntu系统安装Nvidia驱动并配置CUDA 安装Nvidia驱动 #打开终端，删除旧的驱动 $ sudo apt-get purge nvidia* #禁用自带的 nouveau nvidia驱动 $ sudo vi ...","title":"【常用Linux命令】","tags":[],"feature":"","link":"https://chenjie04.github.io/post/chang-yong-linux-ming-ling/","stats":{"text":"13 min read","time":759000,"words":2591,"minutes":13},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4\">常用命令</a></li>\n</ul>\n","date":"2023-02-22 16:55:21","dateFormat":"2023-02-22"},{"fileName":"bi-ji-underwater-image-enhancement-method-based-on-denoising-diffusion-probabilistic-model","abstract":"","description":" 水下图像由于光线的选择性吸收和散射等原因造成了颜色偏差、模糊等退化现象，导致目标检测等水下探测活动效果较差，因此研究水下图像增强成为了一个研究热点。下图是水下图像增强的示例，其中(a)是原图，(b-j)是不同水下图像增强算法的增强效果，水...","title":"（未完成）笔记：Underwater Image Enhancement ","tags":[],"feature":"","link":"https://chenjie04.github.io/post/bi-ji-underwater-image-enhancement-method-based-on-denoising-diffusion-probabilistic-model/","stats":{"text":"2 min read","time":63000,"words":284,"minutes":2},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#1-uncertainty-inspired-underwater-image-enhancement\"><strong>1、Uncertainty Inspired Underwater Image Enhancement</strong></a></li>\n<li><a href=\"#2-simple-baselines-for-image-restoration\"><strong>2、Simple Baselines for Image Restoration</strong></a></li>\n<li><a href=\"#3-adaptive-uncertainty-distribution-in-deep-learning-for-unsupervised-underwater-image-enhancement\"><strong>3、Adaptive Uncertainty Distribution in Deep Learning for Unsupervised Underwater Image Enhancement</strong></a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-02-22 10:47:43","dateFormat":"2023-02-22"},{"fileName":"mmdetection-3x-xue-xi-bi-ji","abstract":"","description":"一、学习配置文件 MMDetection 仓库使用 MMEngine 的配置文件系统，所有功能的模块都可以通过配置文件进行配置。 1、默认运行时配置（configs/base/default_runtime.py） default_scop...","title":"mmdetection-3.x学习笔记——默认运行时配置（default_runtime.py）","tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/"}],"feature":"","link":"https://chenjie04.github.io/post/mmdetection-3x-xue-xi-bi-ji/","stats":{"text":"2 min read","time":119000,"words":498,"minutes":2},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E5%AD%A6%E4%B9%A0%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6\">一、学习配置文件</a>\n<ul>\n<li><a href=\"#1-%E9%BB%98%E8%AE%A4%E8%BF%90%E8%A1%8C%E6%97%B6%E9%85%8D%E7%BD%AEconfigsbasedefault_runtimepy\">1、默认运行时配置（configs/<em>base</em>/default_runtime.py）</a>\n<ul>\n<li><a href=\"#1mmengin%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%88%86%E6%9E%90%E4%B8%80hook\">（1）MMEngin核心组件分析（一）：Hook</a></li>\n<li><a href=\"#2-%E9%85%8D%E7%BD%AE%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83\">（2） 配置程序运行环境</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2023-02-21 08:50:56","dateFormat":"2023-02-21"},{"fileName":"cth-bo-shi-lun-wen-ji-yu-qiang-hua-xue-xi-de-zi-dong-jia-shi-jue-ce","abstract":"","description":"论文地址：https://research.chalmers.se/en/publication/526543 ...","title":"（未完成）【CTH博士论文】基于强化学习的自动驾驶决策 ","tags":[{"index":-1,"name":"未完成","slug":"e2WlN7XCF","used":true,"link":"https://chenjie04.github.io/tag/e2WlN7XCF/"},{"index":-1,"name":"强化学习","slug":"FnJUUTGLL","used":true,"link":"https://chenjie04.github.io/tag/FnJUUTGLL/"}],"feature":"","link":"https://chenjie04.github.io/post/cth-bo-shi-lun-wen-ji-yu-qiang-hua-xue-xi-de-zi-dong-jia-shi-jue-ce/","stats":{"text":"1 min read","time":3000,"words":11,"minutes":1},"isTop":false,"toc":"","date":"2023-02-21 08:47:28","dateFormat":"2023-02-21"},{"fileName":"bi-ji-rtmdet-an-empirical-study-of-designing-real-time-object-detectors","abstract":"","description":"论文地址：RTMDet: An Empirical Study of Designing Real-Time Object Detectors 代码地址：https://github.com/open-mmlab/mmdetection/t...","title":"【论文阅读笔记】RTMDet: An Empirical Study of Designing Real-Time Object Detectors","tags":[],"feature":"","link":"https://chenjie04.github.io/post/bi-ji-rtmdet-an-empirical-study-of-designing-real-time-object-detectors/","stats":{"text":"5 min read","time":282000,"words":1218,"minutes":5},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E5%AE%8F%E8%A7%82%E6%9E%B6%E6%9E%84\">一、宏观架构</a></li>\n<li><a href=\"#%E4%BA%8C-%E6%A8%A1%E5%9E%8B\">二、模型</a>\n<ul>\n<li><a href=\"#1-basic-building-black\">1、Basic building black</a></li>\n<li><a href=\"#2-balance-of-model-width-and-depth\">2、Balance of model width and depth</a></li>\n<li><a href=\"#3-balance-of-backbone-and-neck\">3、Balance of backbone and neck</a></li>\n<li><a href=\"#4-shared-detection-head\">4、Shared detection head</a></li>\n<li><a href=\"#5-label-assignment-and-losses\">5、Label assignment and losses</a></li>\n<li><a href=\"#6-cached-mosaic-and-mixup\">6、Cached Mosaic and MixUp</a></li>\n<li><a href=\"#7-two-stage-training\">7、Two-stage training</a></li>\n<li><a href=\"#8-implementation-details\">8、Implementation Details</a></li>\n<li><a href=\"#9-experimental-result\">9、Experimental result</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-02-15 21:40:23","dateFormat":"2023-02-15"},{"fileName":"bi-ji-internimage-exploring-large-scale-vision-foundation-models-with-deformable-convolutions","abstract":"","description":"论文地址：https://arxiv.org/abs/2211.05778 代码地址：https://github.com/OpenGVLab/InternImage 一、引言 现在深度学习都去研究大模型了，不管CV还是NLP，都上大模型。...","title":"【论文阅读笔记】InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions","tags":[],"feature":"","link":"https://chenjie04.github.io/post/bi-ji-internimage-exploring-large-scale-vision-foundation-models-with-deformable-convolutions/","stats":{"text":"11 min read","time":644000,"words":2664,"minutes":11},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E5%BC%95%E8%A8%80\">一、引言</a></li>\n<li><a href=\"#%E4%BA%8C-internimage%E6%A8%A1%E5%9E%8B\">二、InternImage模型</a>\n<ul>\n<li><a href=\"#1-%E6%A0%B8%E5%BF%83%E7%AE%97%E5%AD%90\">1、核心算子</a>\n<ul>\n<li><a href=\"#1dcnv2%E5%9B%9E%E9%A1%BE\">（1）DCNv2回顾</a></li>\n<li><a href=\"#2dcvv2%E6%94%B9%E9%80%A0\">（2）DCVv2改造</a></li>\n</ul>\n</li>\n<li><a href=\"#2-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA\">2、模型构建</a></li>\n</ul>\n</li>\n<li><a href=\"#%E4%B8%89-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C\">三、实验结果</a></li>\n<li><a href=\"#%E5%9B%9B-%E6%80%BB%E7%BB%93\">四、总结</a></li>\n<li><a href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\">参考文献：</a></li>\n</ul>\n","date":"2023-02-12 15:18:44","dateFormat":"2023-02-12"},{"fileName":"wsl-2-pei-zhi-dai-li-zhuan-zai","abstract":"","description":"原文地址：https://www.cnblogs.com/tuilk/p/16287472.html ","title":"WSL 2 配置代理（转载）","tags":[],"feature":"","link":"https://chenjie04.github.io/post/wsl-2-pei-zhi-dai-li-zhuan-zai/","stats":{"text":"1 min read","time":3000,"words":12,"minutes":1},"isTop":false,"toc":"","date":"2023-02-10 22:54:52","dateFormat":"2023-02-10"},{"fileName":"qiang-hua-xue-xi-ru-men-bi-ji","abstract":"","description":"——根据台大李宏毅老师的《强化学习》课程笔记而来！ 一、什么是强化学习 当我们谈论什么是强化学习时总会看到上面这张图：强化学习就是智能体（Actor）对环境进行观测，然后根据观测结果（Observation）采取相应的动作（Action）...","title":"强化学习入门笔记","tags":[{"index":-1,"name":"强化学习","slug":"FnJUUTGLL","used":true,"link":"https://chenjie04.github.io/tag/FnJUUTGLL/"}],"feature":"","link":"https://chenjie04.github.io/post/qiang-hua-xue-xi-ru-men-bi-ji/","stats":{"text":"36 min read","time":2138000,"words":8578,"minutes":36},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li><a href=\"#%E4%B8%80-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\">一、什么是强化学习</a></li>\n<li><a href=\"#%E4%BA%8C-%E6%80%8E%E4%B9%88%E8%AE%AD%E7%BB%83%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\">二、怎么训练强化学习</a>\n<ul>\n<li><a href=\"#version-0\">Version 0</a></li>\n<li><a href=\"#version-1\">Version 1</a></li>\n<li><a href=\"#version-2\">Version 2</a></li>\n<li><a href=\"#version-3\">Version 3</a>\n<ul>\n<li><a href=\"#policy-gradient\">Policy Gradient</a></li>\n<li><a href=\"#on-policy-vs-off-policy\">On-policy v.s Off-policy</a></li>\n<li><a href=\"#proximal-policy-optimization-ppo\">Proximal Policy Optimization (PPO)</a></li>\n<li><a href=\"#policy-based-vs-value-based\">Policy-based v.s. Value-based</a></li>\n<li><a href=\"#critic%E7%9A%84%E5%BA%94%E7%94%A8\">Critic的应用</a>\n<ul>\n<li><a href=\"#q-learning\">Q-learning</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#version-35\">Version 3.5</a></li>\n<li><a href=\"#version-4\">Version 4</a></li>\n<li><a href=\"#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%85%B6%E4%BB%96%E7%9B%B8%E5%85%B3%E4%B8%BB%E9%A2%98\">强化学习中其他相关主题</a>\n<ul>\n<li><a href=\"#reward-shaping\">Reward Shaping</a></li>\n<li><a href=\"#inverse-reinforcement-learning\">Inverse Reinforcement Learning</a></li>\n<li><a href=\"#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%84%E7%90%86%E8%BF%9E%E7%BB%AD%E5%8A%A8%E4%BD%9C%E9%97%AE%E9%A2%98\">强化学习处理连续动作问题</a>\n<ul>\n<li><a href=\"#solution-1\">Solution 1：</a></li>\n<li><a href=\"#solution-2\">Solution 2：</a></li>\n</ul>\n</li>\n<li><a href=\"#solution-3\">Solution 3：</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n","date":"2023-02-08 16:57:52","dateFormat":"2023-02-08"},{"fileName":"google-research-2022-and-beyond-language-vision-and-generative-models","abstract":"","description":"—— Posted by Jeff Dean，谷歌2022年在AI领域的研究进展总结，窥一斑而知全豹 原博客地址：https://ai.googleblog.com/2023/01/google-research-2022-beyond-l...","title":"Google Research, 2022 & Beyond: Language, Vision and Generative Models","tags":[],"feature":"","link":"https://chenjie04.github.io/post/google-research-2022-and-beyond-language-vision-and-generative-models/","stats":{"text":"4 min read","time":211000,"words":932,"minutes":4},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#%E4%B8%80-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\">一、语言模型</a></li>\n<li><a href=\"#%E4%BA%8C-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\">二、计算机视觉</a></li>\n<li><a href=\"#%E4%B8%89-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B\">三、多模态模型</a></li>\n<li><a href=\"#%E5%9B%9B-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B\">四、生成模型</a></li>\n<li><a href=\"#%E4%BA%94-%E6%80%BB%E7%BB%93\">五、总结</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-01-25 09:52:15","dateFormat":"2023-01-25"},{"fileName":"kong-jian-zhu-yi-li-ji-zhi","abstract":"","description":" 注意力机制是由于人类无法同时处理接收到的海量视觉信息而选择性地重点关注或忽略部分信息的信息处理方式。空间注意力是最直接的注意力机制，空间注意力最自然的实现应该就是计算出一个空间注意力权重图然后和特征图相乘，即 图片来自博客：热力图与原始...","title":"空间注意力机制","tags":[{"index":-1,"name":"目标检测","slug":"B6C956tKF","used":true,"link":"https://chenjie04.github.io/tag/B6C956tKF/"}],"feature":"","link":"https://chenjie04.github.io/post/kong-jian-zhu-yi-li-ji-zhi/","stats":{"text":"5 min read","time":276000,"words":1099,"minutes":5},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#%E5%8E%9F%E5%A7%8B%E7%A9%BA%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B\">原始空间注意力</a></li>\n<li><a href=\"#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%A9%BA%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B\">自注意力模式的空间注意力</a></li>\n</ul>\n</li>\n</ul>\n","date":"2023-01-09 11:34:46","dateFormat":"2023-01-09"},{"fileName":"bi-ji-efficient-non-local-contrastive-attention-for-image-super-resolution","abstract":"","description":"论文地址：Efficient Non-Local Contrastive Attention for Image Super-Resolution 代码地址：https://github.com/Zj-BinXia/ENLCA Non-Lo...","title":"【论文阅读笔记】Efficient Non-Local Contrastive Attention for Image Super-Resolution","tags":[{"index":-1,"name":"目标检测","slug":"B6C956tKF","used":true,"link":"https://chenjie04.github.io/tag/B6C956tKF/"}],"feature":"","link":"https://chenjie04.github.io/post/bi-ji-efficient-non-local-contrastive-attention-for-image-super-resolution/","stats":{"text":"23 min read","time":1330000,"words":4586,"minutes":23},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#non-local-attention\">Non-Local Attention</a></li>\n<li><a href=\"#kernel-method\">Kernel Method</a></li>\n<li><a href=\"#efficient-non-local-attention\">Efficient Non-Local Attention</a></li>\n<li><a href=\"#contrastive-learning-for-sparse-aggragation\">Contrastive Learning for Sparse Aggragation</a></li>\n<li><a href=\"#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C\">实验结果</a></li>\n<li><a href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\">参考文献</a></li>\n</ul>\n</li>\n</ul>\n","date":"2022-12-31 10:34:37","dateFormat":"2022-12-31"},{"fileName":"xia-xie-zuo-ce-shi","abstract":"","description":"一、先验知识 1、条件概率的一般形式 P(A,B,C)=P(C∣B,A)P(B,A)=P(C∣B,A)P(B∣A)P(A)P(A,B,C) = P(C|B,A)P(B,A) = P(C|B,A)P(B|A)P(A) P(A,B,C)=P(...","title":"扩散模型（Diffusion Model）学习笔记","tags":[],"feature":"","link":"https://chenjie04.github.io/post/xia-xie-zuo-ce-shi/","stats":{"text":"50 min read","time":2955000,"words":8796,"minutes":50},"isTop":false,"toc":"<ul class=\"markdownIt-TOC\">\n<li>\n<ul>\n<li><a href=\"#%E4%B8%80-%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86\">一、先验知识</a></li>\n<li><a href=\"#%E4%BA%8C-diffusion-model\">二、Diffusion Model</a>\n<ul>\n<li><a href=\"#1-%E6%89%A9%E6%95%A3%E8%BF%87%E7%A8%8B\">1、扩散过程</a></li>\n<li><a href=\"#2-%E9%80%86%E6%89%A9%E6%95%A3%E8%BF%87%E7%A8%8Breverse-process\">2、逆扩散过程（Reverse Process）</a></li>\n<li><a href=\"#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\">3、损失函数</a></li>\n<li><a href=\"#4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%87%8D%E5%8F%82%E6%95%B0%E5%8C%96\">4、损失函数的重参数化</a></li>\n<li><a href=\"#5-%E5%B0%8F%E7%BB%93\">5、小结</a></li>\n</ul>\n</li>\n<li><a href=\"#%E4%B8%89-extracting-training-data-from-diffusion-models\">三、Extracting Training Data from Diffusion Models</a></li>\n<li><a href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\">参考文献</a></li>\n</ul>\n</li>\n</ul>\n","date":"2022-12-28 17:38:35","dateFormat":"2022-12-28"}],"tags":[{"index":-1,"name":"mmdetection-3.x","slug":"MzsbKaVdP","used":true,"link":"https://chenjie04.github.io/tag/MzsbKaVdP/","count":8},{"index":-1,"name":"未完成","slug":"e2WlN7XCF","used":true,"link":"https://chenjie04.github.io/tag/e2WlN7XCF/","count":1},{"index":-1,"name":"强化学习","slug":"FnJUUTGLL","used":true,"link":"https://chenjie04.github.io/tag/FnJUUTGLL/","count":2},{"index":-1,"name":"目标检测","slug":"B6C956tKF","used":true,"link":"https://chenjie04.github.io/tag/B6C956tKF/","count":2}],"menus":[{"link":"/","name":"首页","openType":"Internal"},{"link":"/archives","name":"归档","openType":"Internal"},{"link":"/tags","name":"标签","openType":"Internal"},{"link":"/post/about","name":"关于","openType":"Internal"}],"themeConfig":{"themeName":"bitcron-pro","postPageSize":10,"archivesPageSize":50,"siteName":"chenjie04's blog","siteDescription":"温故而知新","footerInfo":"Powered by <a href=\"https://github.com/getgridea/gridea\" target=\"_blank\">Gridea</a>","showFeatureImage":true,"domain":"https://chenjie04.github.io","postUrlFormat":"SLUG","tagUrlFormat":"SHORT_ID","dateFormat":"YYYY-MM-DD","feedFullText":true,"feedCount":10,"archivesPath":"archives","postPath":"post","tagPath":"tag"},"customConfig":{},"utils":{"now":1690208408768}}
